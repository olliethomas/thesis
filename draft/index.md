# Fair Representations of Biased Data
Thank you for looking at this work-in-progress document.

The purpose of this format is that work used in the annual review can be easily extended into a thesis.
This format has been selected so that runnable code examples can be used to express a point (if appropriate). 

If on any page there are questions you'd like to raise, please raise an issue using the github icon at the top of each page.
Similarly, if there are any minor corrections, you can use that tool as well.

# Abstract

Consistent decision making as an idividual is hard. 
Once distributed to many people, making consistent decisions across an organisation is even harder.
Because of this, there is an appeal to automated, consistent decision making systems.
The promise is that these systems are reliable, transparent, and just.
However, in practice, this is not always the case, often to the detriment of society's least powerful.
The common retort is that machines aren't biased, but the data they learn from can be.
In this thesis I investigate this claim. 
I show that positive improvements can be made by changing the data, mapping from the original data to a "fair" representation.
I then demonstrate that we can query these changes to ask what needs to be changed about the data for it to stop being _not_ "fair".
Then, I draw a parallel between this work and the work of causality to demonstrate that we can identiy "at-risk" individuals.

# Citation
To reference this thesis, please use the following BibTex

```bibtex
@phdthesis{FairReprTho21,
    author = {Thomas, Oliver},
    title  = {Fair Representations of Data},
    school = {University of Sussex},
    year   = {2021},
}
```