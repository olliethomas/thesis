{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Scenario 4\n",
    "\n",
    "(Before we start, let's just import some stuff that we'll need later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ethicml.algorithms.inprocess import LR, LRCV, InAlgorithm, Majority\n",
    "from ethicml.algorithms.inprocess.blind import Blind\n",
    "from ethicml.data import adult, compas, credit, crime, health, sqf, synthetic\n",
    "from ethicml import metric_per_sensitive_attribute, diff_per_sensitive_attribute\n",
    "from ethicml.implementations.pytorch_common import CustomDataset\n",
    "from ethicml.metrics import Accuracy, Metric, ProbPos\n",
    "from ethicml.preprocessing import scale_continuous, train_test_split\n",
    "from ethicml.utility import DataTuple\n",
    "from ethicml.visualisation import plot_results, save_2d_plot, save_jointplot, save_label_plot, save_multijointplot\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "epochs=20\n",
    "latent_dims=2\n",
    "latent_multiplier = 10\n",
    "scenario = 4\n",
    "targets = [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Throughout we're going to be using synthetic data. We could easily run all of this throughout with real-world datasets (launch this file using the icon at the top of the page and replace `synthetic(*args, **kwargs)` with any from `adult(), compas(), credit(), crime(), health(), sqf()`).\n",
    "\n",
    "The synthetic data covers 4 scenarios. To avoid getting overly complex, we'll use Scenario 4 throughout unless otherwise specified.\n",
    "\n",
    "### Scenario 1\n",
    "![Scenario 1](./assets/scenario_1.png)\n",
    "\n",
    "In this scenario, there are two input variables, $X_1$ and $X_2$.\n",
    "There are three outcome variables, $Y_1$, $Y_2$ & $Y_3$.\n",
    "There is one sensitive attribute, $S$, which is independent of all $X$ & $Y$.\n",
    "\n",
    "### Scenario 2\n",
    "![Scenario 2](./assets/scenario_2.png)\n",
    "\n",
    "In this scenario, there are two input variables, $X_1$ and $X_2$.\n",
    "There are three outcome variables, $Y_1$, $Y_2$ & $Y_3$.\n",
    "There is one sensitive attribute, $S$, which is independent of $X_2$ & $Y_2$, but not independent of $X_1$, $Y_1$, or $Y_3$.\n",
    "\n",
    "### Scenario 3\n",
    "![Scenario 3](./assets/scenario_3.png)\n",
    "\n",
    "In this scenario, there are two input variables, $X_1$ and $X_2$.\n",
    "There are three outcome variables, $Y_1$, $Y_2$ & $Y_3$.\n",
    "There is one sensitive attribute, $S$, which is independent of both $X$, $Y_1$ and $Y_2$, but not independent of $Y_3$.\n",
    "\n",
    "### Scenario 4\n",
    "![Scenario 4](./assets/scenario_4.png)\n",
    "\n",
    "In this scenario, there are two input variables, $X_1$ and $X_2$.\n",
    "There are three outcome variables, $Y_1$, $Y_2$ & $Y_3$.\n",
    "There is one sensitive attribute, $S$, which is independent of $X_2$ & $Y_2$, but not independent of $X_1$, $Y_1$, or $Y_3$.\n",
    "\n",
    "## Strategy 1\n",
    "\n",
    "We have some input data $x$. We want a function that produces a version of this data ($z_x$), such that $z_x$ is independent of some protected characteristic $s$. In other words we want to find $e: X \\rightarrow Z_x ~~\\mathrm{s.t.}~ Z_x \\perp S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at that. \n",
    "\n",
    "![setup1](assets/setup1.png)\n",
    "\n",
    "The red line indicates that you cannot learn $S$ from $Z_x$; there is no mutual information between these two variables. They are independent.\n",
    "\n",
    "The problem here is that the easiest way for a network to achieve this is to just learn nothing. Make $Z_x$ all $0$'s and your job is done. But that's a claim... let's demonstrate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building\n",
    "\n",
    "Let's build the adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"Gradient reversal layer\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: Tensor, lambda_: float) -> Tensor:\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        return grad_output.neg().mul(ctx.lambda_), None\n",
    "\n",
    "\n",
    "def grad_reverse(features: Tensor, lambda_: float = 1.0) -> Tensor:\n",
    "    return GradReverse.apply(features, lambda_)\n",
    "\n",
    "class FeatureAdv(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim, latent_dim*latent_multiplier)\n",
    "        self.hid_1 = nn.Linear(latent_dim*latent_multiplier, latent_dim*latent_multiplier)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*latent_multiplier)\n",
    "        self.out = nn.Linear(latent_dim*latent_multiplier, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        s = self.bn_1(F.relu(self.hid(grad_reverse(z))))\n",
    "        return self.out(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid_1 = nn.Linear(in_size, latent_dim*latent_multiplier)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*latent_multiplier)\n",
    "        self.hid_2 = nn.Linear(latent_dim*latent_multiplier, latent_dim*latent_multiplier)\n",
    "        self.bn_2 = nn.BatchNorm1d(latent_dim*latent_multiplier)\n",
    "\n",
    "        self.mu = nn.Linear(latent_dim*latent_multiplier, latent_dim)\n",
    "        self.logvar = nn.Linear(latent_dim*latent_multiplier, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.bn_1(F.relu(self.hid_1(x)))\n",
    "        z = F.relu(self.hid_2(z))\n",
    "        return td.Normal(loc=self.mu(z), scale=F.softplus(self.logvar(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        s = self.adv(z.rsample())\n",
    "        return z, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add some helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_z(\n",
    "    train: DataTuple,\n",
    "    test: DataTuple,\n",
    "    model: InAlgorithm = LRCV,\n",
    "    metric: Metric = Accuracy,\n",
    "    per_sens: bool = False,\n",
    "):\n",
    "    model = model()\n",
    "    preds = model.run(train, test)\n",
    "\n",
    "    if per_sens:\n",
    "        overall = metric_per_sensitive_attribute(preds, test, metric())\n",
    "        score = diff_per_sensitive_attribute(overall)\n",
    "        print(f\"{metric().name}: { {k: round(v*100, 3) for k, v in overall.items()} }, { {k: round(v*100, 3) for k, v in score.items()} }\\n\")\n",
    "        score = [v*100 for k, v in score.items()][0]\n",
    "    else:\n",
    "        score = metric().score(preds, test)*100\n",
    "        print(f\"{metric().name}: {score:.3f}\\n\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode1(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=[f\"{i}\" for i in range(latent_dims)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _ = model(x)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=[f\"{i}\" for i in range(latent_dims)]),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "datasets = []\n",
    "scalers = []\n",
    "data = []\n",
    "train = []\n",
    "test = []\n",
    "train_data = []\n",
    "train_loaders = []\n",
    "test_data = []\n",
    "test_loaders = []\n",
    "for target in targets:\n",
    "    datasets.insert(target-1, synthetic(scenario=scenario, target=target, num_samples=10_000))\n",
    "    data.insert(target-1, datasets[target-1].load())\n",
    "    scalers.insert(target-1, StandardScaler())\n",
    "    data.insert(target-1, scale_continuous(datasets[target-1], data[target-1], scalers[target-1])[0])\n",
    "    train_test_tuple = train_test_split(data[target-1], train_percentage=0.8)\n",
    "    train.insert(target-1, train_test_tuple[0])\n",
    "    test.insert(target-1, train_test_tuple[1])\n",
    "    train_data.insert(target-1, CustomDataset(train[target-1]))\n",
    "    train_loaders.insert(target-1, DataLoader(train_data[target-1], batch_size=256))\n",
    "    test_data.insert(target-1, CustomDataset(test[target-1]))\n",
    "    test_loaders.insert(target-1, DataLoader(test_data[target-1], batch_size=256))\n",
    "\n",
    "\n",
    "for target in targets:\n",
    "    # save_2d_plot(_train[target], f\"./plots/train_scenario_{scenario}_target_{target}.png\")\n",
    "    save_2d_plot(DataTuple(x=train[target-1].x[[\"x1\", \"x2\"]], s=train[target-1].s, y=train[target-1].y), f\"./plots/train_priv_scenario_{scenario}_target_{target}.png\")\n",
    "    save_jointplot(train[target-1].replace(x=train[target-1].x), f\"./plots/train_joint_scenario_{scenario}_target_{target}.png\")\n",
    "    save_multijointplot(train[target-1].replace(x=train[target-1].x), f\"./plots/train_multijoint_scenario_{scenario}_target_{target}.png\")\n",
    "    save_label_plot(train[target-1], f\"./plots/train_labels_scenario_{scenario}_target_{target}.png\")\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "original_acc = []\n",
    "og_maj_acc = []\n",
    "og_maj_fair = []\n",
    "og_random_acc = []\n",
    "original_fair = []\n",
    "og_maj_fair = []\n",
    "og_random_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    original_acc.insert(train_target-1, [])\n",
    "    og_maj_acc.insert(train_target-1, [])\n",
    "    og_random_acc.insert(train_target-1, [])\n",
    "    original_fair.insert(train_target-1, [])\n",
    "    og_maj_fair.insert(train_target-1, [])\n",
    "    og_random_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the original data trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        original_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1]))\n",
    "\n",
    "        print(f\"Majority classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_maj_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1], model=Majority))\n",
    "\n",
    "        print(f\"Random classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_random_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1], model=Blind))\n",
    "\n",
    "        print(f\"Fairness on the original data trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        original_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Fairness on the original data with a majority classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_maj_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1], model=Majority, metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Fairness on the original data with a random classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_random_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test[test_target-1], model=Blind, metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "models_strategy_1 = []\n",
    "post_trains_strategy_1 = []\n",
    "post_tests_strategy_1 = []\n",
    "\n",
    "for train_target in targets:\n",
    "    post_trains_strategy_1.insert(train_target-1, [])\n",
    "    post_tests_strategy_1.insert(train_target-1, [])\n",
    "\n",
    "    models_strategy_1.insert(train_target-1, Model1(len(train[train_target-1].x.columns), latent_dims))\n",
    "    \n",
    "    optimiser1 = Adam(models_strategy_1[train_target-1].parameters(), lr=1e-3)\n",
    "    scheduler1 = lr_scheduler.ExponentialLR(optimiser1, gamma=0.98)\n",
    "\n",
    "    models_strategy_1[train_target-1].train()\n",
    "\n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            for (x, s, y) in train_loaders[train_target-1]:\n",
    "                z, s_pred = models_strategy_1[train_target-1](x)\n",
    "\n",
    "                feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "                feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "                feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "\n",
    "                loss1 = 0.6*feat_sens_loss + 0.4*feat_kl_loss.mean() #Â If you want to try without a prior, comment out the last term\n",
    "\n",
    "                t.set_postfix(loss=loss1.item())\n",
    "                loss1.backward()\n",
    "                optimiser1.step()\n",
    "                optimiser1.zero_grad()\n",
    "            scheduler1.step()\n",
    "    models_strategy_1[train_target-1].eval()\n",
    "\n",
    "    \n",
    "    for test_target in targets:\n",
    "\n",
    "        post_trains_strategy_1[train_target-1].insert(test_target-1, DataTuple(x=encode1(train_loaders[test_target-1], models_strategy_1[train_target-1], latent_dims), s=train[test_target-1].s, y=train[test_target-1].y))\n",
    "        post_tests_strategy_1[train_target-1].insert(test_target-1, DataTuple(x=encode1(test_loaders[test_target-1], models_strategy_1[train_target-1], latent_dims), s=test[test_target-1].s, y=test[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_trains_strategy_1[train_target-1][test_target-1], f\"./plots/strategy1_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_trains_strategy_1[train_target-1][test_target-1], f\"./plots/strategy1_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_trains_strategy_1[train_target-1][test_target-1], f\"./plots/strategy1_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_trains_strategy_1[train_target-1][test_target-1], f\"./plots/strategy1_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "og_model_1_acc = []\n",
    "og_model_1_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    og_model_1_acc.insert(train_target-1, [])\n",
    "    og_model_1_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the embeddings trained on target {train_target}, evaluated on target{test_target}...\")\n",
    "        og_model_1_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_1[train_target-1][test_target-1], post_tests_strategy_1[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Fairness on the embeddings train on target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_model_1_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_1[train_target-1][test_target-1], post_tests_strategy_1[train_target-1][test_target-1], metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this? Well, we're using a variational autoencoder model and using a prior gaussian distribuiton as a regulariser. We could do away with this, but you'd end up in a similar position. (If you want to try this out, use the rocket icon at the top of the page to run this page as a notebook on mybinder.org.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 1 Learnt Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy1_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 1 Joint Plot of Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy1_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2\n",
    "\n",
    "So the problem is that our representation doesn't have any direction. It's goal is to make $S$ unrecognizable from $Z$. Which it does, it's just that you can't tell anything else from $Z$ either.\n",
    "\n",
    "So let's give $Z$ some direction.\n",
    "\n",
    "![setup2](assets/setup2.png)\n",
    "\n",
    "In this case we want $Z$ to have no information about $S$, but also be representative of $Y$.\n",
    "\n",
    "We can re-use most of the parts from before, but we need a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim, latent_dim*3)\n",
    "        self.hid_1 = nn.Linear(latent_dim*3, latent_dim*3)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*3)\n",
    "        self.out = nn.Linear(latent_dim*3, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        y = self.bn_1(F.relu(self.hid(z)))\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and slightly update one of our helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode2(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=[f\"{i}\" for i in range(latent_dims)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _, _ = model(x)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=[f\"{i}\" for i in range(latent_dims)]),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "        self.pred = EmbeddingPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        s = self.adv(z.rsample())\n",
    "        y = self.pred(z.rsample())\n",
    "        return z, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "models_strategy_2 = []\n",
    "post_trains_strategy_2 = []\n",
    "post_tests_strategy_2 = []\n",
    "\n",
    "for train_target in targets:\n",
    "#     models_strategy_2.insert(train_target-1, [])\n",
    "    post_trains_strategy_2.insert(train_target-1, [])\n",
    "    post_tests_strategy_2.insert(train_target-1, [])\n",
    "    \n",
    "    models_strategy_2.insert(train_target-1, Model2(len(train[train_target-1].x.columns), latent_dims))\n",
    "    \n",
    "    optimiser2 = Adam(models_strategy_2[train_target-1].parameters(), lr=1e-3)\n",
    "    scheduler2 = lr_scheduler.ExponentialLR(optimiser2, gamma=0.98)\n",
    "\n",
    "    models_strategy_2[train_target-1].train()\n",
    "\n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            for (x, s, y) in train_loaders[train_target-1]:\n",
    "                z, s_pred, y_pred = models_strategy_2[train_target-1](x)\n",
    "\n",
    "                feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "                feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "                feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "                pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "                loss2 = 0.1*feat_kl_loss.mean() + 0.2*feat_sens_loss + 0.7*pred_y_loss\n",
    "\n",
    "                t.set_postfix(loss=loss2.item())\n",
    "                loss2.backward()\n",
    "                optimiser2.step()\n",
    "                optimiser2.zero_grad()\n",
    "            scheduler2.step()\n",
    "    models_strategy_2[train_target-1].eval()\n",
    "    \n",
    "    \n",
    "    for test_target in targets:\n",
    "        post_trains_strategy_2[train_target-1].insert(target-1, DataTuple(x=encode2(train_loaders[test_target-1], models_strategy_2[train_target-1], latent_dims), s=train[test_target-1].s, y=train[test_target-1].y))\n",
    "        post_tests_strategy_2[train_target-1].insert(target-1, DataTuple(x=encode2(test_loaders[test_target-1], models_strategy_2[train_target-1], latent_dims), s=test[test_target-1].s, y=test[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_trains_strategy_2[train_target-1][test_target-1], f\"./plots/strategy2_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_trains_strategy_2[train_target-1][test_target-1], f\"./plots/strategy2_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_trains_strategy_2[train_target-1][test_target-1], f\"./plots/strategy2_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_trains_strategy_2[train_target-1][test_target-1], f\"./plots/strategy2_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "og_model_2_acc = []\n",
    "og_model_2_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    og_model_2_acc.insert(train_target-1, [])\n",
    "    og_model_2_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the embeddings trained on target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_model_2_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_2[train_target-1][test_target-1], post_tests_strategy_2[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Fairness on the embeddings train on target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_model_2_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_2[train_target-1][test_target-1], post_tests_strategy_2[train_target-1][test_target-1], metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is certainly more accurate than before, but although were a bit more equal than in the original data, we're not really doing a great job. The reason for this is that there is a tension between removing information that is relevant to $S$ and keeping information that is relevant for $Y$. To demonstrate this, let's tweak the above model to remove this tension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 2 Learnt Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy2_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 2 Joint Plot of Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy2_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3\n",
    "\n",
    "![setup3](assets/setup3.png)\n",
    "\n",
    "In this setup we remove the tension. $Z$ can freely remove $S$, and $Y$ can get all the information it needs about $S$ directly.\n",
    "\n",
    "We'll just update the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode3(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=[f\"{i}\" for i in range(latent_dims)])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _, _ = model(x, s)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=[f\"{i}\" for i in range(latent_dims)]),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And both the predictor and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingAndSPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim+1, latent_dim*3)\n",
    "        self.hid_1 = nn.Linear(latent_dim*3, latent_dim*3)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*3)\n",
    "        self.out = nn.Linear(latent_dim*3, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution, s: torch.Tensor):\n",
    "        y = self.bn_1(F.relu(self.hid(torch.cat([z, s], dim=1))))\n",
    "        return self.out(y)\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "        self.pred = EmbeddingAndSPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        z = self.enc(x)\n",
    "        z_sample  = z.rsample()\n",
    "        s_pred = self.adv(z_sample)\n",
    "        y = self.pred(z_sample, s)\n",
    "        return z, s_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "models_strategy_3 = []\n",
    "post_trains_strategy_3 = []\n",
    "post_tests_strategy_3 = []\n",
    "\n",
    "for train_target in targets:\n",
    "#     models_strategy_3.insert(train_target-1, [])\n",
    "    post_trains_strategy_3.insert(train_target-1, [])\n",
    "    post_tests_strategy_3.insert(train_target-1, [])\n",
    "    \n",
    "    models_strategy_3.insert(train_target-1, Model3(len(train[train_target-1].x.columns), latent_dims))\n",
    "    \n",
    "    optimiser3 = Adam(models_strategy_3[train_target-1].parameters(), lr=1e-3)\n",
    "    scheduler3 = lr_scheduler.ExponentialLR(optimiser3, gamma=0.98)\n",
    "\n",
    "    models_strategy_3[train_target-1].train()\n",
    "\n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            for (x, s, y) in train_loaders[train_target-1]:\n",
    "                z, s_pred, y_pred = models_strategy_3[train_target-1](x, s)\n",
    "\n",
    "                feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "                feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "                feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "                pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "                loss3 = 0.1*feat_kl_loss.mean() + 0.2*feat_sens_loss + 0.7*pred_y_loss\n",
    "\n",
    "                t.set_postfix(loss=loss3.item())\n",
    "                loss3.backward()\n",
    "                optimiser3.step()\n",
    "                optimiser3.zero_grad()\n",
    "            scheduler3.step()\n",
    "    models_strategy_3[train_target-1].eval()\n",
    "\n",
    "    for test_target in targets:\n",
    "        post_trains_strategy_3[train_target-1].insert(test_target-1, DataTuple(x=encode3(train_loaders[test_target-1], models_strategy_3[train_target-1], latent_dims), s=train[test_target-1].s, y=train[test_target-1].y))\n",
    "        post_tests_strategy_3[train_target-1].insert(test_target-1, DataTuple(x=encode3(test_loaders[test_target-1], models_strategy_3[train_target-1], latent_dims), s=test[test_target-1].s, y=test[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_trains_strategy_3[train_target-1][test_target-1], f\"./plots/strategy3_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_trains_strategy_3[train_target-1][test_target-1], f\"./plots/strategy3_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_trains_strategy_3[train_target-1][test_target-1], f\"./plots/strategy3_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_trains_strategy_3[train_target-1][test_target-1], f\"./plots/strategy3_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "og_model_3_acc = []\n",
    "og_model_3_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    og_model_3_acc.insert(train_target-1, [])\n",
    "    og_model_3_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the embeddings trained on target {train_target}, evaluated on target{test_target}...\")\n",
    "        og_model_3_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_3[train_target-1][test_target-1], post_tests_strategy_3[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Fairness on the embeddings train on target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_model_3_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_3[train_target-1][test_target-1], post_tests_strategy_3[train_target-1][test_target-1], metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in line with our understanding - we lose a bit more accuracy, but the probability of a positive outcome is more equal across the groups. This would probably get closer to parity if we had either a more complicated model, or trained for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 3 Learnt Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy3_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 3 Joint Plot of Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy3_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happy Days\n",
    "\n",
    "Well, this is all quite positive. What's the problem?\n",
    "\n",
    "There are 2.\n",
    "\n",
    "1. We've only learnt a fair representation that's any use __as long as you want to predct $Y$__. If you wanted to learn some other label from your data, then this representation isn't going to help that much. We'll pick up on this later.\n",
    "2. The removal of $S$ is only as good as the model we used to remove it. Let's demonstrate this.\n",
    "\n",
    "Let's build a \"shallow\" version of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class ShallowFeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.mu = nn.Linear(in_size, latent_dim)\n",
    "        self.logvar = nn.Linear(in_size, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return td.Normal(loc=self.mu(x), scale=F.softplus(self.logvar(x)))\n",
    "    \n",
    "class ShallowFeatureAdv(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        return self.out(grad_reverse(z))\n",
    "    \n",
    "class ShallowEmbeddingAndSPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(latent_dim+1, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution, s: torch.Tensor):\n",
    "        return self.out(torch.cat([z, s], dim=1))\n",
    "    \n",
    "class Model4(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = ShallowFeatureAdv(latent_dim)\n",
    "        self.pred = EmbeddingAndSPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        z = self.enc(x)\n",
    "        z_sample  = z.rsample()\n",
    "        s_pred = self.adv(z_sample)\n",
    "        y = self.pred(z_sample, s)\n",
    "        return z, s_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "models_strategy_4 = []\n",
    "post_trains_strategy_4 = []\n",
    "post_tests_strategy_4 = []\n",
    "\n",
    "for train_target in targets:\n",
    "    post_trains_strategy_4.insert(train_target-1, [])\n",
    "    post_tests_strategy_4.insert(train_target-1, [])\n",
    "\n",
    "    models_strategy_4.insert(train_target-1, Model4(len(train[train_target-1].x.columns), latent_dims))\n",
    "    optimiser4 = Adam(models_strategy_4[train_target-1].parameters(), lr=1e-3)\n",
    "    scheduler4 = lr_scheduler.ExponentialLR(optimiser4, gamma=0.98)\n",
    "\n",
    "    models_strategy_4[train_target-1].train()\n",
    "\n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            for (x, s, y) in train_loaders[train_target-1]:\n",
    "                z, s_pred, y_pred = models_strategy_4[train_target-1](x, s)\n",
    "\n",
    "                feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "                feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "                feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "                pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "                loss4 = 0.1*feat_kl_loss.mean() + 0.2*feat_sens_loss + 0.7*pred_y_loss\n",
    "\n",
    "                t.set_postfix(loss=loss4.item())\n",
    "                loss4.backward()\n",
    "                optimiser4.step()\n",
    "                optimiser4.zero_grad()\n",
    "            scheduler4.step()\n",
    "    models_strategy_4[train_target-1].eval()\n",
    "\n",
    "    \n",
    "    for test_target in targets:\n",
    "        post_trains_strategy_4[train_target-1].insert(test_target-1, DataTuple(x=encode3(train_loaders[test_target-1], models_strategy_4[train_target-1], latent_dims), s=train[test_target-1].s, y=train[test_target-1].y))\n",
    "        post_tests_strategy_4[train_target-1].insert(test_target-1, DataTuple(x=encode3(test_loaders[test_target-1], models_strategy_4[train_target-1], latent_dims), s=test[test_target-1].s, y=test[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_trains_strategy_4[train_target-1][test_target-1], f\"./plots/strategy4_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_trains_strategy_4[train_target-1][test_target-1], f\"./plots/strategy4_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_trains_strategy_4[train_target-1][test_target-1], f\"./plots/strategy4_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_trains_strategy_4[train_target-1][test_target-1], f\"./plots/strategy4_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "og_model_4_acc = []\n",
    "og_model_4_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    og_model_4_acc.insert(train_target-1, [])\n",
    "    og_model_4_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the embeddings trained on target {train_target}, evaluated on target{test_target}...\")\n",
    "        og_model_4_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_4[train_target-1][train_target-1], post_tests_strategy_4[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Fairness on the embeddings train on target {train_target}, evaluated on target {test_target}...\")\n",
    "        og_model_4_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_4[train_target-1][train_target-1], post_tests_strategy_4[train_target-1][test_target-1], metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 4 Learnt Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy4_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 4 Joint Plot of Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy4_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisations to the real world\n",
    "\n",
    "Let's suppose that we now deploy our model into a world where the effect of the sensitive attribute doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_big = []\n",
    "data_big = []\n",
    "dataset_fair = []\n",
    "data_fair = []\n",
    "train_fair = []\n",
    "test_fair = []\n",
    "train_data_fair = []\n",
    "train_loader_fair = []\n",
    "test_data_fair = []\n",
    "test_loader_fair = []\n",
    "for target in targets:\n",
    "    dataset_big.insert(target-1, synthetic(scenario=scenario, num_samples=10_000, fair=False))\n",
    "    data_big.insert(target-1, dataset_big[target-1].load())\n",
    "\n",
    "    dataset_fair.insert(target-1, synthetic(scenario=scenario, num_samples=10_000, fair=True))\n",
    "\n",
    "    _data_fair = dataset_fair[target-1].load()\n",
    "    _data_fair = DataTuple(x=data_big[target-1].x, s=data_big[target-1].s, y=_data_fair.y)\n",
    "    scaler = StandardScaler()\n",
    "    _data_fair, _ = scale_continuous(dataset_big[target-1], _data_fair, scaler)\n",
    "    data_fair.insert(target-1, _data_fair)\n",
    "\n",
    "    _train_fair, _test_fair = train_test_split(data_fair[target-1], train_percentage=0.8)\n",
    "    train_fair.insert(target-1, _train_fair) \n",
    "    train_data_fair.insert(target-1, CustomDataset(_train_fair))\n",
    "    train_loader_fair.insert(target-1, DataLoader(train_data_fair[target-1], batch_size=256))\n",
    "\n",
    "    test_data_fair.insert(target-1, CustomDataset(_test_fair))\n",
    "    test_fair.insert(target-1, _test_fair)\n",
    "    test_loader_fair.insert(target-1, DataLoader(test_data_fair[target-1], batch_size=256))\n",
    "\n",
    "#     save_2d_plot(_train_fair, f\"./plots/fair_train_scenario_{scenario}_target_{target}.png\")\n",
    "    save_2d_plot(DataTuple(x=_train_fair.x[[\"x1\", \"x2\"]], s=_train_fair.s, y=_train_fair.y), f\"./plots/fair_train_priv_scenario_{scenario}_target_{target}.png\")\n",
    "    save_jointplot(_train_fair.replace(x=_train_fair.x[[\"x1\", \"x2\"]]), f\"./plots/fair_train_joint_scenario_{scenario}_target_{target}.png\")\n",
    "    save_multijointplot(_train_fair.replace(x=_train_fair.x[[\"x1\", \"x2\"]]), f\"./plots/fair_train_multijoint_scenario_{scenario}_target_{target}.png\")\n",
    "    save_label_plot(_train_fair, f\"./plots/fair_train_labels_scenario_{scenario}_target_{target}.png\")\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {target}\")\n",
    "    display(Image(filename=f\"./plots/train_labels_scenario_{scenario}_target_{target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Fair Data: Label Proportions - target {target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_labels_scenario_{scenario}_target_{target}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fair_original_acc = []\n",
    "fair_maj_acc = []\n",
    "fair_maj_fair = []\n",
    "fair_random_acc = []\n",
    "fair_original_fair = []\n",
    "fair_maj_fair = []\n",
    "fair_random_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    fair_original_acc.insert(train_target-1, [])\n",
    "    fair_maj_acc.insert(train_target-1, [])\n",
    "    fair_random_acc.insert(train_target-1, [])\n",
    "    fair_original_fair.insert(train_target-1, [])\n",
    "    fair_maj_fair.insert(train_target-1, [])\n",
    "    fair_random_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance on the original data trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_original_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1]))\n",
    "\n",
    "        print(f\"Majority classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_maj_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1], model=Majority))\n",
    "\n",
    "        print(f\"Random classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_random_acc[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1], model=Blind))\n",
    "\n",
    "        print(f\"Fairness on the original data trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_original_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Fairness on the original data with a majority classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_maj_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1], model=Majority, metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Fairness on the original data with a random classifier trained with target {train_target}, evaluated on target {test_target}...\")\n",
    "        fair_random_fair[train_target-1].insert(test_target-1, evaluate_z(train[train_target-1], test_fair[test_target-1], model=Blind, metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "post_train1_fair = []\n",
    "post_test1_fair = []\n",
    "for train_target in targets:\n",
    "    post_train1_fair.insert(train_target-1, [])\n",
    "    post_test1_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        post_train1_fair[train_target-1].insert(test_target-1, DataTuple(x=encode1(train_loader_fair[test_target-1], models_strategy_1[train_target-1], latent_dims), s=train_fair[test_target-1].s, y=train_fair[test_target-1].y))\n",
    "        post_test1_fair[train_target-1].insert(test_target-1, DataTuple(x=encode1(test_loader_fair[test_target-1], models_strategy_1[train_target-1], latent_dims), s=test_fair[test_target-1].s, y=test_fair[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_train1_fair[train_target-1][test_target-1], f\"./plots/fair_strategy1_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_train1_fair[train_target-1][test_target-1], f\"./plots/fair_strategy1_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_train1_fair[train_target-1][test_target-1], f\"./plots/fair_strategy1_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_train1_fair[train_target-1][test_target-1], f\"./plots/fair_strategy1_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    print(f\"Scenario {scenario}, Fair Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 4 Learnt Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy1_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 4 Joint Plot of Representation, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy1_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fair_model_1_acc = []\n",
    "fair_model_1_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    fair_model_1_acc.insert(train_target-1, [])\n",
    "    fair_model_1_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance of model1 on the fair data: trained on original data with target {train_target}, evaluated on fair data with target {test_target}...\")\n",
    "        fair_model_1_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_1[train_target-1][test_target-1], post_test1_fair[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Difference in performance between the fair data and the original data: trained on {train_target}, evaluated on {test_target}\")\n",
    "        print(f\"{(fair_model_1_acc[train_target-1][test_target-1] - og_model_1_acc[train_target-1][test_target-1]):.3f}\\n\")\n",
    "        \n",
    "        print(f\"Fairness on the embeddings...\")\n",
    "        fair_model_1_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_1[train_target-1][test_target-1], post_test1_fair[train_target-1][test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Difference in performance on the original data...\")\n",
    "        print(f\"{(fair_model_1_fair[train_target-1][test_target-1] - og_model_1_fair[train_target-1][test_target-1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "post_train2_fair = []\n",
    "post_test2_fair = []\n",
    "for train_target in targets:\n",
    "    post_train2_fair.insert(train_target-1, [])\n",
    "    post_test2_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        post_train2_fair[train_target-1].insert(test_target-1, DataTuple(x=encode2(train_loader_fair[test_target-1], models_strategy_2[train_target-1], latent_dims), s=train_fair[test_target-1].s, y=train_fair[test_target-1].y))\n",
    "        post_test2_fair[train_target-1].insert(test_target-1, DataTuple(x=encode2(test_loader_fair[test_target-1], models_strategy_2[train_target-1], latent_dims), s=test_fair[test_target-1].s, y=test_fair[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_train2_fair[train_target-1][test_target-1], f\"./plots/fair_strategy2_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_train2_fair[train_target-1][test_target-1], f\"./plots/fair_strategy2_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_train2_fair[train_target-1][test_target-1], f\"./plots/fair_strategy2_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_train2_fair[train_target-1][test_target-1], f\"./plots/fair_strategy2_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    print(f\"Scenario {scenario}, Fair Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 4 Learnt Representation 2, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy2_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 4 Joint Plot of Representation 2, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy2_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fair_model_2_acc = []\n",
    "fair_model_2_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    fair_model_2_acc.insert(train_target-1, [])\n",
    "    fair_model_2_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance of model2 on the fair data: trained on original data with target {train_target}, evaluated on fair data with target {test_target}...\")\n",
    "        fair_model_2_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_2[train_target-1][test_target-1], post_test2_fair[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Difference in performance between the fair data and the original data: trained on {train_target}, evaluated on {test_target}\")\n",
    "        print(f\"{(fair_model_2_acc[train_target-1][test_target-1] - og_model_2_acc[train_target-1][test_target-1]):.3f}\\n\")\n",
    "        \n",
    "        print(f\"Fairness on the embeddings...\")\n",
    "        fair_model_2_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_2[train_target-1][test_target-1], post_test2_fair[train_target-1][test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Difference in performance on the original data...\")\n",
    "        print(f\"{(fair_model_2_fair[train_target-1][test_target-1] - og_model_2_fair[train_target-1][test_target-1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "post_train3_fair = []\n",
    "post_test3_fair = []\n",
    "for train_target in targets:\n",
    "    post_train3_fair.insert(train_target-1, [])\n",
    "    post_test3_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        post_train3_fair[train_target-1].insert(test_target-1, DataTuple(x=encode3(train_loader_fair[test_target-1], models_strategy_3[train_target-1], latent_dims), s=train_fair[test_target-1].s, y=train_fair[test_target-1].y))\n",
    "        post_test3_fair[train_target-1].insert(test_target-1, DataTuple(x=encode3(test_loader_fair[test_target-1], models_strategy_3[train_target-1], latent_dims), s=test_fair[test_target-1].s, y=test_fair[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_train3_fair[train_target-1][test_target-1], f\"./plots/fair_strategy3_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_train3_fair[train_target-1][test_target-1], f\"./plots/fair_strategy3_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_train3_fair[train_target-1][test_target-1], f\"./plots/fair_strategy3_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_train3_fair[train_target-1][test_target-1], f\"./plots/fair_strategy3_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    print(f\"Scenario {scenario}, Fair Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    print(f\"\\nScenario {scenario}, Original Data Joint plot - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_multijoint_scenario_{scenario}_target_{train_target}.png\"))\n",
    "\n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 4 Learnt Representation 3, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy3_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        print(f\"\\nScenario {scenario}, Stategy 4 Joint Plot of Representation 3, trained on target {train_target}, legend for target {test_target}\")\n",
    "        display(Image(filename=f\"./plots/fair_strategy3_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fair_model_3_acc = []\n",
    "fair_model_3_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    fair_model_3_acc.insert(train_target-1, [])\n",
    "    fair_model_3_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance of model3 on the fair data: trained on original data with target {train_target}, evaluated on fair data with target {test_target}...\")\n",
    "        fair_model_3_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_3[train_target-1][test_target-1], post_test3_fair[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Difference in performance between the fair data and the original data: trained on {train_target}, evaluated on {test_target}\")\n",
    "        print(f\"{(fair_model_3_acc[train_target-1][test_target-1] - og_model_3_acc[train_target-1][test_target-1]):.3f}\\n\")\n",
    "        \n",
    "        print(f\"Fairness on the embeddings...\")\n",
    "        fair_model_3_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_3[train_target-1][test_target-1], post_test3_fair[train_target-1][test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Difference in performance on the original data...\")\n",
    "        print(f\"{(fair_model_3_fair[train_target-1][test_target-1] - og_model_3_fair[train_target-1][test_target-1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train4_fair = []\n",
    "post_test4_fair = []\n",
    "for train_target in targets:\n",
    "    post_train4_fair.insert(train_target-1, [])\n",
    "    post_test4_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        post_train4_fair[train_target-1].insert(test_target-1, DataTuple(x=encode3(train_loader_fair[test_target-1], models_strategy_4[train_target-1], latent_dims), s=train_fair[test_target-1].s, y=train_fair[test_target-1].y))\n",
    "        post_test4_fair[train_target-1].insert(test_target-1, DataTuple(x=encode3(test_loader_fair[test_target-1], models_strategy_4[train_target-1], latent_dims), s=test_fair[test_target-1].s, y=test_fair[test_target-1].y))\n",
    "\n",
    "        save_2d_plot(post_train4_fair[train_target-1][test_target-1], f\"./plots/fair_strategy4_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_train4_fair[train_target-1][test_target-1], f\"./plots/fair_strategy4_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_train4_fair[train_target-1][test_target-1], f\"./plots/fair_strategy4_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_train4_fair[train_target-1][test_target-1], f\"./plots/fair_strategy4_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_model_4_acc = []\n",
    "fair_model_4_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    fair_model_4_acc.insert(train_target-1, [])\n",
    "    fair_model_4_fair.insert(train_target-1, [])\n",
    "    for test_target in targets:\n",
    "        print(f\"Performance of model4 on the fair data: trained on original data with target {train_target}, evaluated on fair data with target {test_target}...\")\n",
    "        fair_model_4_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_4[train_target-1][test_target-1], post_test4_fair[train_target-1][test_target-1]))\n",
    "\n",
    "        print(f\"Difference in performance between the fair data and the original data: trained on {train_target}, evaluated on {test_target}\")\n",
    "        print(f\"{(fair_model_4_acc[train_target-1][test_target-1] - og_model_4_acc[train_target-1][test_target-1]):.3f}\\n\")\n",
    "        \n",
    "        print(f\"Fairness on the embeddings...\")\n",
    "        fair_model_4_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_4[train_target-1][test_target-1], post_test4_fair[train_target-1][test_target-1], metric=ProbPos, per_sens=True))\n",
    "\n",
    "        print(f\"Difference in performance on the original data...\")\n",
    "        print(f\"{(fair_model_4_fair[train_target-1][test_target-1] - og_model_4_fair[train_target-1][test_target-1])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That didn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    for test_target in targets:\n",
    "        if train_target != test_target:\n",
    "            continue\n",
    "        \n",
    "        models = ['Random Classifier', 'Majority Classifier', 'LR Original Data', 'Strategy 1', 'Strategy 2', 'Strategy 3', 'Strategy 3 (weak adv)', 'LR Original (fair setting)', 'Strategy 1 (Fair setting)', 'Strategy 2 (Fair setting)', 'Strategy 3 (Fair setting)', 'Str 3weak (Fair setting)']\n",
    "        accuracies = [og_random_acc[train_target-1][test_target-1], og_maj_acc[train_target-1][test_target-1], original_acc[train_target-1][test_target-1], og_model_1_acc[train_target-1][test_target-1], og_model_2_acc[train_target-1][test_target-1], og_model_3_acc[train_target-1][test_target-1], og_model_4_acc[train_target-1][test_target-1], fair_original_acc[train_target-1][test_target-1], fair_model_1_acc[train_target-1][test_target-1], fair_model_2_acc[train_target-1][test_target-1], fair_model_3_acc[train_target-1][test_target-1], fair_model_4_acc[train_target-1][test_target-1]]\n",
    "        dp_diffs = [og_random_fair[train_target-1][test_target-1], og_maj_fair[train_target-1][test_target-1], original_fair[train_target-1][test_target-1], og_model_1_fair[train_target-1][test_target-1], og_model_2_fair[train_target-1][test_target-1], og_model_3_fair[train_target-1][test_target-1], og_model_4_fair[train_target-1][test_target-1], fair_original_fair[train_target-1][test_target-1], fair_model_1_fair[train_target-1][test_target-1], fair_model_2_fair[train_target-1][test_target-1], fair_model_3_fair[train_target-1][test_target-1], fair_model_4_fair[train_target-1][test_target-1]]\n",
    "\n",
    "        col_width = 27\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print(\"|\"+f\"Train {train_target}\".center(col_width)+\"|\"+f\"Test {test_target}\".center(col_width)+\"|\"+\"\".center(col_width)+\"|\")\n",
    "        print(\"|\"+\"Name\".center(col_width)+\"|\"+\"Accuracy\".center(col_width)+\"|\"+\"DP Diff\".center(col_width)+\"|\")\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        for i in range(len(dp_diffs)):\n",
    "            accuracy = f\"{accuracies[i]:.3f}\"\n",
    "            dp_diff = f\"{dp_diffs[i]:.3f}\"\n",
    "            name = str(models[i])\n",
    "            print(\"|\"+name.center(col_width)+\"|\"+accuracy.center(col_width)+\"|\"+dp_diff.center(col_width)+\"|\")\n",
    "            print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we learn the true label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "new_latent_dims=1\n",
    "class FeatureEncoderY(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid_1 = nn.Linear(in_size, latent_dim*latent_multiplier*2)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*latent_multiplier*2)\n",
    "        self.hid_2 = nn.Linear(latent_dim*latent_multiplier*2, latent_dim*latent_multiplier*2)\n",
    "        self.bn_2 = nn.BatchNorm1d(latent_dim*latent_multiplier*2)\n",
    "\n",
    "        self.mu = nn.Linear(latent_dim*latent_multiplier*2, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.bn_1(F.relu(self.hid_1(x)))\n",
    "        z = self.bn_2(F.relu(self.hid_2(z)))\n",
    "        return td.Bernoulli(probs=torch.sigmoid(self.mu(z)).clamp(0+1e-6, 1-1e-6))\n",
    "    \n",
    "class YAdv(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()        \n",
    "        self.hid_1 = nn.Linear(latent_dim, latent_dim*latent_multiplier*2)\n",
    "        self.bn_1 = nn.BatchNorm1d(latent_dim*latent_multiplier*2)\n",
    "        self.hid_2 = nn.Linear(latent_dim*latent_multiplier*2, latent_dim*latent_multiplier*2)\n",
    "        self.bn_2 = nn.BatchNorm1d(latent_dim*latent_multiplier*2)\n",
    "\n",
    "        self.out = nn.Linear(latent_dim*latent_multiplier*2, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        sp = self.bn_1(F.relu(self.hid_1(grad_reverse(z.logits))))\n",
    "        sp = self.bn_2(F.relu(self.hid_2(sp)))\n",
    "        return self.out(sp)\n",
    "    \n",
    "class YAndSPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(latent_dim+2, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution, s: torch.Tensor):\n",
    "        return z.logits + self.out(torch.cat([z.logits, s], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode4(loader: DataLoader, model: nn.Module, latent_dims: int, sample: bool = False):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=list(range(latent_dims)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _, yh = model(x, s)\n",
    "            if not sample:\n",
    "                feats_train_encs = pd.concat(\n",
    "                    [\n",
    "                        feats_train_encs,\n",
    "                        pd.DataFrame(z.probs.round().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                    ],\n",
    "                    axis=\"rows\",\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                feats_train_encs = pd.concat(\n",
    "                    [\n",
    "                        feats_train_encs,\n",
    "                        pd.DataFrame(td.Bernoulli(logits=yh).sample().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                    ],\n",
    "                    axis=\"rows\",\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model5(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoderY(in_size, latent_dim)\n",
    "        self.adv = YAdv(latent_dim)\n",
    "        self.pred = YAndSPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # OHE\n",
    "        s_prime = torch.ones_like(s) - s\n",
    "        s = torch.cat([s, s_prime], dim=1)\n",
    "        \n",
    "        z = self.enc(x)\n",
    "        s_pred = self.adv(z)\n",
    "        y = self.pred(z, s)\n",
    "        return z, s_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "models_strategy_5 = []\n",
    "post_trains_strategy_5 = []\n",
    "post_tests_strategy_5 = []\n",
    "\n",
    "for train_target in targets:\n",
    "    models_strategy_5.insert(test_target-1, [])\n",
    "    post_trains_strategy_5.insert(train_target-1, [])\n",
    "    post_tests_strategy_5.insert(train_target-1, [])\n",
    "\n",
    "    for test_target in targets:\n",
    "        models_strategy_5[train_target-1].insert(test_target-1, Model5(len(train[train_target-1].x.columns), new_latent_dims))\n",
    "        optimiser5 = Adam(models_strategy_5[train_target-1][test_target-1].parameters(), lr=1e-3)\n",
    "        scheduler5 = lr_scheduler.ExponentialLR(optimiser5, gamma=0.98)\n",
    "\n",
    "        y_mean = train[test_target-1].y.mean().values[0]\n",
    "        models_strategy_5[train_target-1][test_target-1].train()\n",
    "\n",
    "        with trange(epochs) as t:\n",
    "            for epoch in t:\n",
    "                for (x, s, y) in train_loaders[train_target-1]:\n",
    "                    z, s_pred, y_pred = models_strategy_5[train_target-1][test_target-1](x, s)\n",
    "\n",
    "                    feat_prior = td.Bernoulli(probs=torch.ones(x.shape[0], new_latent_dims)*(y_mean))\n",
    "                    feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "                    feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "                    pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "                    loss5 = 0.3*pred_y_loss + 0.3*feat_kl_loss.mean() + 0.4*feat_sens_loss\n",
    "\n",
    "                    t.set_postfix(loss=loss5.item())\n",
    "\n",
    "                    loss5.backward()\n",
    "                    optimiser5.step()\n",
    "                    optimiser5.zero_grad()\n",
    "                    scheduler5.step()\n",
    "        models_strategy_5[train_target-1][test_target-1].eval()\n",
    "\n",
    "        post_trains_strategy_5[train_target-1].insert(test_target-1, DataTuple(x=train[test_target-1].x, s=train[test_target-1].s, y=encode4(train_loaders[test_target-1], models_strategy_5[train_target-1][test_target-1], new_latent_dims, sample=False)))\n",
    "        post_tests_strategy_5[train_target-1].insert(test_target-1, DataTuple(x=test[test_target-1].x, s=test[test_target-1].s, y=encode4(test_loaders[test_target-1], models_strategy_5[train_target-1][test_target-1], new_latent_dims, sample=False)))\n",
    "\n",
    "#         save_2d_plot(post_trains_strategy_5[train_target-1][test_target-1], f\"./plots/strategy5_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_2d_plot(post_trains_strategy_5[train_target-1][test_target-1].replace(x=train[test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/strategy5_train_priv_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_trains_strategy_5[train_target-1][test_target-1].replace(x=train[test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/strategy5_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_trains_strategy_5[train_target-1][test_target-1].replace(x=train[test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/strategy5_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_trains_strategy_5[train_target-1][test_target-1].replace(x=train[test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/strategy5_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')\n",
    "\n",
    "        \n",
    "post_train5_fair = []\n",
    "post_test5_fair = []\n",
    "for train_target in targets:\n",
    "    post_train5_fair.insert(train_target-1, [])\n",
    "    post_test5_fair.insert(test_target-1, [])\n",
    "    for test_target in targets:\n",
    "        post_train5_fair[train_target-1].insert(test_target-1, DataTuple(x=train_fair[test_target-1].x, s=train_fair[test_target-1].s, y=encode4(train_loader_fair[test_target-1], models_strategy_5[train_target-1][test_target-1], new_latent_dims)))\n",
    "        post_test5_fair[train_target-1].insert(test_target-1, DataTuple(x=train_fair[test_target-1].x, s=test_fair[test_target-1].s, y=encode4(test_loader_fair[test_target-1], models_strategy_5[train_target-1][test_target-1], new_latent_dims)))\n",
    "\n",
    "        save_2d_plot(post_train5_fair[train_target-1][test_target-1].replace(x=post_train5_fair[train_target-1][test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/fair_strategy5_train_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_jointplot(post_train5_fair[train_target-1][test_target-1].replace(x=post_train5_fair[train_target-1][test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/fair_strategy5_train_joint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_label_plot(post_train5_fair[train_target-1][test_target-1], f\"./plots/fair_strategy5_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        save_multijointplot(post_train5_fair[train_target-1][test_target-1].replace(x=post_train5_fair[train_target-1][test_target-1].x[[\"x1\", \"x2\"]]), f\"./plots/fair_strategy5_train_multijoint_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Original Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    print(f\"Scenario {scenario}, Fair Data - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_priv_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 5 Learnt Representation: trained on {train_target}, legend {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy5_train_priv_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))\n",
    "\n",
    "        \n",
    "for train_target in targets:\n",
    "    print(f\"Scenario {scenario}, Labels - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/train_labels_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    print(f\"Scenario {scenario}, Fair Labels - Target {train_target}\")\n",
    "    display(Image(filename=f\"./plots/fair_train_labels_scenario_{scenario}_target_{train_target}.png\"))\n",
    "    \n",
    "    for test_target in targets:\n",
    "        print(f\"\\nScenario {scenario}, Strategy 5 Labels: trained on {train_target}, legend {test_target}\")\n",
    "        display(Image(filename=f\"./plots/strategy5_train_labels_scenario_{scenario}_traintarget_{train_target}_testtarget_{test_target}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_model_5_acc = []\n",
    "og_model_5_fair = []\n",
    "\n",
    "fair_model_5_acc = []\n",
    "fair_model_5_fair = []\n",
    "\n",
    "og_combi_acc = []\n",
    "og_combi_fair = []\n",
    "\n",
    "fair_combi_acc = []\n",
    "fair_combi_fair = []\n",
    "\n",
    "for train_target in targets:\n",
    "    og_model_5_acc.insert(train_target-1, [])\n",
    "    og_model_5_fair.insert(train_target-1, [])\n",
    "    \n",
    "    fair_model_5_acc.insert(train_target-1, [])\n",
    "    fair_model_5_fair.insert(train_target-1, [])\n",
    "    \n",
    "    fair_combi_acc.insert(train_target-1, [])\n",
    "    fair_combi_fair.insert(train_target-1, [])\n",
    "    \n",
    "    og_combi_acc.insert(train_target-1, [])\n",
    "    og_combi_fair.insert(train_target-1, [])\n",
    "    \n",
    "    for test_target in targets:\n",
    "        print(f\"Performance of model3 on the fair data: trained on original data with target {train_target}, evaluated on fair data with target {test_target}...\")\n",
    "        og_model_5_acc[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_5[train_target-1][test_target-1], test[test_target-1]))\n",
    "        fair_model_5_acc[train_target-1].insert(test_target-1, evaluate_z(post_train5_fair[train_target-1][test_target-1], test_fair[test_target-1]))\n",
    "        \n",
    "        print(f\"Fairness on the embeddings...\")\n",
    "        og_model_5_fair[train_target-1].insert(test_target-1, evaluate_z(post_trains_strategy_5[train_target-1][test_target-1], test[test_target-1], metric=ProbPos, per_sens=True))\n",
    "        fair_model_5_fair[train_target-1].insert(test_target-1, evaluate_z(post_train5_fair[train_target-1][test_target-1], test_fair[test_target-1], metric=ProbPos, per_sens=True))\n",
    "        \n",
    "        combi_fair_train = DataTuple(x=post_trains_strategy_3[train_target-1][test_target-1].x, s=train[test_target-1].s, y=post_trains_strategy_5[train_target-1][test_target-1].y)\n",
    "        combi_fair_test = DataTuple(x=post_test3_fair[train_target-1][test_target-1].x, s=test_fair[test_target-1].s, y=test_fair[test_target-1].y)\n",
    "\n",
    "        print(f\"~~~Performance on the fair embeddings with fair labels train{train_target} test{test_target}...\")\n",
    "        fair_combi_acc[train_target-1].insert(test_target-1, evaluate_z(combi_fair_train, combi_fair_test))\n",
    "\n",
    "        print(f\"~~~Fairness on the fair embeddings with fair labels...\")\n",
    "        fair_combi_fair[train_target-1].insert(test_target-1, evaluate_z(combi_fair_train, combi_fair_test, metric=ProbPos, per_sens=True))\n",
    "        \n",
    "        combi_train = DataTuple(x=post_trains_strategy_3[train_target-1][test_target-1].x, s=train[test_target-1].s, y=post_trains_strategy_5[train_target-1][test_target-1].y)\n",
    "        combi_test = DataTuple(x=post_tests_strategy_3[train_target-1][test_target-1].x, s=test[test_target-1].s, y=test[test_target-1].y)\n",
    "\n",
    "        print(f\"~~~Performance on the fair embeddings with fair labels train{train_target} test{test_target}...\")\n",
    "        og_combi_acc[train_target-1].insert(test_target-1, evaluate_z(combi_train, combi_test))\n",
    "\n",
    "        print(f\"~~~Fairness on the fair embeddings with fair labels...\")\n",
    "        og_combi_fair[train_target-1].insert(test_target-1, evaluate_z(combi_train, combi_test, metric=ProbPos, per_sens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Result Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_target in targets:\n",
    "    for test_target in targets:\n",
    "        if test_target != train_target:\n",
    "            continue\n",
    "        \n",
    "        models = ['Random Classifier', 'Majority Classifier', 'LR Original Data', 'Strategy 1', 'Strategy 2', 'Strategy 3', 'Strategy 3 (weak)', 'Strategy 5', 'combi 3&5']\n",
    "        accuracies = [og_random_acc[train_target-1][test_target-1], og_maj_acc[train_target-1][test_target-1], original_acc[train_target-1][test_target-1], og_model_1_acc[train_target-1][test_target-1], og_model_2_acc[train_target-1][test_target-1], og_model_3_acc[train_target-1][test_target-1], og_model_4_acc[train_target-1][test_target-1], og_model_5_acc[train_target-1][test_target-1], og_combi_acc[train_target-1][test_target-1]]\n",
    "        dp_diffs = [og_random_fair[train_target-1][test_target-1], og_maj_fair[train_target-1][test_target-1], original_fair[train_target-1][test_target-1], og_model_1_fair[train_target-1][test_target-1], og_model_2_fair[train_target-1][test_target-1], og_model_3_fair[train_target-1][test_target-1], og_model_4_fair[train_target-1][test_target-1], og_model_5_fair[train_target-1][test_target-1], og_combi_fair[train_target-1][test_target-1]]\n",
    "\n",
    "        col_width = 27\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print(\"|\"+f\"Train {train_target}\".center(col_width)+\"|\"+f\"Test {test_target}\".center(col_width)+\"|\"+\"Biased Setting\".center(col_width)+\"|\")\n",
    "        print(\"|\"+\"Name\".center(col_width)+\"|\"+\"Accuracy\".center(col_width)+\"|\"+\"DP Diff\".center(col_width)+\"|\")\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        for i in range(len(dp_diffs)):\n",
    "            accuracy = f\"{accuracies[i]:.3f}\"\n",
    "            dp_diff = f\"{dp_diffs[i]:.3f}\"\n",
    "            name = str(models[i])\n",
    "            print(\"|\"+name.center(col_width)+\"|\"+accuracy.center(col_width)+\"|\"+dp_diff.center(col_width)+\"|\")\n",
    "            print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print()\n",
    "        \n",
    "        models = [\"Random\", \"Majority\", 'LR Original', 'Strategy 1', 'Strategy 2', 'Strategy 3', 'Strategy 3 (weak)', 'Strategy 5', 'Combi 3 & 5']\n",
    "        accuracies = [fair_random_acc[train_target-1][test_target-1], fair_maj_acc[train_target-1][test_target-1], fair_original_acc[train_target-1][test_target-1], fair_model_1_acc[train_target-1][test_target-1], fair_model_2_acc[train_target-1][test_target-1], fair_model_3_acc[train_target-1][test_target-1], fair_model_4_acc[train_target-1][test_target-1], fair_model_5_acc[train_target-1][test_target-1], fair_combi_acc[train_target-1][test_target-1]]\n",
    "        dp_diffs = [fair_random_fair[train_target-1][test_target-1], fair_maj_fair[train_target-1][test_target-1], fair_original_fair[train_target-1][test_target-1], fair_model_1_fair[train_target-1][test_target-1], fair_model_2_fair[train_target-1][test_target-1], fair_model_3_fair[train_target-1][test_target-1], fair_model_4_fair[train_target-1][test_target-1], fair_model_5_fair[train_target-1][test_target-1], fair_combi_fair[train_target-1][test_target-1]]\n",
    "\n",
    "        col_width = 27\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print(\"|\"+f\"Train {train_target}\".center(col_width)+\"|\"+f\"Test {test_target}\".center(col_width)+\"|\"+\"Fair Setting\".center(col_width)+\"|\")\n",
    "        print(\"|\"+\"Name\".center(col_width)+\"|\"+\"Accuracy\".center(col_width)+\"|\"+\"DP Diff\".center(col_width)+\"|\")\n",
    "        print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        for i in range(len(dp_diffs)):\n",
    "            accuracy = f\"{accuracies[i]:.3f}\"\n",
    "            dp_diff = f\"{dp_diffs[i]:.3f}\"\n",
    "            name = str(models[i])\n",
    "            print(\"|\"+name.center(col_width)+\"|\"+accuracy.center(col_width)+\"|\"+dp_diff.center(col_width)+\"|\")\n",
    "            print(\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\"+\"-\"*col_width+\"+\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
