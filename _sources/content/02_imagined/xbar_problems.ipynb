{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# What's wrong with Fair Representations of input data?\n",
    "\n",
    "(Before we start, let's just import some stuff that we'll need later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "from ethicml.algorithms.inprocess import LR, LRCV, InAlgorithm, Majority\n",
    "from ethicml.algorithms.inprocess.blind import Blind\n",
    "from ethicml.data import adult\n",
    "from ethicml.evaluators import metric_per_sensitive_attribute\n",
    "from ethicml.implementations.pytorch_common import CustomDataset\n",
    "from ethicml.metrics import Accuracy, Metric, ProbPos\n",
    "from ethicml.preprocessing import scale_continuous, train_test_split\n",
    "from ethicml.utility import DataTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1\n",
    "\n",
    "We have some input data $x$. We want a function that produces a version of this data ($z_x$), such that $z_x$ is independent of some protected characteristic $s$. In other words we want to find $e: X \\rightarrow Z_x ~~\\mathrm{s.t.}~ Z_x \\perp S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at that. \n",
    "\n",
    "![setup1](assets/setup1.png)\n",
    "\n",
    "The red line indicates that you cannot learn $S$ from $Z_x$; there is no mutual information between these two variables. They are independent.\n",
    "\n",
    "The problem here is that the easiest way for a network to achieve this is to just learn nothing. Make $Z_x$ all $0$'s and your job is done. But that's a claim... let's demonstrate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building\n",
    "\n",
    "Let's build the adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"Gradient reversal layer\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: Tensor, lambda_: float) -> Tensor:\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        return grad_output.neg().mul(ctx.lambda_), None\n",
    "\n",
    "\n",
    "def grad_reverse(features: Tensor, lambda_: float = 1.0) -> Tensor:\n",
    "    return GradReverse.apply(features, lambda_)\n",
    "\n",
    "class FeatureAdv(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim, 100)\n",
    "        self.hid_1 = nn.Linear(100, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        s = self.bn_1(F.relu(self.hid(grad_reverse(z))))\n",
    "        return self.out(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid_1 = nn.Linear(in_size, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.hid_2 = nn.Linear(100, 100)\n",
    "        self.bn_2 = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.mu = nn.Linear(100, latent_dim)\n",
    "        self.logvar = nn.Linear(100, latent_dim)\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        x = self.bn_1(F.relu(self.hid_1(z)))\n",
    "        x = F.relu(self.hid_2(x))\n",
    "        return td.Normal(loc=self.mu(x), scale=F.softplus(self.logvar(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        s = self.adv(z.rsample())\n",
    "        return z, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add some helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_z(\n",
    "    train: DataTuple,\n",
    "    test: DataTuple,\n",
    "    model: InAlgorithm = LRCV,\n",
    "    metric: Metric = Accuracy,\n",
    "    per_sens: bool = False,\n",
    "):\n",
    "    model = model()\n",
    "    preds = model.run(train, test)\n",
    "\n",
    "    if per_sens:\n",
    "        score = metric_per_sensitive_attribute(preds, test, metric())\n",
    "        print(f\"{metric().name}: {score}\\n\")\n",
    "    else:\n",
    "        score = metric().score(preds, test)\n",
    "        print(f\"{metric().name}: {score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode1(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=list(range(latent_dims)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _ = model(x)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = adult()\n",
    "data = dataset.load()\n",
    "scaler = StandardScaler()\n",
    "data, scaler2 = scale_continuous(dataset, data, scaler)\n",
    "\n",
    "_train, _test = train_test_split(data, train_percentage=0.9)\n",
    "train_data = CustomDataset(_train)\n",
    "train_loader = DataLoader(train_data, batch_size=256)\n",
    "\n",
    "test_data = CustomDataset(_test)\n",
    "test_loader = DataLoader(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the original data...\n",
      "Accuracy: 0.852\n",
      "\n",
      "Majority classifier...\n",
      "Accuracy: 0.749\n",
      "\n",
      "Random classifier...\n",
      "Accuracy: 0.500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the original data...\")\n",
    "evaluate_z(_train, _test)\n",
    "\n",
    "print(f\"Majority classifier...\")\n",
    "evaluate_z(_train, _test, model=Majority)\n",
    "\n",
    "print(f\"Random classifier...\")\n",
    "evaluate_z(_train, _test, model=Blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:29<00:00,  1.96s/it, loss=0.646]\n"
     ]
    }
   ],
   "source": [
    "epochs=15\n",
    "latent_dims=50\n",
    "\n",
    "model1 = Model1(len(_train.x.columns), latent_dims)\n",
    "optimizer = Adam(model1.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "with trange(epochs) as t:\n",
    "    for epoch in t:\n",
    "        for (x, s, y) in train_loader:\n",
    "            z, s_pred = model1(x)\n",
    "\n",
    "            feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "            feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "            feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "\n",
    "            loss = feat_kl_loss.mean() + feat_sens_loss\n",
    "\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "post_train = DataTuple(x=encode1(train_loader, model1, latent_dims), s=_train.s, y=_train.y)\n",
    "post_test = DataTuple(x=encode1(test_loader, model1, latent_dims), s=_test.s, y=_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the embeddings...\n",
      "Accuracy: 0.749\n",
      "\n",
      "Fairness on the original data...\n",
      "prob_pos: {'sex_Male_1': 0.26198905696813646, 'sex_Male_0': 0.08615819209039548}\n",
      "\n",
      "Fairness on the embeddings...\n",
      "prob_pos: {'sex_Male_1': 0.0, 'sex_Male_0': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the embeddings...\")\n",
    "evaluate_z(post_train, post_test)\n",
    "\n",
    "print(f\"Fairness on the original data...\")\n",
    "evaluate_z(_train, _test, metric=ProbPos, per_sens=True)\n",
    "\n",
    "print(f\"Fairness on the embeddings...\")\n",
    "evaluate_z(post_train, post_test, metric=ProbPos, per_sens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this? Well, we're using a variational autoencoder model and using a prior gaussian distribuiton as a regulariser. We could do away with this, but you'd end up in a similar position. (If you want to try this out, use the rocket icon at the top of the page to run this page as a notebook on mybinder.org.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2\n",
    "\n",
    "So the problem is that our representation doesn't have any direction. It's goal is to make $S$ unrecognizable from $Z$. Which it does, it's just that you can't tell anything else from $Z$ either.\n",
    "\n",
    "So let's give $Z$ some direction.\n",
    "\n",
    "![setup2](assets/setup2.png)\n",
    "\n",
    "In this case we want $Z$ to have no information about $S$, but also be representative of $Y$.\n",
    "\n",
    "We can re-use most of the parts from before, but we need a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim, 100)\n",
    "        self.hid_1 = nn.Linear(100, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        y = self.bn_1(F.relu(self.hid(z)))\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and slightly update one of our helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode2(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=list(range(latent_dims)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _, _ = model(x)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "        self.pred = EmbeddingPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        s = self.adv(z.rsample())\n",
    "        y = self.pred(z.rsample())\n",
    "        return z, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:45<00:00,  3.03s/it, loss=0.92] \n"
     ]
    }
   ],
   "source": [
    "model2 = Model2(len(_train.x.columns), latent_dims)\n",
    "optimizer = Adam(model2.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "with trange(epochs) as t:\n",
    "    for epoch in t:\n",
    "        for (x, s, y) in train_loader:\n",
    "            z, s_pred, y_pred = model2(x)\n",
    "\n",
    "            feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "            feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "            feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "            pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "            loss = feat_kl_loss.mean() + feat_sens_loss + pred_y_loss\n",
    "\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "post_train = DataTuple(x=encode2(train_loader, model2, latent_dims), s=_train.s, y=_train.y)\n",
    "post_test = DataTuple(x=encode2(test_loader, model2, latent_dims), s=_test.s, y=_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the embeddings...\n",
      "Accuracy: 0.845\n",
      "\n",
      "Fairness on the embeddings...\n",
      "prob_pos: {'sex_Male_1': 0.25329900225297713, 'sex_Male_0': 0.11581920903954802}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the embeddings...\")\n",
    "evaluate_z(post_train, post_test)\n",
    "\n",
    "print(f\"Fairness on the embeddings...\")\n",
    "evaluate_z(post_train, post_test, metric=ProbPos, per_sens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is certainly more accurate than before, but although were a bit more equal, we're not really doing a great job. The reason for this is that there is a tension between removing information that is relevant to $S$ and keeping information that is relevant for $Y$. To demonstrate this, let's tweak the above model to remove this tension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3\n",
    "\n",
    "![setup3](assets/setup3.png)\n",
    "\n",
    "In this setup we remove the tension. $Z$ can freely remove $S$, and $Y$ can get all the information it needs about $S$ directly.\n",
    "\n",
    "We'll just update the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode3(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=list(range(latent_dims)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _, _ = model(x, s)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And both the predictor and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class EmbeddingAndSPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim+1, 100)\n",
    "        self.hid_1 = nn.Linear(100, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution, s: torch.Tensor):\n",
    "        y = self.bn_1(F.relu(self.hid(torch.cat([z, s], dim=1))))\n",
    "        return self.out(y)\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "        self.pred = EmbeddingAndSPredictor(latent_dim)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        z = self.enc(x)\n",
    "        z_sample  = z.rsample()\n",
    "        s_pred = self.adv(z_sample)\n",
    "        y = self.pred(z_sample, s)\n",
    "        return z, s_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:56<00:00,  3.75s/it, loss=0.925]\n"
     ]
    }
   ],
   "source": [
    "model3 = Model3(len(_train.x.columns), latent_dims)\n",
    "optimizer = Adam(model3.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "with trange(epochs) as t:\n",
    "    for epoch in t:\n",
    "        for (x, s, y) in train_loader:\n",
    "            z, s_pred, y_pred = model3(x, s)\n",
    "\n",
    "            feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "            feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "            feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "            pred_y_loss = F.binary_cross_entropy_with_logits(y_pred, y, reduction=\"mean\")\n",
    "\n",
    "            loss = feat_kl_loss.mean() + feat_sens_loss + pred_y_loss\n",
    "\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "post_train = DataTuple(x=encode3(train_loader, model3, latent_dims), s=_train.s, y=_train.y)\n",
    "post_test = DataTuple(x=encode3(test_loader, model3, latent_dims), s=_test.s, y=_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the embeddings...\n",
      "Accuracy: 0.839\n",
      "\n",
      "Fairness on the embeddings...\n",
      "prob_pos: {'sex_Male_1': 0.221435468297393, 'sex_Male_0': 0.16242937853107345}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the embeddings...\")\n",
    "evaluate_z(post_train, post_test)\n",
    "\n",
    "print(f\"Fairness on the embeddings...\")\n",
    "evaluate_z(post_train, post_test, metric=ProbPos, per_sens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in line with our understanding - we lose a bit more accuracy, but the probability of a positive outcome is more equal across the groups. This would probably get closer to parity if we had either a more complicated model, or trained for longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
