{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# What's wrong with Fair Representations of input data?\n",
    "\n",
    "(Before we start, let's just import some stuff that we'll need later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "from ethicml.algorithms.inprocess import LR, LRCV, InAlgorithm, Majority\n",
    "from ethicml.algorithms.inprocess.blind import Blind\n",
    "from ethicml.data import adult\n",
    "from ethicml.evaluators import metric_per_sensitive_attribute\n",
    "from ethicml.implementations.pytorch_common import CustomDataset\n",
    "from ethicml.metrics import Accuracy, Metric, ProbPos\n",
    "from ethicml.preprocessing import scale_continuous, train_test_split\n",
    "from ethicml.utility import DataTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1\n",
    "\n",
    "We have some input data $x$. We want a function that produces a version of this data ($z_x$), such that $z_x$ is independent of some protected characteristic $s$. In other words we want to find $e: X \\rightarrow Z_x ~~\\mathrm{s.t.}~ Z_x \\perp S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at that. \n",
    "\n",
    "![setup1](assets/setup1.png)\n",
    "\n",
    "The red line indicates that you cannot learn $S$ from $Z_x$; there is no mutual information between these two variables. They are independent.\n",
    "\n",
    "The problem here is that the easiest way for a network to achieve this is to just learn nothing. Make $Z_x$ all $0$'s and your job is done. But that's a claim... let's demonstrate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building\n",
    "\n",
    "Let's build the adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"Gradient reversal layer\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: Tensor, lambda_: float) -> Tensor:\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        return grad_output.neg().mul(ctx.lambda_), None\n",
    "\n",
    "\n",
    "def grad_reverse(features: Tensor, lambda_: float = 1.0) -> Tensor:\n",
    "    return GradReverse.apply(features, lambda_)\n",
    "\n",
    "class FeatureAdv(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid = nn.Linear(latent_dim, 100)\n",
    "        self.hid_1 = nn.Linear(100, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, z: td.Distribution):\n",
    "        s = self.bn_1(F.relu(self.hid(grad_reverse(z))))\n",
    "        return self.out(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid_1 = nn.Linear(in_size, 100)\n",
    "        self.bn_1 = nn.BatchNorm1d(100)\n",
    "        self.hid_2 = nn.Linear(100, 100)\n",
    "        self.bn_2 = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.mu = nn.Linear(100, latent_dim)\n",
    "        self.logvar = nn.Linear(100, latent_dim)\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        x = self.bn_1(F.relu(self.hid_1(z)))\n",
    "        x = F.relu(self.hid_2(x))\n",
    "        return td.Normal(loc=self.mu(x), scale=F.softplus(self.logvar(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, in_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc = FeatureEncoder(in_size, latent_dim)\n",
    "        self.adv = FeatureAdv(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        s = self.adv(z.rsample())\n",
    "        return z, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add some helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_z(\n",
    "    train: DataTuple,\n",
    "    test: DataTuple,\n",
    "    model: InAlgorithm = LRCV,\n",
    "    metric: Metric = Accuracy,\n",
    "    per_sens: bool = False,\n",
    "):\n",
    "    model = model()\n",
    "    preds = model.run(train, test)\n",
    "\n",
    "    if per_sens:\n",
    "        score = metric_per_sensitive_attribute(preds, test, metric())\n",
    "        print(f\"{metric().name}: {score}\\n\")\n",
    "    else:\n",
    "        score = metric().score(preds, test)\n",
    "        print(f\"{metric().name}: {score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def encode(loader: DataLoader, model: nn.Module, latent_dims: int):\n",
    "    feats_train_encs: pd.DataFrame = pd.DataFrame(columns=list(range(latent_dims)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, s, y) in loader:\n",
    "            z, _ = model(x)\n",
    "            feats_train_encs = pd.concat(\n",
    "                [\n",
    "                    feats_train_encs,\n",
    "                    pd.DataFrame(z.sample().cpu().numpy(), columns=list(range(latent_dims))),\n",
    "                ],\n",
    "                axis=\"rows\",\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return feats_train_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = adult()\n",
    "data = dataset.load()\n",
    "scaler = StandardScaler()\n",
    "data, scaler2 = scale_continuous(dataset, data, scaler)\n",
    "\n",
    "_train, _test = train_test_split(data, train_percentage=0.9)\n",
    "train_data = CustomDataset(_train)\n",
    "train_loader = DataLoader(train_data, batch_size=256)\n",
    "\n",
    "test_data = CustomDataset(_test)\n",
    "test_loader = DataLoader(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the original data...\n",
      "Accuracy: 0.852\n",
      "\n",
      "Majority classifier...\n",
      "Accuracy: 0.749\n",
      "\n",
      "Random classifier...\n",
      "Accuracy: 0.500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the original data...\")\n",
    "evaluate_z(_train, _test)\n",
    "\n",
    "print(f\"Majority classifier...\")\n",
    "evaluate_z(_train, _test, model=Majority)\n",
    "\n",
    "print(f\"Random classifier...\")\n",
    "evaluate_z(_train, _test, model=Blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:14<00:00,  2.70s/it, loss=0.642]\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "latent_dims=50\n",
    "\n",
    "model = Model1(len(_train.x.columns), latent_dims)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "with trange(epochs) as t:\n",
    "    for epoch in t:\n",
    "        for (x, s, y) in train_loader:\n",
    "            z, s_pred = model(x)\n",
    "\n",
    "            feat_prior = td.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims))\n",
    "            feat_kl_loss = td.kl.kl_divergence(z, feat_prior)\n",
    "\n",
    "            feat_sens_loss = F.binary_cross_entropy_with_logits(s_pred, s, reduction=\"mean\")\n",
    "\n",
    "            loss = feat_kl_loss.mean() + feat_sens_loss\n",
    "\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "post_train = DataTuple(x=encode(train_loader, model, latent_dims), s=_train.s, y=_train.y)\n",
    "post_test = DataTuple(x=encode(test_loader, model, latent_dims), s=_test.s, y=_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the embeddings...\n",
      "Accuracy: 0.749\n",
      "\n",
      "Fairness on the original data...\n",
      "prob_pos: {'sex_Male_1': 0.26198905696813646, 'sex_Male_0': 0.08615819209039548}\n",
      "\n",
      "Fairness on the embeddings...\n",
      "prob_pos: {'sex_Male_1': 0.0, 'sex_Male_0': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on the embeddings...\")\n",
    "evaluate_z(post_train, post_test)\n",
    "\n",
    "print(f\"Fairness on the original data...\")\n",
    "evaluate_z(_train, _test, metric=ProbPos, per_sens=True)\n",
    "\n",
    "print(f\"Fairness on the embeddings...\")\n",
    "evaluate_z(post_train, post_test, metric=ProbPos, per_sens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2\n",
    "\n",
    "So the problem is that our representation doesn't have any direction. It's goal is to make $S$ unrecognizable from $Z$. Which it does, it's just that you can't tell anything else from $Z$ either.\n",
    "\n",
    "So let's give $Z$ some direction.\n",
    "\n",
    "![setup2](assets/setup2.png)\n",
    "\n",
    "In this case we want $Z$ to have no information about $S$, but also be representative of $Y$.\n",
    "\n",
    "We can re-use most of the parts from before, but we need a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}