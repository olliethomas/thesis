
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>An Algorithmic Framework for Positive Action &#8212; Fair Representations of Biased Data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="oliverthomas.ml/content/03_identifying/intro.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Conclusion" href="../08_conclusion/conclusion.html" />
    <link rel="prev" title="Data Domain Fairness" href="../02_data_domain_fairness/intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/pal-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Fair Representations of Biased Data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../content.html">
   Content
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Thesis
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01_introduction/intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_data_domain_fairness/intro.html">
   Data Domain Fairness
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   An Algorithmic Framework for Positive Action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08_conclusion/conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bib.html">
   Bibliography
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Annual Review 2020
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10_annual_review/summary.html">
   Summary
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10_annual_review/area.html">
     Research Area &amp; Question
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10_annual_review/activities_to_date.html">
   Activities to date
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10_annual_review/structure.html">
   Thesis Structure
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09_appendix/published.html">
   Publications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/dfritdd.html">
     Discovering Fair Representations in the Data Domain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/nosinn.html">
     Null-Sampling for Invariant and Interpretable Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/lit_review.html">
     Literature Review (Year 1 Annual Review)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09_appendix/software.html">
   Software
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/software/ethicml.html">
     EthicML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/software/intervene.html">
     Casual Discovery Tool
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/03_identifying/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/olliethomas/thesis/issues/new?title=Issue%20on%20page%20%2Fcontent/03_identifying/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/olliethomas/thesis/edit/master/content/03_identifying/intro.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     Definitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#counterfactual-modelling">
     Counterfactual Modelling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#related-works">
     Related Works
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#deferral">
       Deferral
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#actionable-recourse">
       Actionable Recourse
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#auditing-systems">
       Auditing Systems
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approach">
   Approach
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#positive-action-framework">
     Positive Action Framework
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fairness-worldview">
       Fairness Worldview
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#underlying-mechanisms-and-bias">
       Underlying mechanisms and bias
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#positive-action-candidates">
     Positive Action Candidates
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quantifying-the-difference-between-wae-and-wysiwyg">
       Quantifying the difference between WAE and WYSIWYG
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-the-right-positive-action-candidates">
       Choosing the right positive action candidates
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#building-a-group-classifier">
       Building a group classifier.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model">
       Model
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments">
   Experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#synthetic-data-generation">
       Synthetic Data Generation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#uci-adult-income-data">
       UCI Adult Income Data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-discussion">
   Results &amp; Discussion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analysing-the-baseline-synthetic-data">
     Analysing the baseline synthetic data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#demographic-parity-oracle">
       Demographic Parity Oracle
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logistic-regression-model">
       Logistic Regression model
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#positive-action-framework-model">
       Positive Action Framework Model
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auditing-the-uci-adult-income-for-bias">
     Auditing the UCI Adult Income for Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations-and-intended-use">
     Limitations and Intended Use
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="an-algorithmic-framework-for-positive-action">
<h1>An Algorithmic Framework for Positive Action<a class="headerlink" href="#an-algorithmic-framework-for-positive-action" title="Permalink to this headline">¶</a></h1>
<p>In previous work we have identified characteristics that are overly relied on and lead to unfair decisions.
This work amends this problem statement to instead of identifying characteristics and mitigating them, identify idividuals who are at risk of receivingan unfair decision.</p>
<div class="section" id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h2>
<p>Positive action is defined within anti-discrimination legislation as voluntary, legal actions taken to address imbalance of opportunity affecting individuals belonging to under-represented groups.
We propose a novel algorithmic fairness framework that builds on this notion as a way of advancing equal representation while respecting anti-discrimination legislation and equal-treatment rights.
We seek to identify individuals who have been negatively impacted by bias prior to application, and hence fail under an equal-treatment selection process.
We use counterfactual fairness to assign candidates to one of three groups:</p>
<ol class="simple">
<li><p>Candidates who would have been successful with any set of perceived protected attributes are assigned a successful outcome.</p></li>
<li><p>Candidates who would have remained unsuccessful with any set of protected attributes are assigned an unsuccessful outcome.</p></li>
<li><p>Unsuccessful candidates that would have been successful if they were born with a different set of protected attributes are flagged as positive action candidates.</p></li>
</ol>
</div>
<div class="section" id="introduction">
<span id="sec-intro"></span><h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Allocating resources amongst individuals, for example, jobs or university placements, requires us to evaluate an individual’s suitability for the task.
We also want to ensure the selection process is fair and that positive outcomes are fairly distributed within the population.
Machine Learning (ML) systems are increasingly being used to inform, support, or even directly make decisions within these consequential domains, affecting millions of lives <span id="id1">[<a class="reference internal" href="../bib.html#id83">BS16</a>]</span>.
It is therefore necessary to consider how the notions of fair process and fair outcomes translate into algorithmic decision support frameworks <span id="id2">[<a class="reference internal" href="../bib.html#id92">WMR20</a>, <a class="reference internal" href="../bib.html#id125">Xia21</a>]</span>.</p>
<p>Anti-discrimination legislation in the E.U., U.S. and U.K. among others, indicate that a fair selection process requires equal treatment in the sense that protected attributes, for example, gender and race, should not be considered within the decision making process without a good reason <span id="id3">[<a class="reference internal" href="../bib.html#id49">DHP+12</a>, <a class="reference internal" href="../bib.html#id130">Ben19</a>]</span>.
Ignoring the protected attributes within an algorithmic approach, however, guarantees neither fair process nor fair outcome <span id="id4">[<a class="reference internal" href="../bib.html#id89">PRT08</a>, <a class="reference internal" href="../bib.html#id131">HW19</a>, <a class="reference internal" href="../bib.html#id127">SBW21</a>, <a class="reference internal" href="../bib.html#id92">WMR20</a>, <a class="reference internal" href="../bib.html#id125">Xia21</a>]</span>.</p>
<p>Decision support algorithms are commonly trained on a dataset of past decisions.
The resulting model may disproportionately predict positive outcomes in favour of the majority over historically under-represented groups <span id="id5">[<a class="reference internal" href="../bib.html#id88">KZ18</a>, <a class="reference internal" href="../bib.html#id10">KC12</a>]</span>.
These statistical disparities could arise from two distinct mechanisms:
(i) unequal treatment;<br />
or (ii) equal treatment when the status quo itself is not neutral.
The former occurs when the data contains discriminatory past decisions.
The latter, when historically under-represented groups struggle to compete with the majority under an <em>equal-treatment selection process</em> – a selection process that is `blind’ to the applicant’s protected attribute.
Often, the statistical disparity in the training data, and as a result, in the model’s prediction is a combination of both.</p>
<p>Enforcing <em>Demographic Parity</em> (DP) – an equal fraction of positive outcomes across subgroups – is usually impractical.
In addition to hindering the accuracy of the model’s predictions, this approach will not typically align with anti-discrimination legislation.
A common alternative is to impose an algorithmic fairness constraint that better aligns with the notion of equal treatment, and maintain a disparity in the positive outcome rates <span id="id6">[<a class="reference internal" href="../bib.html#id68">HPS16</a>, <a class="reference internal" href="../bib.html#id92">WMR20</a>]</span>.</p>
<p>Anti-discrimination legislation acknowledges the need to bridge the gap between equal treatment and equal representation.
The Equality Act 2010 (UK) defines <em>positive action</em> as “lawful measures taken to encourage and train people from under-represented groups to help them overcome disadvantages in competing with other applicants”.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>European legislation defines positive action similarly. In the US similar measures can be employed under affirmative action, however, the definition does not completely overlap with positive action.</p>
</div>
</div>
<p>Examples of positive action include, but are not limited to:
additional training opportunities and mentoring programs available to the under-represented group;
targeted advertising, outreach, networking and bursaries.
Policies designed to meet the needs of under-represented groups are also considered positive action.
For example, the European Research Council introduced extensions of eligibility for women with children, in addition to modified CV templates for applicants and anti-bias briefings for the members of evaluation panels.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These measures are included in the European Research Council’s Gender Equality Plan for 2021-2027.</p>
</div>
</div>
<p>The action taken is required to be ‘proportionate’ to both the extent and longevity of the under-representation, and to the barriers the under-represented group experience.</p>
<p>We argue that incorporating the notion of positive action within decision support algorithms can advance the use of these measures, promoting equal representation while respecting anti-discrimination legislation and equal-treatment rights.
In this work we propose a novel algorithmic fairness framework to identify <em>positive action candidates</em> — individuals who were negatively impacted by earlier bias, and hence fail under an equal treatment process. We compare our approach to a baseline of choosing the top rejected candidates from the under-represented group in section <a class="reference internal" href="#sec-compare"><span class="std std-ref">Choosing the right positive action candidates</span></a> (<a class="reference internal" href="#fig-compare"><span class="std std-numref">Fig. 2</span></a>).</p>
<div class="figure align-default" id="fig-story">
<a class="reference internal image-reference" href="../../_images/figure1.png"><img alt="../../_images/figure1.png" src="../../_images/figure1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">A ‘hybrid’ worldview showing biases potentially introduced at each step.
The aptitude is assumed to be independent of all protected attributes, aligning with the WAE worldview.<br />
By the point of observation, however, the construct space might have altered and now  disparity between sub-groups is allowed, aligning with the WYSIWYG worldview. Opportunity bias, selection bias, measurement bias and label bias can introduce or further aggravate the disparity between the protected group and the majority.</span><a class="headerlink" href="#fig-story" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="background">
<span id="sec-background"></span><h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<div class="section" id="definitions">
<span id="sec-background-definitions"></span><h3>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h3>
<p>In this work we discuss sub-groups with respect to <em>protected attributes</em> – characteristics that, by law, must not be the basis for discrimination.
These include, but are not limited to, race, gender, age, religion and disability.
We define a protected subgroup as an under-represented group separated from the majority by a protected attribute.
For example, women in the engineering profession are under-represented when compared to their representation within the population.
In the context of a decision support system, we may observe a statistical disparity — a disproportionate positive outcome (hiring, admissions) rate — in favour of the majority, compared to the protected subgroup.
This can be as a result of the model being trained on past discriminatory decisions, but can also be the result of a statistical disparity in the input features (grades, qualifications).
In this work, we define bias as a mechanism by which statistical disparity between a protected subgroup and the majority emerges or is exacerbated.
Bias within the decision making process will affect the decision outcome.
Bias that occurred prior may affect the features.
To expand the discussion on bias, it is useful to briefly refer to the framework presented in <span id="id7">[<a class="reference internal" href="../bib.html#id67">FSV16</a>]</span>, which defines three spaces – the <em>construct space</em>, <em>observed space</em> and <em>decision space</em> – and uses the mappings between them to formalise several definitions of bias.</p>
<p>The <em>construct space</em> represents the <em>‘ground truth’</em> – an unobserved space that correctly captures differences between individuals with respect to a task;
the <em>observed space</em> represents the measurable features for consideration, and the <em>decision space</em> the outcome <span id="id8">[<a class="reference internal" href="../bib.html#id67">FSV16</a>]</span>.
For example, intelligence resides in the construct space, measured IQ resides in the observed space, and acceptance or rejection from the International Mensa club resides in the decision space.</p>
<p>As the observed space is an estimate of the construct space, we can make certain assumptions regarding the mapping between these spaces.
These assumptions can be referred to as <em>worldviews</em>.
Prior work <span id="id9">[<a class="reference internal" href="../bib.html#id67">FSV16</a>]</span> established the worldviews WAE (“we’re all equal”) and WYSIWYG (“what you see is what you get”), that are considered to be in tension with each other.
WAE assumes that we are all the same in construct space.
If that is not the case in the observed space, this disparity is attributed to structural bias — an incorrect mapping between the construct and the observed space.
WYSIWYG, on the other hand, allows for a disparity between protected subgroups, assuming the observed discrepancies are a true reflection of disparities in the construct space.</p>
<p>To better understand the statistical disparity we may observe in the data, we discuss specific types of bias.
<em>Sample selection bias</em> originates from training on a non-representative sample of the population <span id="id10">[<a class="reference internal" href="../bib.html#id103">Tol19</a>]</span>.
<em>Label bias</em> occurs when the dataset contains past discriminatory decisions <span id="id11">[<a class="reference internal" href="../bib.html#id87">JN20</a>, <a class="reference internal" href="../bib.html#id70">WPT19</a>]</span>.
Mitigation efforts that consider selection bias <span id="id12">[<a class="reference internal" href="../bib.html#id10">KC12</a>, <a class="reference internal" href="../bib.html#id90">ABD+18b</a>]</span>, or label bias <span id="id13">[<a class="reference internal" href="../bib.html#id86">CV10</a>, <a class="reference internal" href="../bib.html#id87">JN20</a>, <a class="reference internal" href="../bib.html#id126">KCQ20</a>]</span> independently are available.
Bias can also be introduced outside of the environment which we can control, i.e. outside the training population, measurements and learning algorithm.
The identification of positive action candidates within decision support systems using the framework presented in this paper aims to acknowledge and mitigate a broader range of biases.
This includes bias that cannot normally be mitigated by an automated rejection / acceptance model while respecting anti-discrimination legislation and the right for equal-treatment.</p>
</div>
<div class="section" id="counterfactual-modelling">
<h3>Counterfactual Modelling<a class="headerlink" href="#counterfactual-modelling" title="Permalink to this headline">¶</a></h3>
<p>To identify positive action candidates, we take a counterfactual approach.
A counterfactual outcome is a hypothetical outcome for a scenario that is identical in all respects except for a specific, well-defined change and its causal consequences <span id="id14">[<a class="reference internal" href="../bib.html#id94">Mil19</a>, <a class="reference internal" href="../bib.html#id95">Hum00</a>]</span>.
In the context of this work, we focus on counterfactual scenarios with respect to a change in a protected attribute, and distinguish between two types of counterfactual questions:</p>
<ol class="simple">
<li><p>Would the outcome change if \textbf{only} the protected attribute was different?</p></li>
<li><p>Would the outcome change if the protected attributed and its \textbf{causal consequences} were different?</p></li>
</ol>
<p>For example, if a female applicant is not invited for a job interview, we can ask the following two questions: if her CV was identical, but the application <em>appeared</em> to be from a male applicant, would she be invited to interview? <span id="id15">[<a class="reference internal" href="../bib.html#id141">BM04</a>]</span>
If she had been <em>born</em> male, experienced life as a male, and then applied for the same job, would she have been invited for an interview?</p>
<p>The second counterfactual question is critical to our approach, as it is used to identify positive action candidates.
We use the first counterfactual question to detect and mitigate label bias.</p>
<p>To evaluate counterfactual outcomes, ideally, we would rely on a Structural Causal Model (SCM) — a graphical model whose vertices represent features and whose edges  represent the <em>causal</em> pathway between them <span id="id16">[<a class="reference internal" href="../bib.html#id31">Pea09</a>]</span>.
The first type of counterfactual question, for example, will manifest in a SCM as a direct pathway between the protected attribute and the outcome.
This SCM-based approach is taken in works related to fairness such as <span id="id17">[<a class="reference internal" href="../bib.html#id73">Chi19</a>, <a class="reference internal" href="../bib.html#id69">KLRS17</a>]</span>.
However, a complete structural model, is challenging to obtain — they are application specific and require specification by domain experts.
In practice, we can find two as-close-as-possible individuals (differing by the protected attribute) within the data (e.g. <span id="id18">[<a class="reference internal" href="../bib.html#id101">RR83</a>, <a class="reference internal" href="../bib.html#id91">Rub90</a>]</span>) or by creating the counterfactual representations using an adversarial learning model (e.g. <span id="id19">[<a class="reference internal" href="../bib.html#id100">GGLRe20</a>, <a class="reference internal" href="../bib.html#id75">MCPZ18b</a>, <a class="reference internal" href="../bib.html#id99">SHDQ20</a>]</span>).
In this paper, we employ the latter approach.</p>
<div class="figure align-default" id="fig-compare">
<a class="reference internal image-reference" href="../../_images/compare_figure.png"><img alt="../../_images/compare_figure.png" src="../../_images/compare_figure.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">How our approach to choosing positive action candidates compares to choosing the top rejected candidates from the under-represented group.
With an equal weight selection role, Applicants A and B have the same overall score.
Re-scaling the minority’s maths grade distribution to match the majority’s distribution highlights applicant B as the better positive action candidate.</span><a class="headerlink" href="#fig-compare" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="related-works">
<h3>Related Works<a class="headerlink" href="#related-works" title="Permalink to this headline">¶</a></h3>
<p>As far as we are aware, this is the first work to address positive action within the context of a decision support system.
However, there are prior works that look at related problems.
We briefly describe the most relevant ones to place the problem of determining positive action candidates in context.</p>
<div class="section" id="deferral">
<h4>Deferral<a class="headerlink" href="#deferral" title="Permalink to this headline">¶</a></h4>
<p>The challenge in learning to defer is identifying which candidates the model is uncertain about.
Once identified, these candidates are referred to a human decision-maker which comes at some cost <span id="id20">[<a class="reference internal" href="../bib.html#id93">MPZ18</a>, <a class="reference internal" href="../bib.html#id132">MS20</a>]</span>.
This poses interesting questions about the practical quantification of uncertainty, and would be a potential extension to our framework.
However, deferment differs from identifying  positive action candidates, as the system we are adapting may be confident in it’s assessment that a candidate who would be suitable for positive action should be rejected.</p>
</div>
<div class="section" id="actionable-recourse">
<h4>Actionable Recourse<a class="headerlink" href="#actionable-recourse" title="Permalink to this headline">¶</a></h4>
<p>Another related field is that of recourse.
Works in this area, such as <span id="id21">[<a class="reference internal" href="../bib.html#id133">JKV+19</a>, <a class="reference internal" href="../bib.html#id135">KScholkopfV21</a>, <a class="reference internal" href="../bib.html#id134">USL19</a>]</span> aim to determine how the world would have had to be different for an alternative outcome to occur.
They aim to explain what would need to change about a rejected candidate for them to be accepted.
Our framework instead asks more direct questions:
If a candidate were perceived to have an alternative protected attribute value, would the outcome be different?
And, would the outcome change if the protected attribute and its causal consequences were different?</p>
</div>
<div class="section" id="auditing-systems">
<h4>Auditing Systems<a class="headerlink" href="#auditing-systems" title="Permalink to this headline">¶</a></h4>
<p>This is a multi-faceted, broad area, but in general auditing aims to evaluate either a dataset <span id="id22">[<a class="reference internal" href="../bib.html#id139">SKS+18</a>]</span>, or a system <span id="id23">[<a class="reference internal" href="../bib.html#id138">KNRW18</a>]</span> for potential bias.
Examples of auditing systems that are similar to ours include <span id="id24">[<a class="reference internal" href="../bib.html#id136">BYF20</a>]</span>.
In their work the authors take an alternative counterfactual approach based on finding the nearest datapoint in the data with a different protected attribute and compare outcomes.
These works differ in motivation as they use their auditing method to look at which <em>groups</em> are most affected, whereas we evaluate which <em>individuals</em> are likely to be affected.</p>
</div>
</div>
</div>
<div class="section" id="approach">
<span id="sec-approach"></span><h2>Approach<a class="headerlink" href="#approach" title="Permalink to this headline">¶</a></h2>
<p>We propose a novel algorithmic fairness framework for advancing equal representation while respecting anti-discrimination legislation and the right for equal treatment:</p>
<ol class="simple">
<li><p>We identify <em>positive action candidates</em>: individuals who were negatively impacted by earlier bias, and hence fail under an equal treatment process.</p></li>
<li><p>Our approach is to use counterfactual fairness to assign applicants into groups:</p>
<ol class="simple">
<li><p>Applicants who would have been successful with any set of perceived protected attributes are assigned a successful outcome;</p></li>
<li><p>Applicants who would have remained unsuccessful with any set of protected attributes are assigned an unsuccessful outcome;</p></li>
<li><p>Unsuccessful applicants that would have been successful if they were born with a different set of protected attributes are flagged as positive action candidates.</p></li>
</ol>
</li>
</ol>
<div class="section" id="positive-action-framework">
<span id="sec-ours"></span><h3>Positive Action Framework<a class="headerlink" href="#positive-action-framework" title="Permalink to this headline">¶</a></h3>
<div class="section" id="fairness-worldview">
<h4>Fairness Worldview<a class="headerlink" href="#fairness-worldview" title="Permalink to this headline">¶</a></h4>
<p>Where does the concept of positive action fit within technical fairness definitions?
If we consider WAE and WYSIWYG, the worldviews discussed in section <a class="reference internal" href="#sec-background-definitions"><span class="std std-ref">Definitions</span></a>, the notion of determining positive action candidates does not fully align with either worldview.
The ‘hybrid’ worldview adopted in this work is illustrated in Figure <a class="reference internal" href="#fig-story"><span class="std std-numref">Fig. 1</span></a> and in Figure <a class="reference internal" href="#fig-our-model"><span class="std std-numref">Fig. 3</span></a> as a graphical model.</p>
<p>We expand  the <em>construct space</em> to include the element of <em>time</em>, with the observed space representing measurements of the construct space over time.
We assume equality was true at some stage, but is not necessarily so at the time of measurement.
We therefore align with WYSIWYG at the point of measurement, and assume that statistical disparity observed within the observed space is, at least partly, due to disparity within the construct space. Positive action is aimed at addressing the disparities within the construct space as we  assume those occur due to imbalance of opportunity or disadvantage affecting individuals belonging to the under-represented group.</p>
<p>To illustrate this, consider a set of measurable features <span class="math notranslate nohighlight">\(X\)</span> within the observed space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> represents the space of all potential feature values.
Each individual sample <span class="math notranslate nohighlight">\(x \in X\)</span> is an approximation to its non-measurable construct-counterpart <span class="math notranslate nohighlight">\(\tilde{x} \in \tilde{X}\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-f842322e-1724-445f-a2a2-bdfd1b5eec26">
<span class="eqno">(4)<a class="headerlink" href="#equation-f842322e-1724-445f-a2a2-bdfd1b5eec26" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  X\approx\tilde{X}=\alpha\cdot\tilde{X}_{apt}+\beta\cdot\Delta\tilde{X}
\end{equation}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \tilde{X}_{apt}\perp S \quad\text{and}\quad
  \Delta\tilde{X} \not \perp S~~,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(S\)</span> is the protected attribute, and <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are non-negative values that sum to 1.
In words, we assume an individual’s suitability for the task, at the time of measurement, is a combination of their aptitude (<span class="math notranslate nohighlight">\(\tilde{X}_{apt}\)</span>), a natural born ability, and their experiences over time (<span class="math notranslate nohighlight">\(\Delta \tilde{X}\)</span>).</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We make no claims regarding the strength of <code class="docutils literal notranslate"><span class="pre">nature'</span> <span class="pre">vs.</span> </code>nurture’. The framework holds for all potential ratios, including either <span class="math notranslate nohighlight">\(\alpha=0\)</span> or <span class="math notranslate nohighlight">\(\beta=0\)</span>.</p>
</div>
</div>
<p>We further assume that the aptitude component, <span class="math notranslate nohighlight">\(\tilde{X}_{apt}\)</span>, is independent of any protected attribute, and hence complies with the WAE worldview.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We are excluding tasks that may correlate with physical attributes, for example, playing professional basketball and height.</p>
</div>
</div>
<p>The ‘life-experience’ component <span class="math notranslate nohighlight">\(\Delta\tilde{X}\)</span>, shifts the aptitude either positively or negatively, and may not be independent of <span class="math notranslate nohighlight">\(S\)</span>.
<span class="math notranslate nohighlight">\(\tilde{X}\)</span> represents the non-observable ‘ground truth’ at the time of measurement, which could be dependent on <span class="math notranslate nohighlight">\(S\)</span>.
A graphical model of our worldview is shown in Figure {numref}`fig:our_model.</p>
<div class="figure align-default" id="fig-our-model">
<a class="reference internal image-reference" href="../../_images/figure2_rev.png"><img alt="../../_images/figure2_rev.png" src="../../_images/figure2_rev.png" style="height: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">The effect of a protected attribute <span class="math notranslate nohighlight">\(S\)</span> on descendants of <span class="math notranslate nohighlight">\(\tilde{\mathcal{X}}_{apt}\)</span> throughout a data-generation procedure.
<span class="math notranslate nohighlight">\(\tilde{\mathcal{X}}\)</span> within the construct space, <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> within the observed space and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> within the decision space.</span><a class="headerlink" href="#fig-our-model" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="underlying-mechanisms-and-bias">
<h4>Underlying mechanisms and bias<a class="headerlink" href="#underlying-mechanisms-and-bias" title="Permalink to this headline">¶</a></h4>
<p>We consider a setting where we observe a statistical disparity between subgroups separated by the value of <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, within both the observed space and the decision space.
The disparity within the decision space may be worse than the disparity within the observed space.
One mechanism that can cause this aggravation is label bias – a direct impact of the protected attribute <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> on the outcome <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> due to past discriminatory decisions within the training dataset.
To achieve equal treatment, the effects of label bias should be eliminated.</p>
<p>The disparity within the observed space can be caused by several mechanisms or their combination:
selection bias occurs when the training set contains a non-representative sample of the population;
measurement bias occurs when the mapping from the construct space to the observed space isn’t as faithful for certain groups or individuals.</p>
<p>Part of the disparity within the observed space can be a true reflection of a disparity within the construct space itself, at the time of measurement.
We assume that the distribution of aptitude <span class="math notranslate nohighlight">\(\mathcal{X}_{apt}\)</span> in the contract space is the same across subgroups.
While a variation in the opportunities available to different individuals is normal, when an imbalance of opportunities affects a protected group more than the majority, it can result in a disparity within the construct space itself.
Addressing this imbalance of opportunities is a principal component of positive action and our framework.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We note that this is not an extensive discussion of bias and there are other underlying mechanisms that can lead to a statistical disparity between an under-represented group and the majority.</p>
</div>
</div>
</div>
</div>
<div class="section" id="positive-action-candidates">
<h3>Positive Action Candidates<a class="headerlink" href="#positive-action-candidates" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="fig-gcm-redux">
<a class="reference internal image-reference" href="../../_images/figure3.png"><img alt="../../_images/figure3.png" src="../../_images/figure3.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text"><em>Top row</em>:
<em>Left</em>: The accepted (<span class="math notranslate nohighlight">\(y=1\)</span>) and rejected (<span class="math notranslate nohighlight">\(y=0\)</span>) ratios difference between a protected group (<span class="math notranslate nohighlight">\(s=0\)</span>) and the majority (<span class="math notranslate nohighlight">\(s=1\)</span>), under an equal-treatment selection rule (WYSIWYG worldview).
<em>Right</em>: The accepted and rejected ratios difference between protected group and the majority when demographic parity is enforced (WAE worldview).
<em>Bottom</em>: Overlapping the above two worldviews.
The population captured by groups <span class="math notranslate nohighlight">\(G_{1}\)</span>, <span class="math notranslate nohighlight">\(G_{2}\)</span>, <span class="math notranslate nohighlight">\(G_{5}\)</span> and <span class="math notranslate nohighlight">\(G_{6}\)</span> have consistent outcome across both worldviews.
Groups <span class="math notranslate nohighlight">\(G_{3}\)</span> and <span class="math notranslate nohighlight">\(G_{4}\)</span> represent individuals that will receive different outcome if a different worldview is assumed.</span><a class="headerlink" href="#fig-gcm-redux" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-wae-wys-thirdoption">
<a class="reference internal image-reference" href="../../_images/model.png"><img alt="../../_images/model.png" src="../../_images/model.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Diagram illustrating our method.
The original representation <span class="math notranslate nohighlight">\(x\)</span> is mapped to a representation <span class="math notranslate nohighlight">\(z\)</span> that is independent of the protected attribute <span class="math notranslate nohighlight">\(s\)</span>.
The invariant representation <span class="math notranslate nohighlight">\(z\)</span> is then mapped back into both <span class="math notranslate nohighlight">\(x_{s_x=0}\)</span> and <span class="math notranslate nohighlight">\(x_{s_x=1}\)</span>, reintroducing biases associated with each subgroup.
Each of those representations is labelled, resulting in four representation in total.
The four corresponding predicted outcomes then determine the group classification according to one of three final outcomes: <em>accept</em>, <em>reject</em>, or <em>disagreement</em> which has two outcomes associated.
Candidates from under-represented groups that were rejected, but would have received a positive outcome in a counterfactual world are <em>flagged for positive action</em>.
Candidates from over-represented groups that were flagged for acceptance, but would <em>not</em> have received a positive outcome in a counterfactual world remain accepted under a ‘no-detriment’ policy.</span><a class="headerlink" href="#fig-wae-wys-thirdoption" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="quantifying-the-difference-between-wae-and-wysiwyg">
<h4>Quantifying the difference between WAE and WYSIWYG<a class="headerlink" href="#quantifying-the-difference-between-wae-and-wysiwyg" title="Permalink to this headline">¶</a></h4>
<p>To quantify the difference between the WAE and the WYSIWYG worldviews we divide the data into six subgroups, as shown in Figure <a class="reference internal" href="#fig-gcm-redux"><span class="std std-numref">Fig. 4</span></a>.
This procedure be done for any pair of fairness metrics and definitions.
We compare positive outcome ratios between an equal-treatment selection rule and demographic parity, metrics associated with WYSIWYG and WAE, respectively.
We conceptually overlay the observed data (Figure <a class="reference internal" href="#fig-gcm-redux"><span class="std std-numref">Fig. 4</span></a>, top left) on a representation of the data with demographic parity enforced (Figure <a class="reference internal" href="#fig-gcm-redux"><span class="std std-numref">Fig. 4</span></a>, top right).
When overlaid, the data can be separated into six subgroups, as shown in Figure <a class="reference internal" href="#fig-gcm-redux"><span class="std std-numref">Fig. 4</span></a>, bottom.
Subgroups <span class="math notranslate nohighlight">\(G_{1}\)</span> and <span class="math notranslate nohighlight">\(G_{2}\)</span> get a positive outcome under both worldviews.
Subgroups <span class="math notranslate nohighlight">\(G_{5}\)</span> and <span class="math notranslate nohighlight">\(G_{6}\)</span>, get a negative outcome under both worldviews.
Subgroups <span class="math notranslate nohighlight">\(G_{3}\)</span> and <span class="math notranslate nohighlight">\(G_{4}\)</span>, however, represent <em>a different outcome under the two worldviews</em>.
Subgroup <span class="math notranslate nohighlight">\(G_{3}\)</span>, represents the subgroup that would have received a positive outcome had demographic parity been enforced, and a negative outcome based on the observed data.
This subgroup may be interpreted as individuals who were negatively impacted by earlier bias, and hence fail under an equal-treatment selection process.
We cannot accept these applicants while aligning with anti-discrimination legislation.
We can highlight them as candidates for positive action — high-potential applicants from an under-represented group whom targeted positive action can help succeed under a equal-treatment selection process.</p>
<table class="colwidths-auto table" id="table-1">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">Selection rules for mapping from the groups represented in Figure~\ref{fig:gcm_redux} and Figure~\ref{fig:wae_wys_thirdoption} to a decision. As <span class="math notranslate nohighlight">\(s=0\)</span> represents an disadvantaged group, we identify those in group 3 to be suitable for \emph{positive action}. These are candidates who would have been accepted had a counterfactual version of themselves been considered. N.b. Combinations not listed are identified and the outcome reverts to the outcome from an unconstrained model.</span><a class="headerlink" href="#table-1" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Selection Rule</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(s\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y_{s_x=0,s_y=0}\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y_{s_x=0,s_y=1}\)</span></p></th>
<th class="text-align:center head"><p>{<span class="math notranslate nohighlight">\(y_{s_x=1,s_y=0}\)</span>}</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y_{s_x=1,s_y=1}\)</span></p></th>
<th class="text-align:center head"><p>Subgroup</p></th>
<th class="text-align:center head"><p>Bias</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td class="text-align:center"><p>0 or 1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{1}\)</span> or <span class="math notranslate nohighlight">\(G_{2}\)</span></p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td class="text-align:center"><p>0 or 1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{1}\)</span> or <span class="math notranslate nohighlight">\(G_{2}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{3}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{4}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{1}\)</span> or <span class="math notranslate nohighlight">\(b_{2}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{4}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{3}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{4}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{3}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{3}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{1}\)</span> or <span class="math notranslate nohighlight">\(b_{2}\)</span></p></td>
<td class="text-align:center"><p>2</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{3}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{3}\)</span></p></td>
<td class="text-align:center"><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{1}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(b_{3}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td class="text-align:center"><p>0 or 1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_{5}\)</span> or <span class="math notranslate nohighlight">\(G_{6}\)</span></p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="choosing-the-right-positive-action-candidates">
<span id="sec-compare"></span><h4>Choosing the right positive action candidates<a class="headerlink" href="#choosing-the-right-positive-action-candidates" title="Permalink to this headline">¶</a></h4>
<p>Demographic parity is a group fairness measure that compares the ratios of positive outcome rates between subgroups.
We still need to identify which applicants we want to highlight as positive action candidates.
Our approach is to use counterfactual fairness to identify the most suitable applicants.
We highlight unsuccessful applicants that would have been successful if they were born with a different value of <span class="math notranslate nohighlight">\(S\)</span>, the protected attribute.</p>
<p>The reader might now consider the merit of this approach compared to the more straightforward ‘baseline’ approach of highlighting the top rejected candidates from the under-represented group.
This baseline is only applicable when there is a clear way to rank candidates and does not account for two potential issues: measurement bias and an uneven dispersion of disparity amongst the input features.
We illustrate these two issues with the following motivating example:</p>
<p>Consider a minority who traditionally sends their children to schools that teach English to a good level but teaches mathematics only to a basic level.
This minority is under-represented within STEM subjects.
A good level of mathematical ability is crucial for succeeding in STEM university courses and a high grade in a Maths exam is a key part of the selection process.
To keep this example simple, we consider the application to consist of grades in only two subjects, Maths and English, with equal weight.
Blindly taking the best rejected applicants will not spot the applicants who did exceptionally well in Maths, considering the poor education they received in this subject.
In our approach, the minority’s Maths grade distribution gets re-calibrated to match the majority’s distribution, while the distribution of the English grades is left unaffected because there is no disparity with the majority’s distribution.
This means that a minority applicant who is good in Maths, relative to their minority subgroup, will be preferred compared to one that is relatively good in English.
Figure <a class="reference internal" href="#fig-compare"><span class="std std-numref">Fig. 2</span></a> illustrates how two applicants would be ranked under our approach compared the baseline of choosing the top rejected candidates.
For the majority, the distribution ranges between 0-10 for both English and maths.
For the minority, the English distribution ranges between 1-10 but the maths distribution only ranges between 1-5.
Applicant A’s grades are 2 and 9 in maths and English, respectively.
Applicant B’s grades are 5 and 6 in maths and English, respectively.
With an equal weight selection rule, they both have an overall score of 11.
When we re-scale the maths grade distributions of the minority to match the majority’s distribution, applicant B is highlighted as the better positive action candidate with an overall score of 16 compared to 13 for applicant B.
This re-calibration is only put into effect when populating the positive action candidates group.
When applicants are considered to decide if they should receive an accept outcome, the features are taken as they are.
In the case of this example, we may not be able to accept applicant B, but they are flagged as a positive action candidate — a maths foundation course, for example, is likely to allow them to successfully compete in a subsequent selection process.</p>
</div>
</div>
<div class="section" id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="building-a-group-classifier">
<h4>Building a group classifier.<a class="headerlink" href="#building-a-group-classifier" title="Permalink to this headline">¶</a></h4>
<p>To identify which candidates may benefit most from positive action we use a general two-step approach following the scheme in Figure <a class="reference internal" href="#fig-wae-wys-thirdoption"><span class="std std-numref">Fig. 5</span></a>.
Our aim is to produce, with respect to a protected attribute, both counterfactual samples; and counterfactual decisions.
The first accounts for differences in the features.
The latter accounts for decisions that are potentially discriminatory (e.g. past positive-discrimination attempts).</p>
<p>We use an approach from fair representation literature to make a representation of the data that, as best possible, is invariant to <span class="math notranslate nohighlight">\(S\)</span>.
First, we train an adversarial autoencoder model that maps the observed data point <span class="math notranslate nohighlight">\(x\)</span> from the dataset <span class="math notranslate nohighlight">\(X\)</span> into a latent representation <span class="math notranslate nohighlight">\(z \in \mathcal{Z}\)</span> (where <span class="math notranslate nohighlight">\(\mathcal{Z}=\mathbb{R}^{N_z}\)</span>), that is independent of the protected attribute, <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the set of possible protected attribute values. For example, <span class="math notranslate nohighlight">\(\mathcal{S} = \{0, 1\}\)</span> if the protected attribute is binary.
From an instance <span class="math notranslate nohighlight">\(z\)</span>, two mirror representations can be created, <span class="math notranslate nohighlight">\(x_{s_x=0}\)</span> and <span class="math notranslate nohighlight">\(x_{s_x=1}\)</span>.
The variables <span class="math notranslate nohighlight">\(x_{s_x=0}\)</span> and <span class="math notranslate nohighlight">\(x_{s_x=1}\)</span> are then labelled by concatenating the perceived protected attribute to the covariates, creating four representation in total: <span class="math notranslate nohighlight">\(x_{s_x=0,s_y=0}\)</span>, <span class="math notranslate nohighlight">\(x_{s_x=0,s_y=1}\)</span>, <span class="math notranslate nohighlight">\(x_{s_x=1,s_y=0}\)</span> and <span class="math notranslate nohighlight">\(x_{s_x=1,s_y=1}\)</span>.
Here, <span class="math notranslate nohighlight">\(S_y\)</span> denotes the value of the protected attribute concatenated to the set of covariates, adding a direct path in the data to <span class="math notranslate nohighlight">\(S\)</span>.
A classifier can use this value directly if it is indeed the basis of a decision, rather than extracting the protected attribute from the remaining features.</p>
<p>For the second step, we train a second model, a shared classifier to perform predictions on the counterfactual representations.
We then feed in the counterfactuals and get a corresponding set of outputs:
<span class="math notranslate nohighlight">\(y_{s_x=0,s_y=0}\)</span>, <span class="math notranslate nohighlight">\(y_{s_x=0,s_y=1}\)</span>, <span class="math notranslate nohighlight">\(y_{s_x=1,s_y=0}\)</span> and <span class="math notranslate nohighlight">\(y_{s_x=1,s_x=1}\)</span>.
With this knowledge, the Outcome Comparator then sorts the set of original datapoints <span class="math notranslate nohighlight">\(X\)</span> into one of six subgroups <span class="math notranslate nohighlight">\(G_{1-6}\)</span>.
The full selection rules are presented in Table <a class="reference internal" href="#table-1"><span class="std std-numref">Table 3</span></a>, but we give some intuition:</p>
<p>Groups 1 &amp; 2 (<span class="math notranslate nohighlight">\(G_{1,2}\)</span>) consist of candidates whose outcomes were either unanimously accepted across all counterfactual inputs (selection rule 1), or differed due to <span class="math notranslate nohighlight">\(S_y\)</span>, the concatenated <em>perceived</em> protected attribute, changing (selection rules 2 &amp; 8).
Unanimous <em>negative</em> outcomes for all counterfactual inputs are assigned to groups <span class="math notranslate nohighlight">\(G_{5,6}\)</span> (selection rule 9).
Lastly, applicants who receive a disagreement amongst the outcomes, i.e. their outcome depends on the value of <span class="math notranslate nohighlight">\(S_x\)</span>, are assigned to groups <span class="math notranslate nohighlight">\(G_{3,4}\)</span> (selection rules 3-7).
Members of group <span class="math notranslate nohighlight">\(G_4\)</span> are accepted as they would by an unconstrained classifier as our positive action approach has no-detriment to the over-represented group.
Members of group <span class="math notranslate nohighlight">\(G_{3}\)</span> are highlighted as <em>positive action candidates</em>.</p>
</div>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>Our model is implemented as two successive neural networks representing distinct phases as mentioned above.</p>
<p>The adversarial autoencoder has a similar architecture to <span id="id25">[<a class="reference internal" href="../bib.html#id52">MCPZ18a</a>]</span>, with multiple decoders <span id="id26">[<a class="reference internal" href="../bib.html#id129">PCC+21</a>, <a class="reference internal" href="../bib.html#id128">SJS17</a>]</span>, and comprises:</p>
<ol class="simple">
<li><p>An encoder function <span class="math notranslate nohighlight">\(g: (\mathcal{X}, \mathcal{S}) \to \mathcal{Z}\)</span> to map the input <span class="math notranslate nohighlight">\(x\)</span> to a more malleable representation <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
<li><p>An Adversary function <span class="math notranslate nohighlight">\(h:\mathcal{Z} \to \mathcal{S} \)</span> to encourage the representation in the latent space to \emph{not} be predictive of <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>An ensemble of <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>-specific decoders.
The task is to produce a reconstruction <span class="math notranslate nohighlight">\(x_s\)</span> from <span class="math notranslate nohighlight">\(z\)</span> and is defined as a function <span class="math notranslate nohighlight">\(k: (\mathcal{Z}) \to \mathcal{X}_s \quad \forall ~ s \in \mathcal{S}\)</span>.
Where <span class="math notranslate nohighlight">\(\mathcal{X}_s\)</span> is an array of reconstructions, each corresponding to a possible <span class="math notranslate nohighlight">\(s\)</span>-value.
During training, <span class="math notranslate nohighlight">\(\mathcal{X}_s\)</span> is indexed by the real <span class="math notranslate nohighlight">\(s\)</span> value so that only the <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>-head that corresponds to the true protected attribute is used for training.</p></li>
</ol>
<p>The encoder’s purpose is to produce a <em>likely</em> counterfactual <span class="math notranslate nohighlight">\(X\)</span> with respect to <span class="math notranslate nohighlight">\(S\)</span>.
To do this, we produce a latent embedding, <span class="math notranslate nohighlight">\(Z\)</span> which removes as much information about <span class="math notranslate nohighlight">\(S\)</span> as possible.
Then, we have one decoder-head per possible <span class="math notranslate nohighlight">\(S\)</span>-label, allowing the effect of <span class="math notranslate nohighlight">\(s\)</span> to be reintroduced.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This could be performed with a conditional decoder that additionally accepts the protected attribute as input, but in practice, we found our approach to work more consistently.</p>
</div>
</div>
<p>We train this model by optimising the objective function in Equation <span class="xref std std-ref">eq:ae_loss</span>, where <span class="math notranslate nohighlight">\(\ell_{\textrm{recon}}\)</span> is an appropriate loss between the reconstructions and the features, and <span class="math notranslate nohighlight">\(\ell_{\textrm{adv}}\)</span> is the adversarial loss realised as cross-entropy between the predicted and target <span class="math notranslate nohighlight">\(S\)</span> coupled with a supplementary non-parametric measure (Maximum Mean Discrepancy <span id="id27">[<a class="reference internal" href="../bib.html#id81">GBR+07</a>]</span>) with a linear kernel between the embeddings per group (i.e. <span class="math notranslate nohighlight">\(\textrm{MMD}(Z_{s=0}, Z_{s=1})\)</span> ).
A hyper-parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is incorporated to allow for a trade-off between the two competing losses.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In our experiments, we use <span class="math notranslate nohighlight">\(\lambda = 1.0\)</span></p>
</div>
</div>
<div class="amsmath math notranslate nohighlight" id="equation-4c2e8945-0736-4bb8-9d3b-095db573f3b6">
<span class="eqno">(5)<a class="headerlink" href="#equation-4c2e8945-0736-4bb8-9d3b-095db573f3b6" title="Permalink to this equation">¶</a></span>\[\begin{align} \nonumber \label{eq:ae_loss}
    \mathcal{L}_{\mathrm{AE}} = &amp; \min_{\theta, \pi}\max_{\phi} \mathbb{E}_{x \sim X} [ \ell_{\textrm{recon}}(k_{\pi}(g_{\theta}(x), s)_s; x) \nonumber \\
    &amp;\qquad\qquad\qquad\quad - \lambda \ell_{\textrm{adv}}(h_{\phi}(g_{\theta}(x)), s)] 
\end{align}\]</div>
<p>The classification model consists of a shared network with, in a similar fashion to the autoencoder, <span class="math notranslate nohighlight">\(S\)</span>-specific task-heads.
This is to capture any potential direct discrimination that the model determines to exist based on past data.
For the classification model the task is to produce an ensemble of predictions of the class label <span class="math notranslate nohighlight">\(y_s\)</span> from <span class="math notranslate nohighlight">\(x\)</span> and is defined as <span class="math notranslate nohighlight">\(f_s: (\mathcal{X}) \rightarrow \mathcal{Y}_s \quad \forall ~ s \in \mathcal{S}\)</span>.
As with the autoencoder, only the <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>-head that corresponds to the true protected label is used for training.
The objective is shown in the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-697dda26-da1a-4581-9ab8-16c662118d42">
<span class="eqno">(6)<a class="headerlink" href="#equation-697dda26-da1a-4581-9ab8-16c662118d42" title="Permalink to this equation">¶</a></span>\[\begin{equation} \label{eq:clf_loss}
    \mathcal{L}_{\mathrm{Clf}} = \min_{\omega, \xi} \mathbb{E}_{x \sim X}[ \ell_{\textrm{pred}}(f_{\omega}(x)_s; y) ]
\end{equation}\]</div>
<p>At inference time, the autoencoder model produces one reconstruction per <span class="math notranslate nohighlight">\(S\)</span>-label, per sample, and likewise for the classification model.
In the case of a binary <span class="math notranslate nohighlight">\(S\)</span>\label, this produces two reconstructions per sample and two decisions per reconstruction, resulting in 4 outcomes per sample.</p>
<div class="figure align-default" id="fig-syngen2">
<a class="reference internal image-reference" href="../../_images/figure6.png"><img alt="../../_images/figure6.png" src="../../_images/figure6.png" style="height: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Changes in the engineered synthetic data.
Starting from a uniform distribution, we visualise how the additive effect of bias can result in a significant disproportion of success between groups differing by a protected attribute.
The opportunity bias and measurement bias are modelled as a shift between the distributions.
The label bias is modelled by having different acceptance thresholds for the different groups (vertical dashed lines in the right figure).</span><a class="headerlink" href="#fig-syngen2" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
<div class="section" id="experiments">
<span id="sec-demo"></span><h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
<p>We first use synthetic data to demonstrate how our approach can be applied to a candidate-filtering task within a biased setting.
We consider applicants to a university course in a fictitious world that is inhabited by <em>blue</em> and <em>green</em> people, such that we take a person’s <em>colour</em> as the protected attribute.
This university course is for a traditionally <em>blue</em> profession, rendering the setting potentially biased.
The department receives applications from many more promising <em>blue</em> candidates than from promising <em>green</em> candidates.</p>
<p>We then demonstrate our approach on the UCI Adult Income dataset, and use it to highlight potential challenges in real-world deployment.</p>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<div class="section" id="synthetic-data-generation">
<span id="sec-synthetic-generation"></span><h4>Synthetic Data Generation<a class="headerlink" href="#synthetic-data-generation" title="Permalink to this headline">¶</a></h4>
<p>We define a data generation procedure for a dataset with binary <span class="math notranslate nohighlight">\(S\)</span>-labels and a binary outcome, with <span class="math notranslate nohighlight">\(2\)</span> imperfect observers of <span class="math notranslate nohighlight">\(3\)</span> features, making a feature-space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> comprising 6 features. Full details are in Appendix <span class="xref std std-ref">app:synthetic</span>.</p>
<p>We first draw samples for <span class="math notranslate nohighlight">\(S\)</span> from a Bernoulli distribution, and model the underlying construct as a Uniform distribution (Figure <a class="reference internal" href="#fig-syngen2"><span class="std std-numref">Fig. 6</span></a>(i)) — this is where the WAE worldview is applied, as <span class="math notranslate nohighlight">\(\tilde{X}_{apt}\)</span> is independent of <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    S \sim \mathcal{B}(0.5) \quad\text{and}\quad
    \tilde{X}_{apt} \sim \mathcal{U}(0,1)~~.
\end{align*}\]</div>
<p>To represent the imbalance of opportunity between the groups, for example, due to variation in parental support between blue and green parents, we map from the uniform distribution to an <span class="math notranslate nohighlight">\(S\)</span>-conditioned distribution for each feature using an inverse-CDF (percent point) function, <span class="math notranslate nohighlight">\(\tilde{X}_{apt}\)</span> to <span class="math notranslate nohighlight">\(\tilde{X}\)</span>.
This mapping is captured by <span class="math notranslate nohighlight">\(\Delta\tilde{X}_{s=0,1}\)</span> (Figure <a class="reference internal" href="#fig-syngen2"><span class="std std-numref">Fig. 6</span></a>(ii)).</p>
<p>The features <span class="math notranslate nohighlight">\(\tilde{X}_{s=0,1}\)</span> are still in the construct space, representing the potential to  successfully graduate from the university course at the point of applying.
The mapping between <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and <span class="math notranslate nohighlight">\(X\)</span> is made of two noisy observations for each feature.
A measurement bias further aggravates the disparity between the blue and green distributions (Figure <a class="reference internal" href="#fig-syngen2"><span class="std std-numref">Fig. 6</span></a>(iii)).
We then generate two outcome scores:</p>
<ol class="simple">
<li><p>An ‘acceptance score’ based on a linear combination of the observed features.
When mapping from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>, from the observed to the decision space, we add a label bias by setting different acceptance thresholds depending on the value of <span class="math notranslate nohighlight">\(S\)</span> (Figure <a class="reference internal" href="#fig-syngen2"><span class="std std-numref">Fig. 6</span></a>(iv)).</p></li>
<li><p>A ‘graduation grade’ based on a linear combination of the features in <span class="math notranslate nohighlight">\(\tilde{X}\)</span>, bypassing the effect of the introduced measurement bias and label bias.</p></li>
</ol>
</div>
<div class="section" id="uci-adult-income-data">
<h4>UCI Adult Income Data<a class="headerlink" href="#uci-adult-income-data" title="Permalink to this headline">¶</a></h4>
<p>We evaluate our approach on the UCI Adult Income Dataset, which is often used for evaluating fairness-enhancing systems.
This dataset comprises <span class="math notranslate nohighlight">\(45,222\)</span> samples from the 1994 U.S. census with <span class="math notranslate nohighlight">\(14\)</span> features including occupation, maximum attained education level and relationship status.
Of these <span class="math notranslate nohighlight">\(14\)</span>, we reserve the binary <code class="docutils literal notranslate"><span class="pre">salary</span></code> feature as the target label, with <code class="docutils literal notranslate"><span class="pre">&gt;$50K</span></code> as the positive outcome.
We consider <span class="math notranslate nohighlight">\(3\)</span> binary features as protected attributes: sex (Male/Female), race (White/Not White) and marital status (Married/Not Married).</p>
<p>\subsection{Evaluation}
To evaluate our model in context, we train the following models on the synthetic data:
a Demographic Parity Oracle, \texttt{DemPar}, enforcing exact Demographic Parity; an unconstrained \texttt{Logistic Regression} model;
established fair classification models
\texttt{K &amp; C Reweighting} \cite{KamCal12},
\texttt{Kamishima} \cite{KamAkaAso2012} and
\texttt{FairLearn} \cite{AgaBeyDudLanetal18};
and our positive action approach using counterfactual modelling, which we refer to as \texttt{PAF} (Positive Action Framework).</p>
<p>We define the following metrics:</p>
<p><em>Acceptance percentage per colour</em> (Acceptance). When this is equalised across groups, demographic parity is satisfied.
\begin{equation}
\textrm{Acceptance}(s)=P(Y=1|S=s) \quad \forall s \in \mathcal{S}
\end{equation}
With <span class="math notranslate nohighlight">\(Y=1\)</span> being the ‘accepted’ outcome.</p>
<p><em>True Capture percentage</em> (TCP). This captures the rate of applicants with the ability to graduate, that are not rejected:
\begin{align}
\textrm{TCP}(s)=P(Y\in{1, 2}|S=s, G=1) \quad \forall s \in \mathcal{S} %\nonumber
\end{align}
With <span class="math notranslate nohighlight">\(Y=2\)</span> being the `positive action candidate’ outcome.</p>
<p><em>False Identification Difference</em> (FID) measures the level of `Equality of Opportunity’ (EqOP), i.e. once a candidate is accepted, does their chance of graduating depend on the protected attribute? It is calculated as:<br />
\begin{equation}
\textrm{FID}=|P(G=0|S=1, Y=1) - P(G=0|S=0, Y=1)|
\end{equation}</p>
<p><em>Accuracy</em>. We evaluate the utility of the model with regard to both <span class="math notranslate nohighlight">\(Y\)</span>, predicting a proxy-label based on the best assumptions from the data; and <span class="math notranslate nohighlight">\(G\)</span>, predicting the obscured ‘true’ outcome.
\begin{align}
&amp; \textrm{Accuracy}(y)=P(\text{prediction}=y) \quad \forall y \in \mathcal{Y} \
&amp; \textrm{Accuracy}(g)=P(\text{prediction}=g) \quad \forall g \in \mathcal{G}
\end{align}</p>
<p>\begin{table*}[ht]
\caption{Comparison table for the synthetic data results.
Oracle values show the desired values for the WAE worldview (\texttt{DemPar}).
The best result (excluding oracle values) is highlighted in \textbf{boldface}.
Our positive action framework model (\texttt{PAF}) captures 97% (see TCP|G row) of the green applicants capable of graduating: they are either accepted or flagged as positive action candidates.
This high TCP value is achieved while maintaining low FID (as per WYSIWYG worldview).
}</p>
<table class="colwidths-auto table" id="table-results">
<caption><span class="caption-number">Table 4 </span><span class="caption-text">Comparison table for the synthetic data results. Oracle values show the desired values for the WAE worldview (\texttt{DemPar}). The best result (excluding oracle values) is highlighted in \textbf{boldface}. Our positive action framework model (\texttt{PAF}) captures 97% (see TCP|G row) of the green applicants capable of graduating: they are either accepted or flagged as positive action candidates. This high TCP value is achieved while maintaining low FID (as per WYSIWYG worldview).</span><a class="headerlink" href="#table-results" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:center head"><p></p></th>
<th class="text-align:center head"><p>Theoretical Values</p></th>
<th class="text-align:center head"><p></p></th>
<th class="text-align:center head"><p>Unconstrained models</p></th>
<th class="text-align:center head"><p></p></th>
<th class="text-align:center head"><p>Fair models</p></th>
<th class="text-align:right head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Metric</p></td>
<td class="text-align:center"><p>\texttt{DemPar}</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>\texttt{Logistic Regression}</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>\texttt{Fairlearn}</p></td>
<td class="text-align:right"><p>\texttt{PAF} (ours)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Acceptance|B</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(23.12 \pm 16.31\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(34.54 \pm 0.79\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(26.65 \pm 1.17\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(\bf{35.15} \pm \bf{1.36}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Acceptance|G</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(23.13 \pm 16.13\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(5.43 \pm 0.52\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\bf{7.83} \pm \bf{0.52}\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(6.04 \pm 0.60\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>TCP|B <span class="math notranslate nohighlight">\(\uparrow\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(60.57 \pm 42.80\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(91.70 \pm 1.92\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(71.51 \pm 3.72\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(\bf{92.63} \pm \bf{2.08}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>TCP|G <span class="math notranslate nohighlight">\(\uparrow\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(69.83 \pm 6.03\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(69.84 \pm 5.00\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(53.17 \pm 6.02\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(\bf{96.74} \pm \bf{1.93}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>FIDiff <span class="math notranslate nohighlight">\(\downarrow\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(5.55 \pm 6.58\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.97 \pm 0.44\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(4.42 \pm 0.82\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(\bf{0.86} \pm \bf{0.43}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Accuracy(Y) <span class="math notranslate nohighlight">\(\uparrow\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(84.50\pm 0.43\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(92.64 \pm 0.41\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\bf{98.51} \pm \bf{0.18} \)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(98.31 \pm 0.26\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Accuracy(G) <span class="math notranslate nohighlight">\(\uparrow\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(79.25 \pm 9.48\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\bf{87.05} \pm \bf{0.46}\)</span></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(86.16 \pm 0.44\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(86.67 \pm 0.47\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="results-discussion">
<span id="sec-conclusion"></span><h2>Results &amp; Discussion<a class="headerlink" href="#results-discussion" title="Permalink to this headline">¶</a></h2>
<div class="section" id="analysing-the-baseline-synthetic-data">
<h3>Analysing the baseline synthetic data<a class="headerlink" href="#analysing-the-baseline-synthetic-data" title="Permalink to this headline">¶</a></h3>
<p>Results comparing the models found in Table <a class="reference internal" href="#table-results"><span class="std std-numref">Table 4</span></a>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For ease of comparison, we choose to omit the results for <code class="docutils literal notranslate"><span class="pre">K&amp;C</span> <span class="pre">Reweighting</span></code> and <code class="docutils literal notranslate"><span class="pre">Kamishima</span></code> from the table and only report <code class="docutils literal notranslate"><span class="pre">FairLearn</span></code>, the model with the highest acceptance percentage for green applicants.</p>
</div>
</div>
<p>The baseline data was engineered to demonstrate a biased setting: only <span class="math notranslate nohighlight">\(4\%\)</span> of the <em>green</em> candidates are admitted, in comparison to <span class="math notranslate nohighlight">\(36\%\)</span> of the <em>blue</em> candidates.
We can evaluate the True Capture percentage (TCP): how many candidates with the ability to graduate, are not being rejected.
The potential to graduate, <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, represents the ‘ground truth’ potential and correlates directly to <span class="math notranslate nohighlight">\(\mathcal{\tilde{X}}\)</span>.
While for blue the TCP is at a high <span class="math notranslate nohighlight">\(94\%\)</span>, the green TCP is only <span class="math notranslate nohighlight">\(60\%\)</span>.
The False Identification difference (FID) measures how well the data conforms to equality of opportunity.
A low FID, <span class="math notranslate nohighlight">\(0.4\%\)</span>, means the data conforms to EqOP: once a candidate is accepted, the likelihood of graduation is nearly the same for both groups.</p>
<div class="section" id="demographic-parity-oracle">
<h4>Demographic Parity Oracle<a class="headerlink" href="#demographic-parity-oracle" title="Permalink to this headline">¶</a></h4>
<p>When enforcing demographic parity on the model, the metrics change substantially.
Acceptance percentage is now equal between the <em>blue</em> and <em>green</em> candidates, but at a cost: the TCP for the blue has gone down to <span class="math notranslate nohighlight">\(61\%\)</span> and the FID is up to <span class="math notranslate nohighlight">\(5.5\%\)</span>, meaning there is a higher likelihood of graduating once accepted is no longer independent of <span class="math notranslate nohighlight">\(S\)</span>.</p>
</div>
<div class="section" id="logistic-regression-model">
<h4>Logistic Regression model<a class="headerlink" href="#logistic-regression-model" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Logistic</span> <span class="pre">Regression</span></code> gives similar results to the baseline data.
We use it as a baseline model for the comparison with subsequent models.</p>
<p><code class="docutils literal notranslate"><span class="pre">FairLearn</span></code> achieved the second best acceptance rate for green applicants, after the DP oracle.
Similar to the DP oracle, however, the additional green applicants who get accepted are not the ones capable of graduating.</p>
</div>
<div class="section" id="positive-action-framework-model">
<h4>Positive Action Framework Model<a class="headerlink" href="#positive-action-framework-model" title="Permalink to this headline">¶</a></h4>
<p>Our <code class="docutils literal notranslate"><span class="pre">PAF</span></code> model shows a <span class="math notranslate nohighlight">\(40\%\)</span> improvement in the acceptance rate of green applicants, compared to the engineered baseline data.
These additional candidates were flagged by our model as falling victim to label bias, i.e. they would have been accepted if they were <em>perceived</em> as blue.
They are reassigned to ‘accept’ by the group classifier (selection rule 8 in Table <a class="reference internal" href="#table-1"><span class="std std-numref">Table 3</span></a>).
Unlike the DP oracle (<code class="docutils literal notranslate"><span class="pre">DemPar</span></code>) and <code class="docutils literal notranslate"><span class="pre">FairLearn</span></code>, the FID remains under <span class="math notranslate nohighlight">\(1\%\)</span>, as these additional accepted green applicants are capable of graduating.
A more notable success, in comparison to the other models, is the high TCP combined with a low FID.
The addition of the positive action candidate outcome increases the percentage of applicants capable of graduating that are not rejected from <span class="math notranslate nohighlight">\(53\%\)</span> to <span class="math notranslate nohighlight">\(97\%\)</span>.
This outcome does not come at the expense of equality of opportunity, as the accepted applicants have a similar chance of graduating with a good grade if they were accepted.</p>
<p>The positive action candidate outcome enables us to not simply reject high-potential candidates from under-represented groups, even if we are not able to accept them under an equal-treatment selection process.
Equal treatment and equality of opportunity are both maintained for accepted applicants.</p>
<p>Table <a class="reference internal" href="#table-subgroups"><span class="std std-numref">Table 5</span></a> shows the breakdown of the outcome groups, in respect to all the candidates, produced by our <code class="docutils literal notranslate"><span class="pre">PAF</span></code> model.
This is compared to the outcome groups of a ‘perfect’ counterfactual (<code class="docutils literal notranslate"><span class="pre">PCF</span></code>).
The <code class="docutils literal notranslate"><span class="pre">PCF</span></code> is the values we expect if both the encoder and classifier performed without error.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These values assume a consistent decision rule across all populations</p>
</div>
</div>
<p>We can see that the majority of the candidates receive a counterfactual consensus in both <code class="docutils literal notranslate"><span class="pre">PCF</span></code> and <code class="docutils literal notranslate"><span class="pre">PAF</span></code>.
<code class="docutils literal notranslate"><span class="pre">PAF</span></code> underestimates the consensus by an overall <span class="math notranslate nohighlight">\(7\%\)</span> when compared to the <code class="docutils literal notranslate"><span class="pre">PCF</span></code>, having larger <span class="math notranslate nohighlight">\(G_3\)</span> and <span class="math notranslate nohighlight">\(G_4\)</span> at the expense of the consensus subgroups.
In this example, this means the <code class="docutils literal notranslate"><span class="pre">PCF</span></code> assigns rejection for <span class="math notranslate nohighlight">\(3\%\)</span> more candidates then the <code class="docutils literal notranslate"><span class="pre">PAF</span></code>.</p>
<table class="colwidths-auto table" id="table-subgroups">
<caption><span class="caption-number">Table 5 </span><span class="caption-text">Comparison of the number of samples allocated to each group for a ground truth Perfect Counterfactual (<span class="math notranslate nohighlight">\(\texttt{PCF}\)</span>) in comparison to the Learned Counterfactuals of our Positive Action Framework (<span class="math notranslate nohighlight">\(\texttt{PAF}\)</span>). We are able to make this comparison only because we know the ground truth for the synthetic data. Our learned model (\texttt{PAF}) is, in general, in agreement with the ground truth (\texttt{PCF}).</span><a class="headerlink" href="#table-subgroups" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Subgroup</p></th>
<th class="text-align:center head"><p>Outcome</p></th>
<th class="text-align:center head"><p><code class="docutils literal notranslate"><span class="pre">PCF</span></code></p></th>
<th class="text-align:center head"><p><code class="docutils literal notranslate"><span class="pre">LCF</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(g_{1}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(4.73 \pm 0.15\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(3.01 \pm 0.28\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(g_{2}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(4.73 \pm 0.09\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(2.40 \pm 0.48\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(g_{3}\)</span></p></td>
<td class="text-align:center"><p>2</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(10.99 \pm 0.19\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(13.77 \pm 2.08\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(g_{4}\)</span></p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(10.96 \pm 0.25\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(15.27 \pm 0.91 \)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(g_{5}\)</span></p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(34.35 \pm 0.26\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(32.97 \pm 2.86\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(g_{6}\)</span></p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(34.23 \pm 0.28\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(32.59 \pm 0.86\)</span></p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="fig-adult-results">
<a class="reference internal image-reference" href="../../_images/adult_figure.png"><img alt="../../_images/adult_figure.png" src="../../_images/adult_figure.png" style="height: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Breakdown of group allocations on the withheld test set of the UCI Adult dataset averaged over 10 repeats, using 3 values as the protected attribute.
<em>Left</em>: The binary ‘sex’ feature.
<em>Middle</em>: The ‘race’ feature binarised to membership of the majority group (white).
<em>Right</em>: The ‘marital status’ feature binarised to whether currently married.
In all cases, the x-axis represents the percentage of the data that belongs to each protected attribute, while the y-axis represents the percentage of the population assigned each outcome.
Group membership is defined in Table <a class="reference internal" href="#table-1"><span class="std std-numref">Table 3</span></a>.
For all attributes, subgroups <span class="math notranslate nohighlight">\(G_{3}\)</span> and <span class="math notranslate nohighlight">\(G_{4}\)</span> highlight the proportion of the population for which intervening on the protected attribute will result in the outcome changing as well.
In <span class="math notranslate nohighlight">\(G_{3}\)</span> the outcome changes from negative to positive when <span class="math notranslate nohighlight">\(S\)</span> changes, while changes in <span class="math notranslate nohighlight">\(G_{4}\)</span> result in the opposite outcome.
Although an intriguing visualisation of the effect of the different attributes, conclusions should be drawn carefully as the attributes can act as a proxy to hidden patterns in the data.
Further discussion can be found in Section <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec:UCI_results</span></code>.</span><a class="headerlink" href="#fig-adult-results" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="auditing-the-uci-adult-income-for-bias">
<span id="sec-uci-results"></span><h3>Auditing the UCI Adult Income for Bias<a class="headerlink" href="#auditing-the-uci-adult-income-for-bias" title="Permalink to this headline">¶</a></h3>
<p>Figure <a class="reference internal" href="#fig-adult-results"><span class="std std-numref">Fig. 7</span></a> shows a counterfactual subgroup analysis for <span class="math notranslate nohighlight">\(3\)</span> protected attributes within the UCI Adult Income data set.
We note that the accuracy of the PAF model is on par with baseline models.
As before, the individuals within subgroups <span class="math notranslate nohighlight">\(G_{4}\)</span> and <span class="math notranslate nohighlight">\(G_{3}\)</span> did not achieve counterfactual consensus.
For example, for sex, the subgroup <span class="math notranslate nohighlight">\(G_{4}\)</span> contains males that are above the <span class="math notranslate nohighlight">\(\$50,000\)</span> threshold, but their female counterfactual counterparts would be under the threshold, whereas <span class="math notranslate nohighlight">\(G_{3}\)</span> captures females under the threshold whose male counterfactual counterparts would be above the threshold.
When comparing the effects of the <span class="math notranslate nohighlight">\(3\)</span> protected attributes we examined, we can see that changing the marital status is most likely to result in a change the outcome.
That alone, however, is not enough to deduce a causal relationship between marital status and salary.
Martial status might have a direct impact on salary but it is also a proxy to other relevant attributes such as age, for example.
Similarly, the effect of sex can be a combination of a direct effect and a proxy effect from additional relevant attributes such as occupation type and working hours.
The effect of race may seem to be the least influential of the <span class="math notranslate nohighlight">\(3\)</span>, but this can be misleading as we can’t quantify the contributions of proxy effects on marital status and sex.</p>
<p>Employing our approach on the adult dataset highlights some important challenges and choices:
Protected attributes may be correlated to other attributes that are relevant to the task.
These can add proxy effects and mask the direct effect of the protected attribute.
We can choose to leave the re-calibration of features unrestricted or to keep some features as they were originally.
The latter may impact the quality of counterfactual representations we can achieve <span id="id28">[<a class="reference internal" href="../bib.html#id89">PRT08</a>]</span>.</p>
<p>There can be some disparity in the opposite direction to what we expect.
When comparing males to females in the adult dataset, overall, the direction of bias is in favour of males.
We do detect, however, females earning above the <span class="math notranslate nohighlight">\(\$50,000\)</span> threshold, whose male counterparts would be under the threshold.
This ‘reversed’ bias could be present in a subset of occupations.</p>
<p>Under-represented groups can be separated from the majority by a combination of protected attributes.
Analysing each protected attribute separately does not capture any compounding effects that might be experienced by a specific under-represented group, for example, unmarried, not-white women <span id="id29">[<a class="reference internal" href="../bib.html#id18">FIKP20</a>, <a class="reference internal" href="../bib.html#id140">BG18</a>]</span>.
Considering every possible combination, however, will significantly increase the size of the feature space and as a result, the required size of training data.</p>
<table class="colwidths-auto table" id="table-g1">
<caption><span class="caption-number">Table 6 </span><span class="caption-text">Breakdown of the <span class="math notranslate nohighlight">\(G1\)</span> group, comprising individuals funnelled into this group due to different selection rules. Consensus corresponds to selection rule 1 in Table {numref}table:1. Direct bias corresponds to selection rules 2 and 8 in Table~\ref{table:1}. Fallback indicates bias was detected in the opposite direction and the decision reverted to the original outcome.</span><a class="headerlink" href="#table-g1" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Selection Rule</p></th>
<th class="text-align:right head"><p>Gender</p></th>
<th class="text-align:right head"><p>Race</p></th>
<th class="text-align:right head"><p>Marital Status</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Consensus</p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(4.3 \)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(69.2\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\( 71.7 \)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Direct bias</p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(18.4\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(0 \)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Fallback</p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\( 77.2 \)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(30.8\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(28.3\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="limitations-and-intended-use">
<h3>Limitations and Intended Use<a class="headerlink" href="#limitations-and-intended-use" title="Permalink to this headline">¶</a></h3>
<p>When we are considering an algorithmic decision and support system deployed in a real-world biased setting, we can distinguish between different mechanisms that may lead to a disparity in positive outcomes rates between subgroups:
an aggravation of an existing disparity within the training data caused directly by the use of ML, for example, through over-fitting the training data;
bias we can successfully intervene on by mitigating, or even completely removing, label bias from the training data and the learnt model;
and, a disparity we can detect, but cannot directly intervene on without employing positive discrimination, which is opposed to anti-discrimination legislation.</p>
<p>In this work, we assume we are required to enforce the mapping between the observed and the decision space to be independent of the protected attribute, i.e., we assume it is a requirement to mitigate for label bias (selection rules <span class="math notranslate nohighlight">\(2\)</span> &amp; <span class="math notranslate nohighlight">\(8\)</span>, Table <a class="reference internal" href="#table-1"><span class="std std-numref">Table 3</span></a>).
This is the only bias that is mitigated at the accept / reject level.
The inclusion of the positive action candidate outcome and the <span class="math notranslate nohighlight">\(G_{3}\)</span> subgroup enables us to audit and mitigate, in the form of recommending candidates for positive action, any additional effects that may cause disparity, i.e. selection bias and imbalance of opportunities.</p>
<p>We choose to adopt a no-detriment, or positive-corrective approach.
This means that no individual, even if they allegedly benefit from past biased decisions, will be made worse off by the positive action approach.
In practice, selection rules can be adapted to suit the context and objectives at hand.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We present a novel algorithmic fairness framework that builds on the notion of positive action as a way of advancing equal representation while respecting anti-discrimination legislation and the right for equal treatment.
We aim not to reject high-potential applicants from under-represented groups, even if they cannot yet successfully compete in an equal-treatment selection process against applicants from the majority group.
As we are unable to accept them directly, they are highlighted as promising candidates for positive action measures.</p>
<p>Positive action initiatives can already be found in practice and can include outreach activities, targeted training and adaptive policies.
Specific positive action measures will be case and context dependent and should be determined by domain experts.
Our aim is to demonstrate that machine learning has the potential to help identify those applicants who would benefit from this additional support.</p>
<p>We consider the different mechanisms that can lead to an observed disparity in the rate of positive outcomes between a protected subgroup and the majority.
We highlight that, at least in part, this disparity can be due to disadvantages affecting applicants belonging to a protected subgroup, hindering their ability to compete with other applicants.</p>
<p>Our counterfactual implementation achieves the goal we set for it:
it maintains predictive utility while minimising the rejection of candidates with high potential from the disadvantaged group.
Through this framework we overcome potential challenges in the real-world deployment of decision support systems.</p>
<p>We hope this work will form part of a larger, constructive discussion, around the role of machine learning in promoting the use and effectiveness of positive action measures.</p>
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<p id="id30"><dl class="citation">
<dt class="label" id="id119"><span class="brackets"><a class="fn-backref" href="#id12">ABD+18</a></span></dt>
<dd><p>Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 60–69. Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.</p>
</dd>
<dt class="label" id="id112"><span class="brackets"><a class="fn-backref" href="#id1">BS16</a></span></dt>
<dd><p>S. Barocas and A. Selbst. Big data's disparate impact. <em>California Law Review</em>, 104(3):671–732, 2016.</p>
</dd>
<dt class="label" id="id159"><span class="brackets"><a class="fn-backref" href="#id3">Ben19</a></span></dt>
<dd><p>Jason R Bent. Is algorithmic affirmative action legal. <em>THE GEORGETOWN LAW JOURNAL</em>, 108:803, 2019.</p>
</dd>
<dt class="label" id="id170"><span class="brackets"><a class="fn-backref" href="#id15">BM04</a></span></dt>
<dd><p>Marianne Bertrand and Sendhil Mullainathan. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimination. <em>American economic review</em>, 94(4):991–1013, 2004.</p>
</dd>
<dt class="label" id="id165"><span class="brackets"><a class="fn-backref" href="#id24">BYF20</a></span></dt>
<dd><p>Emily Black, Samuel Yeom, and Matt Fredrikson. Fliptest: fairness testing via optimal transport. In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, FAT* '20, 111–121. New York, NY, USA, 2020. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">https://doi.org/10.1145/3351095.3372845</a>, <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">doi:10.1145/3351095.3372845</a>.</p>
</dd>
<dt class="label" id="id169"><span class="brackets"><a class="fn-backref" href="#id29">BG18</a></span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. PMLR, 2018.</p>
</dd>
<dt class="label" id="id115"><span class="brackets"><a class="fn-backref" href="#id13">CV10</a></span></dt>
<dd><p>Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. <em>Data Mining and Knowledge Discovery</em>, 21(2):277–292, September 2010. URL: <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">https://doi.org/10.1007/s10618-010-0190-x</a>, <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">doi:10.1007/s10618-010-0190-x</a>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets"><a class="fn-backref" href="#id17">Chi19</a></span></dt>
<dd><p>Silvia Chiappa. Path-specific counterfactual fairness. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 33, 7801–7808. 2019.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id3">DHP+12</a></span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>, ITCS '12, 214–226. New York, NY, USA, 2012. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>, <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">doi:10.1145/2090236.2090255</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets"><a class="fn-backref" href="#id29">FIKP20</a></span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. <em>2020 IEEE 36th International Conference on Data Engineering (ICDE)</em>, April 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1109/ICDE48307.2020.00203">http://dx.doi.org/10.1109/ICDE48307.2020.00203</a>, <a class="reference external" href="https://doi.org/10.1109/icde48307.2020.00203">doi:10.1109/icde48307.2020.00203</a>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">FSV16</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id8">2</a>,<a href="#id9">3</a>)</span></dt>
<dd><p>Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of fairness. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.07236">http://arxiv.org/abs/1609.07236</a>.</p>
</dd>
<dt class="label" id="id129"><span class="brackets"><a class="fn-backref" href="#id19">GGLRe20</a></span></dt>
<dd><p>Karan Goel, Albert Gu, Yixuan Li, and Christopher Ré. Model patching: closing the subgroup performance gap with data augmentation. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id110"><span class="brackets"><a class="fn-backref" href="#id27">GBR+07</a></span></dt>
<dd><p>Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander J Smola. A kernel approach to comparing distributions. In <em>Proceedings of the 22. AAAI Conference on Artificial Intelligence</em>, 1637–1641. Menlo Park, CA, USA, July 2007. Max-Planck-Gesellschaft, AAAI Press.</p>
</dd>
<dt class="label" id="id97"><span class="brackets"><a class="fn-backref" href="#id6">HPS16</a></span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 3315–3323. Curran Associates, Inc., 2016. URL: <a class="reference external" href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf</a>.</p>
</dd>
<dt class="label" id="id160"><span class="brackets"><a class="fn-backref" href="#id4">HW19</a></span></dt>
<dd><p>Zach Harned and Hanna Wallach. Stretching human laws to apply to machines: the dangers of a “colorblind” computer. <em>Florida State Univeristy Law Review</em>, 47:617, 2019.</p>
</dd>
<dt class="label" id="id124"><span class="brackets"><a class="fn-backref" href="#id14">Hum00</a></span></dt>
<dd><p>David Hume. <em>An enquiry concerning human understanding: A critical edition</em>. Volume 3. Oxford University Press, 2000.</p>
</dd>
<dt class="label" id="id116"><span class="brackets">JN20</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In Silvia Chiappa and Roberto Calandra, editors, <em>The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]</em>, volume 108 of Proceedings of Machine Learning Research, 702–712. PMLR, 2020.</p>
</dd>
<dt class="label" id="id162"><span class="brackets"><a class="fn-backref" href="#id21">JKV+19</a></span></dt>
<dd><p>Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards realistic individual recourse and actionable explanations in black-box decision making systems. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1907.09615">http://arxiv.org/abs/1907.09615</a>, <a class="reference external" href="https://arxiv.org/abs/1907.09615">arXiv:1907.09615</a>.</p>
</dd>
<dt class="label" id="id117"><span class="brackets"><a class="fn-backref" href="#id5">KZ18</a></span></dt>
<dd><p>Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. In Jennifer G. Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018</em>, volume 80 of Proceedings of Machine Learning Research, 2444–2453. PMLR, 2018.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">KC12</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1):1–33, 2012. <a class="reference external" href="https://doi.org/10.1007/s10115-011-0463-8">doi:10.1007/s10115-011-0463-8</a>.</p>
</dd>
<dt class="label" id="id164"><span class="brackets"><a class="fn-backref" href="#id21">KScholkopfV21</a></span></dt>
<dd><p>Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT '21, 353–362. New York, NY, USA, 2021. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">https://doi.org/10.1145/3442188.3445899</a>, <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">doi:10.1145/3442188.3445899</a>.</p>
</dd>
<dt class="label" id="id167"><span class="brackets"><a class="fn-backref" href="#id23">KNRW18</a></span></dt>
<dd><p>Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 2564–2572. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/kearns18a.html">http://proceedings.mlr.press/v80/kearns18a.html</a>.</p>
</dd>
<dt class="label" id="id155"><span class="brackets"><a class="fn-backref" href="#id13">KCQ20</a></span></dt>
<dd><p>Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto. Tuning fairness by balancing target labels. <em>Frontiers in Artificial Intelligence</em>, 3:33, 2020. URL: <a class="reference external" href="https://www.frontiersin.org/article/10.3389/frai.2020.00033">https://www.frontiersin.org/article/10.3389/frai.2020.00033</a>, <a class="reference external" href="https://doi.org/10.3389/frai.2020.00033">doi:10.3389/frai.2020.00033</a>.</p>
</dd>
<dt class="label" id="id98"><span class="brackets"><a class="fn-backref" href="#id17">KLRS17</a></span></dt>
<dd><p>Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 30</em>, pages 4066–4076. Curran Associates, Inc., 2017. URL: <a class="reference external" href="http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf">http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf</a>.</p>
</dd>
<dt class="label" id="id81"><span class="brackets"><a class="fn-backref" href="#id25">MCPZ18a</a></span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3384–3393. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/madras18a.html">http://proceedings.mlr.press/v80/madras18a.html</a>.</p>
</dd>
<dt class="label" id="id104"><span class="brackets"><a class="fn-backref" href="#id19">MCPZ18b</a></span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Fairness through causal awareness: learning latent-variable models for biased data. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.02519">http://arxiv.org/abs/1809.02519</a>, <a class="reference external" href="https://arxiv.org/abs/1809.02519">arXiv:1809.02519</a>.</p>
</dd>
<dt class="label" id="id122"><span class="brackets"><a class="fn-backref" href="#id20">MPZ18</a></span></dt>
<dd><p>David Madras, Toni Pitassi, and Richard Zemel. Predict responsibly: improving fairness and accuracy by learning to defer. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 31, 6147–6157. Curran Associates, Inc., 2018. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id123"><span class="brackets"><a class="fn-backref" href="#id14">Mil19</a></span></dt>
<dd><p>Tim Miller. Explanation in artificial intelligence: insights from the social sciences. <em>Artificial Intelligence</em>, 267:1–38, 2019. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0004370218305988">http://www.sciencedirect.com/science/article/pii/S0004370218305988</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2018.07.007">doi:https://doi.org/10.1016/j.artint.2018.07.007</a>.</p>
</dd>
<dt class="label" id="id161"><span class="brackets"><a class="fn-backref" href="#id20">MS20</a></span></dt>
<dd><p>Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In Hal Daumé III and Aarti Singh, editors, <em>Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of Proceedings of Machine Learning Research, 7076–7087. PMLR, 13–18 Jul 2020. URL: <a class="reference external" href="http://proceedings.mlr.press/v119/mozannar20b.html">http://proceedings.mlr.press/v119/mozannar20b.html</a>.</p>
</dd>
<dt class="label" id="id158"><span class="brackets"><a class="fn-backref" href="#id26">PCC+21</a></span></dt>
<dd><p>Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, and Hyunjung Shim. Multiple heads are better than one: few-shot font generation with multiple localized experts. 2021. <a class="reference external" href="https://arxiv.org/abs/2104.00887">arXiv:2104.00887</a>.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id16">Pea09</a></span></dt>
<dd><p>Judea Pearl. <em>Causality: Models, Reasoning and Inference</em>. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">PRT08</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id28">2</a>)</span></dt>
<dd><p>Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Ying Li, Bing Liu, and Sunita Sarawagi, editors, <em>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008</em>, 560–568. ACM, 2008. <a class="reference external" href="https://doi.org/10.1145/1401890.1401959">doi:10.1145/1401890.1401959</a>.</p>
</dd>
<dt class="label" id="id130"><span class="brackets"><a class="fn-backref" href="#id18">RR83</a></span></dt>
<dd><p>Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. <em>Biometrika</em>, 70(1):41–55, 1983.</p>
</dd>
<dt class="label" id="id120"><span class="brackets"><a class="fn-backref" href="#id18">Rub90</a></span></dt>
<dd><p>Donald B Rubin. Formal mode of statistical inference for causal effects. <em>Journal of statistical planning and inference</em>, 25(3):279–292, 1990.</p>
</dd>
<dt class="label" id="id168"><span class="brackets"><a class="fn-backref" href="#id22">SKS+18</a></span></dt>
<dd><p>Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. Aequitas: a bias and fairness audit toolkit. <em>arXiv preprint arXiv:1811.05577</em>, 2018.</p>
</dd>
<dt class="label" id="id157"><span class="brackets"><a class="fn-backref" href="#id26">SJS17</a></span></dt>
<dd><p>Uri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In Doina Precup and Yee Whye Teh, editors, <em>Proceedings of the 34th International Conference on Machine Learning</em>, volume 70 of Proceedings of Machine Learning Research, 3076–3085. PMLR, 06–11 Aug 2017. URL: <a class="reference external" href="http://proceedings.mlr.press/v70/shalit17a.html">http://proceedings.mlr.press/v70/shalit17a.html</a>.</p>
</dd>
<dt class="label" id="id128"><span class="brackets"><a class="fn-backref" href="#id19">SHDQ20</a></span></dt>
<dd><p>Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive examples for addressing the tyranny of the majority. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id156"><span class="brackets"><a class="fn-backref" href="#id4">SBW21</a></span></dt>
<dd><p>Joshua Simons, Sophia Adams Bhatti, and Adrian Weller. Machine learning and the meaning of equal treatment. <em>AIES</em>, 2021.</p>
</dd>
<dt class="label" id="id132"><span class="brackets"><a class="fn-backref" href="#id10">Tol19</a></span></dt>
<dd><p>Songül Tolan. Fair and unbiased algorithmic decision making: current state and future challenges. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1901.04730">http://arxiv.org/abs/1901.04730</a>, <a class="reference external" href="https://arxiv.org/abs/1901.04730">arXiv:1901.04730</a>.</p>
</dd>
<dt class="label" id="id163"><span class="brackets"><a class="fn-backref" href="#id21">USL19</a></span></dt>
<dd><p>Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 10–19. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">https://doi.org/10.1145/3287560.3287566</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">doi:10.1145/3287560.3287566</a>.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">WMR20</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why fairness cannot be automated: bridging the gap between eu non-discrimination law and ai. <em>SSRN Electronic Journal</em>, 2020. URL: <a class="reference external" href="http://dx.doi.org/10.2139/ssrn.3547922">http://dx.doi.org/10.2139/ssrn.3547922</a>, <a class="reference external" href="https://doi.org/10.2139/ssrn.3547922">doi:10.2139/ssrn.3547922</a>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id11">WPT19</a></span></dt>
<dd><p>Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan. Unlocking fairness: a trade-off revisited. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle  Alché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 32</em>, pages 8783–8792. Curran Associates, Inc., 2019. URL: <a class="reference external" href="http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf">http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf</a>.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">Xia21</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Alice Xiang. Reconciling legal and technical approaches to algorithmic bias. <em>Tennessee Law Review</em>, 2021. URL: <a class="reference external" href="https://ssrn.com/abstract=3650635">https://ssrn.com/abstract=3650635</a>.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/03_identifying"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../02_data_domain_fairness/intro.html" title="previous page">Data Domain Fairness</a>
    <a class='right-next' id="next-link" href="../08_conclusion/conclusion.html" title="next page">Conclusion</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Oliver Thomas<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-170288604-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>