# This Year

![](https://img.shields.io/badge/status-in%20progress-yellow)

## 2019-2020: Full-Time

Since the last annual review, I worked on 3 papers

1. _Imagined Examples:_ We use a Variational Autoencoder model to produce a model that not only considers the features, but also the reported outcomes, as a source of bias.
   We use this model to generate likely counterfactual samples but without the need for a causal model.
2. _NoSINN:_ We use the bijective property of invertible neural networks in conjunction with an adversarial network to _disentangle_ a sensitive attribute's effect on other features.
   We then generate samples that are neutral with respect to a sensitive attribute, using these neutral images as our representation of the data we can investigate how the model has made the data "fairer".
3. _Invisible Demographics:_ Training data doesn't always reflect the deployment setting, with some demographic groups either mis-represented, or simply missing.
   We use a

In addition I managed the Surgo reseach project for 7 months to produce the causal modelling tool [Intervene](https://github.com/predictive-analytics-lab/Intervene).

## Other works

Gave a [presentation](https://predictive-analytics-lab.github.io/presentations/toronto2019.html#/) on fairness in machine learning at [Conference on Data Science and Optimization](http://www.fields.utoronto.ca/talks/Transparency-fairness) in Toronto.
