
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction &#8212; Fair Representations of Biased Data</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="oliverthomas.ml/content/01_introduction/intro.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Data Domain Fairness" href="../02_data_domain_fairness/intro.html" />
    <link rel="prev" title="Content" href="../content.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="oliverthomas.ml/content/01_introduction/intro.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Introduction" />
<meta property="og:description" content="Introduction  This thesis is ultimately about exerting additional control over a machine learning model. Instead of allowing the simplest possible function to b" />
<meta property="og:image"       content="oliverthomas.ml/_static/pal-logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/pal-logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Fair Representations of Biased Data</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../content.html">
   Content
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Thesis
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_data_domain_fairness/intro.html">
   Data Domain Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03_identifying/intro.html">
   An Algorithmic Framework for Positive Action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08_conclusion/conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bib.html">
   Bibliography
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Annual Review 2020
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../10_annual_review/summary.html">
   Summary
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../10_annual_review/area.html">
     Research Area &amp; Question
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10_annual_review/activities_to_date.html">
   Activities to date
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10_annual_review/structure.html">
   Thesis Structure
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../09_appendix/published.html">
   Publications
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/dfritdd.html">
     Discovering Fair Representations in the Data Domain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/nosinn.html">
     Null-Sampling for Invariant and Interpretable Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/publications/lit_review.html">
     Literature Review (Year 1 Annual Review)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../09_appendix/software.html">
   Software
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/software/ethicml.html">
     EthicML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09_appendix/software/intervene.html">
     Casual Discovery Tool
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/01_introduction/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/olliethomas/thesis/issues/new?title=Issue%20on%20page%20%2Fcontent/01_introduction/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/olliethomas/thesis/edit/master/content/01_introduction/intro.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-goals-of-machine-learning">
     The Goals of Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attributes-and-features">
     Attributes and Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sensitive-attributes">
     Sensitive Attributes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#human-biases">
     Human Biases
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accuracy-as-an-objective">
     Accuracy as an objective
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simpson-s-paradox">
       Simpson’s Paradox
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feedback-loops">
       Feedback Loops
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pareto-optimality">
       Pareto Optimality
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretability">
     Interpretability
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fair-machine-learning">
   Fair Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approaches">
     Approaches
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-process">
       Pre-process
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-process">
       In-process
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#post-process">
       Post-process
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#metrics">
       Metrics
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datasets">
     Datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     Definitions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#group-notions-of-fairness">
       Group Notions of Fairness
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#independence">
         Independence
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#separation">
         Separation
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sufficiency">
         Sufficiency
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#competing-definitions">
       Competing Definitions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#individual-notions-of-fairness">
       Individual Notions of Fairness
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#individual-fairness">
         Individual Fairness
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#counterfactual-fairness">
         Counterfactual Fairness
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#challenges">
     Challenges
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adversarial-methods">
     Adversarial Methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distribution-matching">
     Distribution Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id47">
     Counterfactual Fairness
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#research-question">
   Research Question
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>This thesis is ultimately about exerting additional control over a machine learning model.
Instead of allowing the simplest possible function to be learnt, we want to encourage learning truth.
We want our models to find complex, but true answers.
Not simple convenient heuristiics.</p>
<p><img alt="" src="https://i.imgur.com/F2Y91nd.jpg" /></p>
<p>Machine Learning is a tool that is growing in popularity.
There have been a number of high profile successes, and new applications are being regularly identified.
These applications include translation (in both image and natural-language domains), pattern recognition and decision-making.
Contexts for these applications include Geology, Meteorology, Sports forecasting and Agriculture, to name a few.</p>
<p>Because of this success there is a desire to incorporate these systems in more and more situations, including those directly applicable to people.
For example, Machine Learning systems have already been applied to police allocation, recidivism prediction, candidate screening and credit approval.</p>
<p>On top of the benefits of automated decision making (speed, scale, etc) there is an additional promise to automated decisions.
The promise is that instead of many human decision makers, each one biased with their own prejudices, heuristics and experience, we can have a uniform approach.
The hope is that by treating everybody the same, then unequal, biased behaviour can be removed.</p>
<p>Unfortunately, that’s not always the case.</p>
<p>Recent headlines include:</p>
<ul class="simple">
<li><p><strong>Wrongfully Accused by an Algorithm</strong>: In what may be the first known case of its kind, a faulty facial recognition match
led to a Michigan man’s arrest for a crime he did not commit. – NYTimes <span id="id1">[<a class="reference internal" href="../bib.html#id63"><span>Hil20</span></a>]</span></p></li>
<li><p><strong>Amazon ditched AI recruiting tool that favored men for technical jobs</strong>. – The Guardian <span id="id2">[<a class="reference internal" href="../bib.html#id64"><span>Reu18</span></a>]</span></p></li>
<li><p><strong>Police officers raise concerns about ‘biased’ AI data</strong>. – BBC News <span id="id3">[<a class="reference internal" href="../bib.html#id65"><span>BBC19</span></a>]</span></p></li>
</ul>
<p>But how does this happen?
A prediction model has to be designed and there are a number of legal and moral obstacles to prevent a group/individual from purposefully designing a biased system.
However, even for the best intentioned there are a number of potential problems.
Examples of these problems include (but are not limited to):</p>
<ul class="simple">
<li><p><em>The tyranny of the majority:</em> We optimise to be right for the many, at the expense of minority groups.</p></li>
<li><p><em>Sampling bias:</em> We don’t have representative data of our population.</p></li>
<li><p><em>Proxy labels:</em> We don’t (or can’t) measure what we truly want to measure, so use a related quality as a proxy.</p></li>
<li><p><em>Biased data:</em> The recorded human decision was just plain biased.</p></li>
</ul>
<p>And unfortunately these aren’t mutually exclusive.</p>
<p>An unconstrained machine learning model is susceptible to all of these problems.
To face this challenge, the machine learning community has focused on creating a class of learning models that are constrained
to exhibit less bias than an unconstrained model.
Typically, these are referred to as “Fair Machine Learning Models”.</p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-goals-of-machine-learning">
<h3>The Goals of Machine Learning<a class="headerlink" href="#the-goals-of-machine-learning" title="Permalink to this headline">¶</a></h3>
<p>Given inputs <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span> and a target <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>, the goal is to learn a function <span class="math notranslate nohighlight">\(f: x \rightarrow y\)</span> that emulates some underlying relationship.</p>
</div>
<div class="section" id="attributes-and-features">
<h3>Attributes and Features<a class="headerlink" href="#attributes-and-features" title="Permalink to this headline">¶</a></h3>
<p>Stratified sampling?</p>
</div>
<div class="section" id="sensitive-attributes">
<h3>Sensitive Attributes<a class="headerlink" href="#sensitive-attributes" title="Permalink to this headline">¶</a></h3>
<p>Not all attributes are equally valid.</p>
</div>
<div class="section" id="human-biases">
<h3>Human Biases<a class="headerlink" href="#human-biases" title="Permalink to this headline">¶</a></h3>
<p>People can make inconsistent decisions.
Sometimes these are not based on an objective measure.
Large number of decision-makers, then of course you will have a greater number of inconsistent decisions.
The promise of an automated decision-maker is that the decisions should be consistent.
New instances of the decision-making system can be spun-up as required to handle large numbers of decisions.
The system, once running, is cheaper than hiring many people.</p>
</div>
<div class="section" id="accuracy-as-an-objective">
<h3>Accuracy as an objective<a class="headerlink" href="#accuracy-as-an-objective" title="Permalink to this headline">¶</a></h3>
<p>Goodhart’s Law: When a measure becomes a target, it ceases to be a good measure.
If we optimise just for accuracy, then we will get a model that is overly narrow.
It is good at that specific-thing.
It will not necessarily capture the values of our organisation during the decision-making process.</p>
<div class="section" id="simpson-s-paradox">
<h4>Simpson’s Paradox<a class="headerlink" href="#simpson-s-paradox" title="Permalink to this headline">¶</a></h4>
<p>A trend can exist in subgroups of the data, but the trend can be reversed when these groups are combined.
In relation to the previous section, when acuracy is improved overall, it can be that the accuracy for certain seubgroups is actually reduced.</p>
</div>
<div class="section" id="feedback-loops">
<h4>Feedback Loops<a class="headerlink" href="#feedback-loops" title="Permalink to this headline">¶</a></h4>
<p>We should consider systems over time.
Humans aren’t static and the result of a decision system can have an effect on an individual.
Consider a credit decision system.
If an applicant is rejected for a loan, then this will affect their credit rating, making it harder in the future to pass a credit decision-making step.</p>
</div>
<div class="section" id="pareto-optimality">
<h4>Pareto Optimality<a class="headerlink" href="#pareto-optimality" title="Permalink to this headline">¶</a></h4>
<p>When there are competing objectives, it’s often that the “correct” answer is not obvious.
In fact, there can be multiple “correct” answers.
The Pareto Frontier is collection of outputs, where each is the optimal output for that speicific balance between $<span class="math notranslate nohighlight">\(n\)</span>$ outcomes.</p>
</div>
</div>
<div class="section" id="interpretability">
<h3>Interpretability<a class="headerlink" href="#interpretability" title="Permalink to this headline">¶</a></h3>
<p>Interpretability is the degree to which a human can understand the cause
of a decision. Some models are inherently interpretable, such as
Decision Trees, or to a lesser extent linear models. There are also
model agnostic interpretability techniques such as local surrogate
models such as “Local Interpretable Model-agnostic Explanations (LIME)”
<span id="id4">[<a class="reference internal" href="../bib.html#id34"><span>RSG16</span></a>]</span>, or game-theory approaches to explanation
such as the Shapley Value <span id="id5">[<a class="reference internal" href="../bib.html#id60"><span>Rot88</span></a>]</span>.</p>
<p>From “Explanation in Artificial Intelligence : Insights from the Social
Sciences” <span id="id6">[<a class="reference internal" href="../bib.html#id94"><span>Mil19</span></a>]</span>, Interpretability is the degree
to which a human can understand the cause of a decision. The book
“Interpretable Machine Learning” <span id="id7">[<a class="reference internal" href="../bib.html#id28"><span>Mol18</span></a>]</span> frames that statement in a
slightly different way, describing interpretability is “the degree to
which a human can consistently predict the model’s result”. In this
review we have already distinguished between transparency and
interpretability, but <span id="id8">[<a class="reference internal" href="../bib.html#id28"><span>Mol18</span></a>]</span> distinguishes further between
interpretability and explanation. Miller <span id="id9">[<a class="reference internal" href="../bib.html#id94"><span>Mil19</span></a>]</span>
argues that even if we are capable of interpreting the results of a
model, unless we receive an explanation of how that model came to make a
decision, then we will be unable to reliably reproduce the results. not
only that, but as humans, any old explanation will not do, we require a
<em>good explanation</em>. According to Miller <span id="id10">[<a class="reference internal" href="../bib.html#id94"><span>Mil19</span></a>]</span>,
good explanations are:-</p>
<ul class="simple">
<li><p><strong>contrastive</strong>. We tend to think in a counterfactual way i.e. would
I have been approved for a loan if I earned more money. Explanations
should reflect this.</p></li>
<li><p><strong>selected</strong>. The world is complex and we don’t like to receive too
much information. As such, we should only give 1 to 3 explanations
that cover the majority of cases.</p></li>
<li><p><strong>social</strong>. They should be tailored to your audience.</p></li>
<li><p><strong>focused on the abnormal</strong>. If a data-point contains an anomaly
that impacts the result, use that in the explanation. i.e. house
price being predicted unusually highly as the property contains 5
balconies.</p></li>
<li><p><strong>truthful</strong>.</p></li>
<li><p><strong>coherent with prior beliefs of the explainee</strong></p></li>
<li><p><strong>general and probable</strong></p></li>
</ul>
</div>
</div>
<div class="section" id="fair-machine-learning">
<h2>Fair Machine Learning<a class="headerlink" href="#fair-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Fair Machine Learning, in general, aims to resolve “unfairness” (or the effect of bias) by affecting a model in one of
three places in the general learning pipeline.</p>
<ol class="simple">
<li><p>Pre-model training.</p>
<ul class="simple">
<li><p>Using a model to reduce bias in the underlying data, so that an unconstrained downstream classifier will exhibit fairer outcomes.</p></li>
</ul>
</li>
<li><p>During model training.</p>
<ul class="simple">
<li><p>Adding constraints to a model so that breaches in these constraints are heavily penalised during training.</p></li>
</ul>
</li>
<li><p>Post-model training.</p>
<ul class="simple">
<li><p>Adjusting the output of an unconstrained model so that the adjusted outputs don’t breach a given constraint.</p></li>
</ul>
</li>
</ol>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This comparison is of course a little over-simplistic.</p>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Approach</p></th>
<th class="text-align:center head"><p>Multi-task support</p></th>
<th class="text-align:center head"><p>Task model agnostic</p></th>
<th class="text-align:center head"><p>Pareto-optimal</p></th>
<th class="text-align:center head"><p>Constraints guaranteed to be enforced</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Pre-process</p></td>
<td class="text-align:center"><p>X</p></td>
<td class="text-align:center"><p>X</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>During</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>X</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Post-process</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>X</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>X</p></td>
</tr>
</tbody>
</table>
<p>In my work I focus on removing bias in the ‘pre-processing’ stage.
This is an exciting and active research area with spotlight tutorials at top conferences such as NeurIPS<a class="footnote-reference brackets" href="#footnote" id="id11">1</a> <span id="id12">[<a class="reference internal" href="../bib.html#id66"><span>CK</span></a>]</span>.
Part of the reason for the excitement in this area is that the underlying data itself is a major source of bias.
After all, this is what a model is using to determine “correct” behaviour.
If we are able to understand and counteract bias that exists in the underlying data, then we can use an unconstrained
classifier (which may already have been heavily invested in) for a task.
Crucially, we may be potentially able to use the same data for performing <em>multiple fair tasks</em>.</p>
<p>Ultimately though, even a model counteracting bias will not be fully trusted without being able to interpret the actions
that it has taken to counteract bias.
Simply off-loading a problem from one black-box to another only masks the issue.
My work specifically deals with <em>this</em> problem.</p>
<div class="section" id="approaches">
<h3>Approaches<a class="headerlink" href="#approaches" title="Permalink to this headline">¶</a></h3>
<p>As with all areas, the lines between the various points that fairness
constraints have been injected isn’t an easy thing to split. For
example, at what point does pre-processing the data become learning a
new representation for the data? As such, some of these methods blur
boundaries. However, in general, we can think of fairness interventions
occurring before, during, or after the training of the model as
suggested in <span id="id13">[<a class="reference internal" href="../bib.html#id22"><span>BHN19</span></a>]</span>.</p>
<p>In this section we’ll look onto the broad categories <em>pre-processing</em> -
which includes feature selection and feature adjustment, <em>during
training</em> - which covers minimising fairness constraints directly,
adversarially removing sensitive attributes and learning fair
representations and <em>post-processing</em> - which hasn’t been well utilised,
but is still a valid point to insert fairness constraints.</p>
<div class="section" id="pre-process">
<h4>Pre-process<a class="headerlink" href="#pre-process" title="Permalink to this headline">¶</a></h4>
<p>This can form in two approaches. On one hand, the features can be
pre-processed, or the labels can be pre-processed. There is a general
trend to transforming the features to a new space, called learning a
fair representation of the data. This is covered under the ‘During
Training’ section. This section is reserved for those papers which
explicitly change the input features, such as “Certifying and Removing
Disparate Impact” <span id="id14">[<a class="reference internal" href="../bib.html#id42"><span>FFM+15</span></a>]</span>. This paper compares
the probability distributions of features across protected groups and
seeks to rectify this. Enforcing</p>
<div class="math notranslate nohighlight">
\[
P(\bar{x}|s=0) = P(\bar{x}|s=1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}\)</span> is a modified version of the original feature <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>In this approach, each feature is ranked within sensitive sub-groups,
then shifted so as to retain the same rank whilst having an identical
distribution for each sub-group such that <span class="math notranslate nohighlight">\(P(s|\bar{x}) = 0.5\)</span>.</p>
<p>The concept of changing the input space is an intriguing one. On one
hand, a person’s features are not discriminatory in themselves, after
all, discriminatory features are <em>usually</em> qualitative. That said, some
features are due to the disparate impact of previous unfair decisions.
An example of this may be disproportionate wealth distribution in the US
manifesting itself as an unusually small number of white people
receiving public schooling
<span id="id15">[<a class="reference internal" href="../bib.html#id29"><span>USC</span></a>,<a class="reference internal" href="../bib.html#id30"><span>USN</span></a>]</span>. Modifying this feature
to correct for previous biases may have merit. It may also be useful to
consider that amending the input space allows us to interpret what has
changed about an individual to make them appear less worthy of being
biased against, and to feed this back to policy makers. This is under a
reasonable assumption that the input space records some features that we
are able to interpret. For example, if we knew that an applicant were
more likely to pass the CV screening stage of a job application in a
decision agnostic to their gender by increasing the level of relevant
work experience, it may be that there should be a policy change to
encourage people into apprenticeships or internships. The other approach
is changing the labels, as discussed in section
<a class="reference external" href="#background">2</a>{reference-type=”ref” reference=”background”} about the
Kamiran and Calders paper <span id="id16">[<a class="reference internal" href="../bib.html#id32"><span>KC09</span></a>]</span>.</p>
</div>
<div class="section" id="in-process">
<h4>In-process<a class="headerlink" href="#in-process" title="Permalink to this headline">¶</a></h4>
<p>One of the more direct ways to enforce fairness is to follow Quadrianto
et al <span id="id17">[<a class="reference internal" href="../bib.html#id56"><span>QS17</span></a>]</span> who noticed that enforcing
fairness constraints is an application of Distribution Matching. They
use a modified SVM from the Learning Using Privileged Information (LUPI)
paradigm to ensure the sensitive feature is not used during test time,
but is available during training. They then pose a question about how
much fairness to apply. There is a fairness-utility trade-off and the
authors suggest a human should be responsible for selecting how much
fairness (within a legal limit) to apply. This concept of bringing
accountability into automated decision making is an important though
overlooked addition.</p>
</div>
<div class="section" id="post-process">
<h4>Post-process<a class="headerlink" href="#post-process" title="Permalink to this headline">¶</a></h4>
<p>This area isn’t as well explored, but <span id="id18">[<a class="reference internal" href="../bib.html#id68"><span>HPS16</span></a>]</span> use it in their
paper.</p>
</div>
<div class="section" id="metrics">
<h4>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h3>
<p>Broadly, definitions of fairness can bve split into two sections: Group and Individual notions of fairness.</p>
<div class="section" id="group-notions-of-fairness">
<h4>Group Notions of Fairness<a class="headerlink" href="#group-notions-of-fairness" title="Permalink to this headline">¶</a></h4>
<p>Fairness constraints on the other hand have three forms. In the
currently unfinished book on Fairness in Machine Learning
<span id="id19">[<a class="reference internal" href="../bib.html#id22"><span>BHN19</span></a>]</span> the definitions of fairness are described as
belonging to one of three groups, <em>Independence</em>, <em>Separation</em> or
<em>Sufficiency</em>. This pattern is adopted in this section.</p>
<div class="section" id="independence">
<h5>Independence<a class="headerlink" href="#independence" title="Permalink to this headline">¶</a></h5>
<p>The most intuitive notion of fairness is <em>Independence</em>. This is the
notion that given a prediction (<span class="math notranslate nohighlight">\(\hat{Y}\)</span>) and a protected sensitive
attribute (<span class="math notranslate nohighlight">\(S\)</span>), then the prediction should be independent of the
sensitive attribute</p>
<div class="math notranslate nohighlight">
\[P(\hat{Y}) \perp S\]</div>
<p>In fact, one of the first
papers in this area by Kamiran and Calders <span id="id20">[<a class="reference internal" href="../bib.html#id32"><span>KC09</span></a>]</span> use this as
their discrimination measure. Although written in a different form, they
later use the notation latterly adopted in fairness literature during
their journal article <span id="id21">[<a class="reference internal" href="../bib.html#id10"><span>KC12</span></a>]</span> expanding on their previous work
<span id="id22">[<a class="reference internal" href="../bib.html#id32"><span>KC09</span></a>,<a class="reference internal" href="../bib.html#id33"><span>Kam11</span></a>]</span></p>
<div class="math notranslate nohighlight">
\[disc = P(\hat{y}|s=0) - P(\hat{y}|s=1)\]</div>
<p>Statistical Parity (or
Demographic Parity as it often known) appeals to an intuitive sense of
group fairness, namely that the outcome of the model should be
independent of some sensitive attribute(s). For example, the probability
of you being accepted at university should be the same if you are male
as if you are female (and vice-versa).</p>
<p>There are situations however, where this doesn’t work as intended. In
these cases, instead of promoting the perceived harmed group based on
the quality of the individual, as long as the probability of acceptance
is the same, the criteria is met.</p>
<p>To illustrate this, let’s consider an example that will be used across
all our definitions. Imagine that we are in charge of admissions at a
university and we are particularly concerned with complying to a
fairness criteria with regard to male and female subgroups. At this
university we can only accept 50% of all applicants. To determine if you
are likely to succeed there is an entrance criteria, which is highly
predictive of success. In fact, 80% of people who meet the entry
criteria successfully graduate. However, many students apply despite not
meeting the criteria. Universally, only 10% of students successfully
graduate if they do not meet the entrance criteria. Both male and female
subgroups apply to our university in equal numbers, though only 40% of
applying males meet the entrance criteria, whilst 60% of applying
females meet the entrance criteria. As already stated, we can only
accept 50% of applicants to be students. Under Demographic Parity, we
would require that 50% of both males and females be accepted, regardless
of likely academic performance. So even though only 40% of male
applicants meet the qualifying academic criteria, an additional 10% of
the population would have to be accepted at random to be ‘fair’, whilst
10% of qualified females would be rejected.</p>
<p>To counter this, yet keeping within the frame of <em>independence</em>,
relaxations of this criterion have also been suggested to include parity
up to some threshold, <span class="math notranslate nohighlight">\(\epsilon\)</span>. <span id="id23">[<a class="reference internal" href="../bib.html#id48"><span>ZVGRG19</span></a>]</span></p>
<div class="math notranslate nohighlight">
\[P(\hat{Y}|s=a) \geq P(\hat{Y}|s=b) - \epsilon\]</div>
<p>or via a ratio</p>
<div class="math notranslate nohighlight">
\[\frac{P(\hat{Y}|s=a)}{P(\hat{Y}|s=b)} \geq 1 - \epsilon\]</div>
<p>When set to <span class="math notranslate nohighlight">\(\epsilon = 0.2\)</span>, this is seen as comparable to the <em>80%
rule</em> mentioned in disparate impact law <span id="id24">[<a class="reference internal" href="../bib.html#id42"><span>FFM+15</span></a>]</span>.
This rule suggests that as long as the selection rate of the ‘harmed’
group is within 80% of the ‘privileged’ group, then it is fair enough.
Though critics of this point out that 80% was chosen arbitrarily.</p>
</div>
<div class="section" id="separation">
<h5>Separation<a class="headerlink" href="#separation" title="Permalink to this headline">¶</a></h5>
<p>A more complex definition of fairness is <em>separation</em>, which is
independence given the actual outcome (<span class="math notranslate nohighlight">\(Y\)</span>) $<span class="math notranslate nohighlight">\(\hat{Y} \perp S | Y\)</span>$</p>
<p>This has been formalised by the metric Equalised Odds <span id="id25">[<a class="reference internal" href="../bib.html#id68"><span>HPS16</span></a>]</span> which
considers all values of <span class="math notranslate nohighlight">\(Y\)</span>, and the looser constraint Equality of
Opportunity <span id="id26">[<a class="reference internal" href="../bib.html#id68"><span>HPS16</span></a>]</span>, which only constrains independence given the
outcome is positive .</p>
<p>Equalised Odds</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    P(\hat{Y}|s=0, y=0) &amp;= P(\hat{Y}|s=1, y=0) \\
    \&amp; \\
    P(\hat{Y}|s=0, y=1) &amp;= P(\hat{Y}|s=1, y=1)    
\end{align*}\end{split}\]</div>
<p>Equality of Opportunity $<span class="math notranslate nohighlight">\(P(\hat{Y}|s=0, y=1) = P(\hat{Y}|s=1, y=1)\)</span>$</p>
<p>Essentially, Equalised Odds ensure matching true positive rate (TPR) and
false positive rate (FPR) across the sensitive groups, whereas Equality
of Opportunity only ensures that the TPR of both (all) sensitive groups
are equal. The benefit of this is that it is a truer representation of
fairness.</p>
<p>In our university admission example, Equality of Opportunity is
equivalent to accepting members of both female and male subgroups at
different rates, as long as the true positive rate of both groups is
equal. So we would be looking to accept <span class="math notranslate nohighlight">\(44.5\%\)</span> of males and <span class="math notranslate nohighlight">\(55.5\%\)</span>
of females, which would give both groups a TPR of 85.4%.</p>
<p>If we were enforcing Equalised Odds, we would have to make sure that we
were not only matching the true positive rate, but also the false
positive rate. In our example, the selection rate would be <span class="math notranslate nohighlight">\(46.4\%\)</span> for
males and <span class="math notranslate nohighlight">\(53.6\%\)</span> for females.</p>
<p>But could an algorithm satisfy both independence and separation?
Unfortunately not, it is shown in <span id="id27">[<a class="reference internal" href="../bib.html#id68"><span>HPS16</span></a>]</span> that independence based
notions of fairness are incompatible with separation based notions of
fairness.</p>
</div>
<div class="section" id="sufficiency">
<h5>Sufficiency<a class="headerlink" href="#sufficiency" title="Permalink to this headline">¶</a></h5>
<p>There is a less well utilised general area of fairness criteria called
<em>sufficiency</em>. This is the concept that the true outcome, given the
predicted score is independent of <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="math notranslate nohighlight">
\[Y \perp S | \hat{Y}\]</div>
<p>In our example, we would leave the selection rates alone, giving a
selection rate of 40% for the male subgroup and 60% for the female
subgroup as then we treat applicants equally based our anticipation of
their success.</p>
<p>Similarly to the conflict between independence and separation, there is
a conflict between independence and sufficiency, and between separation
and sufficiency. These conflicts can be described succinctly, so we
demonstrate them below.</p>
<p>To show that sufficiency and independence are mutually exclusive, let’s
assume that there exists data where <span class="math notranslate nohighlight">\(Y\)</span> is dependent on <span class="math notranslate nohighlight">\(S\)</span>.
Independence is <span class="math notranslate nohighlight">\(\hat{Y} \perp S\)</span>, sufficiency is <span class="math notranslate nohighlight">\(Y \perp S | \hat{Y}\)</span>.
By simply claiming both of these statements to be true, we then have</p>
<div class="math notranslate nohighlight">
\[
S \perp \hat{Y} \textbf{ and } S \perp Y | \hat{Y} 
\Rightarrow S \perp Y
\]</div>
<p>This shows that independence and sufficiency can only hold when we
contradict our assumption.</p>
<p>Similarly, for separation and sufficiency if we assume that again we
have data where <span class="math notranslate nohighlight">\(Y \not \perp S\)</span> we can show that for both to hold we
would have</p>
<div class="math notranslate nohighlight">
\[
S \perp \hat{Y} | Y \textbf{ and } S \perp Y | \hat{Y}
\Rightarrow S \perp Y
\]</div>
<p>which again is contradictory to our assumption, demonstrating that we
can’t have both sufficiency and separation based notions of fairness
unless the data is inherently unbiased. This notion of trade-offs and
balancing tension is a common one throughout fair machine learning, and
certainly one that we will come back to.</p>
</div>
</div>
<div class="section" id="competing-definitions">
<h4>Competing Definitions<a class="headerlink" href="#competing-definitions" title="Permalink to this headline">¶</a></h4>
<p>The above is a useful framework for viewing fairness constraints and
helps us to categorise various definitions of fairness, such as those in
table 1, but that shouldn’t diminish work that seeks to make novel
strides within each of the areas. For example, the work in “An
Intersectional Definition of Fairness”
<span id="id28">[<a class="reference internal" href="../bib.html#id18"><span>FIKP20</span></a>]</span> expands the independence notion of
fairness. Their inspiration comes from third-wave feminism and
intersectional privacy, and is trying to expand beyond binary sensitive
groups and measuring an unfairness value at each intersect. For example,
consider we have a dataset with three sensitive attributes, sex, race
and religion. Most approaches to date consider these to be one feature
<em>sex_race_religion</em>. This paper measures the difference with respect
to demographic parity between each combination of sensitive attributes
so that sex, race and religion are all viewed as separate, measurable
points of potential discrimination, being concerned with whether
discrimination occurs in any, some, or if only with all attributes
present.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Fairness Goal</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Example of</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Demographic Parity</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y} \| S = 0) = P(\hat{Y} \| S = 1)\)</span></p></td>
<td><p>Independence</p></td>
</tr>
<tr class="row-odd"><td><p>Equal Opportunity</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y} = 1 \| Y = 1, S = 0) = P(\hat{Y} = 1 \| Y = 1, S = 1)\)</span></p></td>
<td><p>Separation</p></td>
</tr>
<tr class="row-even"><td><p>Equalised Odds</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y} = 1 \| Y = y, S = 0) = P(\hat{Y} = 1 \| Y = y, S = 1) \forall y \in Y\)</span></p></td>
<td><p>Separation</p></td>
</tr>
<tr class="row-odd"><td><p>Equal Accuracy</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y} = Y \| S = 0) = P(\hat{Y} = Y \| S = 1)\)</span></p></td>
<td><p>Independence</p></td>
</tr>
<tr class="row-even"><td><p>Accurate Coverage</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y} = 1 \| S = s) = P(Y = 1 \| S = s) \forall s \in S\)</span></p></td>
<td><p>Sufficiency</p></td>
</tr>
<tr class="row-odd"><td><p>Not Worse Off / No Lost Accuracy</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y}_{\text{new}} = Y \| S = 1) \geq P(\hat{Y}_{\text{current}} = Y \| S = 1)\)</span></p></td>
<td><p>Separation</p></td>
</tr>
<tr class="row-even"><td><p>No Lost Benefits / No Lost Coverage</p></td>
<td><p><span class="math notranslate nohighlight">\(P(\hat{Y}_{\text{new}} = 1 \| S = 1) \geq P(\hat{Y}_{\text{current}} = 1 \| S = 1)\)</span></p></td>
<td><p>Independence</p></td>
</tr>
</tbody>
</table>
<p>Differing Fairness Criterion and their categorisation</p>
<p>Which fairness definition (or family of definitions) should one be
using? This is a more complex question to answer. Some, such as
<span id="id29">[<a class="reference internal" href="../bib.html#id97"><span>YT21</span></a>,<a class="reference internal" href="../bib.html#id4"><span>HLGK19</span></a>]</span> make the assumption the the choice of which
to apply should come from the designer’s moral perspective, arguing that
this is a task outside of the expertise of computer scientists and
instead should be debated by philosophers.</p>
<p>However, not everyone is agreed. Recently there has started to be work
investigating the delayed impact that fair interventions in machine
learning have on society <span id="id30">[<a class="reference internal" href="../bib.html#id76"><span>LDR+18</span></a>]</span>. This work looks to determine the
impact that different notions of fairness have on the groups involved,
recognising that there is more than the initial ‘accepted for a loan’ or
‘rejected for a loan’ dichotomy, but that this has an impact in terms of
credit score for the individual applying. This is a bold approach that
tries to measure the effect that automated decisions may have 1
generation into the future. Although not explicitly stated, it leaves
the suggestion that maybe we should be using statistical models to
underpin moral assumptions.</p>
<p>A recent paper “Evaluating Fairness Metrics in the Presence of Dataset
Bias” <span id="id31">[<a class="reference internal" href="../bib.html#id45"><span>HCMD18</span></a>]</span> looked into the problem of
determining which fairness criteria to apply. They look at a dataset<a class="footnote-reference brackets" href="#id166" id="id32">5</a>
which they use to create 4 datasets, with combinations of Sample Bias,
No Sample Bias, Label Bias and No Label Bias. They consider a binary
race attribute (Black, White) where White race is <span class="math notranslate nohighlight">\(s=0\)</span> and Black race
is <span class="math notranslate nohighlight">\(s=1\)</span>. Label Bias is where there are different label thresholds based
on race, in this case</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{Y} = 
\begin{cases}
1 \text{ if score} \geq 0.3, \text{else } 0 &amp; \text{if race = white} \\
1 \text{ if score} \geq 0.7, \text{else } 0 &amp; \text{if race = black}
\end{cases}
\end{split}\]</div>
<p>Sample Bias is where one group (in this case White race) has people
selected at a higher rate, in this case</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{P}(x \in \mathcal{X}) = 
\begin{cases}
0.8 &amp; \text{if race = white and score} \geq 0.5 \\
0.2 &amp; \text{if race = white and score} &lt; 0.5 \\
1   &amp; \text{if race = black}
\end{cases}
\end{split}\]</div>
<p>This paper demonstrates that no single fairness metric is able to pick
up all discrimination and that all fairness metrics require “a healthy
dose of human judgement”.</p>
</div>
<div class="section" id="individual-notions-of-fairness">
<h4>Individual Notions of Fairness<a class="headerlink" href="#individual-notions-of-fairness" title="Permalink to this headline">¶</a></h4>
<p>All definitions of fairness that we’ve looked at so far consider
proportions of groups, called <em>group fairness</em>. There is another
approach, called <em>individual fairness</em>. This is the idea that regardless
of any groups as a whole, similar individuals should be treated
similarly. This idea was proposed by Luong et al.
<span id="id33">[<a class="reference internal" href="../bib.html#id51"><span>LRT11</span></a>]</span> in the context of k-NN and later refined by
Dwork et al. <span id="id34">[<a class="reference internal" href="../bib.html#id49"><span>DHP+12</span></a>]</span>.</p>
<p>Luong et al. use a Manhattan distance of z-scores for interval-scaled
attributes, and percentage of mismatching values for nominal attributes
to determine the distance between data-points. They determine that
discrimination has occurred if in its k-nearest neighbours those within
the same protected category have been treated differently to the
neighbours of a different category. They propose that on finding points
where they are confident that some discrimination has occurred, then the
class-label for that point should be amended. This data should then be
used to train subsequent models.</p>
<p>A seminal paper in the field, Dwork et al. <span id="id35">[<a class="reference internal" href="../bib.html#id49"><span>DHP+12</span></a>]</span>
continued with the concept of a distance measure. They propose again
that individual fairness should be the goal. They suggest that given
some <em>task-specific</em> similarity metric, <span class="math notranslate nohighlight">\(\delta\)</span>, then
<span class="math notranslate nohighlight">\(\forall x, x' \in \mathcal{X}: \quad |f(x) - f(x')| \leq \delta (x, x')\)</span>
where <span class="math notranslate nohighlight">\(f(x)\)</span> produces a continuous score as opposed to a discrete label.
The authors acknowledge that obtaining <span class="math notranslate nohighlight">\(\delta\)</span> is a tricky problem,
described as “one of the most challenging aspects of this framework”. It
may require input from social and legal scholars or domain experts to
help formulate this metric. This paper is particularly notable because
each of the authors have gone on to write about fairness extensively,
each gaining a reputation as an expert within their own right and covers
themes that have been spun into more fleshed out ideas. Examples of this
are Demographic Parity, the predominant fairness measure at the time,
being weak in some situations which has been explored in
<span id="id36">[<a class="reference internal" href="../bib.html#id68"><span>HPS16</span></a>,<a class="reference internal" href="../bib.html#id76"><span>LDR+18</span></a>]</span>, and noting that it there is a differentiation
between a data-provider (who obtains the data) and a vendor (who uses
the data to make decisions). It’s discussed that the vendor may not care
about fairness and this is explored further by two of the authors in
<span id="id37">[<a class="reference internal" href="../bib.html#id77"><span>ZWS+13</span></a>,<a class="reference internal" href="../bib.html#id52"><span>MCPZ18a</span></a>]</span>. This is
discussed in section <a class="reference external" href="#impl_repr">4.3</a>{reference-type=”ref”
reference=”impl_repr”}.</p>
<div class="section" id="individual-fairness">
<h5>Individual Fairness<a class="headerlink" href="#individual-fairness" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="counterfactual-fairness">
<h5>Counterfactual Fairness<a class="headerlink" href="#counterfactual-fairness" title="Permalink to this headline">¶</a></h5>
<p>With the acknowledgement that fairness is difficult to solve
arithmetically, methods to incorporate subject matter expertise are
being explored. Causal models are appealing in this regard because they
allow for an explicit causal relationship to be accounted for rather
than relying on correlation.</p>
<p>DeDeo <span id="id38">[<a class="reference internal" href="../bib.html#id62"><span>DeD14</span></a>]</span> argues that without understanding the
causal relationship between attributes, then it becomes particularly
difficult to differentiate between innocent relationships, and those
which at first glance may appear innocent, but when you understand the
socio-economic background of those attributes, they might infer a less
innocent relationship.</p>
<p>Stemming from the work of Pearl <span id="id39">[<a class="reference internal" href="../bib.html#id31"><span>Pea09</span></a>]</span>, causal models
are an attempt to model cause and effect. An example would be
atmospheric pressure and the position of the needle on a barometer
reading. We know that the two are linked and our data about this will
demonstrate a high correlation between observations, but correlation
does not imply causation. Whist we know that changing the pressure will
effect the barometer, moving the needle on the barometer will not effect
the pressure in the room. The benefit of viewing the world in this way
is that we can transparently interpret why decisions have been made.
Obviously the relationships between features is complex, but we can
utilise experts from the domain we are trying to apply our model to.
This is a nice feature given that fairness itself is domain specific.
Whilst this may seem simple on the surface, it is highly complicated to
correctly model the world. For example, not all features are captured.
There may be an unobserved feature that confounds two features, so
whilst they may look as though they are connected in some way, they are
actually both reflective of the unseen confounder. An example of this is
height and level of education. On the surface we could draw a
correlation that the taller (on average) a population is, the higher the
level of education. This can be observed by visiting any primary or
secondary school, but we’re missing a confounder, age. What’s more,
there can be multiple confounders that affect different sets of
features. Whilst not insurmountable, this is nonetheless a very labour
intensive approach. In many ways, if this approach is fully realised, it
is the gold standard for ethical models.</p>
<p>A recent paper, “Path-Specific Counterfactual Fairness”
<span id="id40">[<a class="reference internal" href="../bib.html#id73"><span>Chi19</span></a>]</span> poses some thought provoking
questions. They note that not all affects of a sensitive attribute on
the outcome are potentially discriminatory. They give the example of the
Berkley admissions data that was suggested to be discriminatory to
women. They note that women were applying with greater proportions to
classes with low acceptance rates, thus the influence of gender on the
class applied for is not discriminatory and should be taken into account
to learn a highly predictive model. This is similar to the idea first
mentioned in Pedreschi et al. <span id="id41">[<a class="reference internal" href="../bib.html#id89"><span>PRT08</span></a>]</span>
that there is a difference between sensitive attributes and potentially
discriminatory attributes. In <span id="id42">[<a class="reference internal" href="../bib.html#id73"><span>Chi19</span></a>]</span> they
use the power of a causal model to isolate this to effects along
specific pathways noting “approaches based on statistical relations
among observations are in danger of not discerning correlation from
causation, and are unable to distinguish the different ways in which the
sensitive attribute might influence the decision”
<span id="id43">[<a class="reference internal" href="../bib.html#id73"><span>Chi19</span></a>]</span>. This paper views unfairness as the
presence of an unfair causal effect of <span class="math notranslate nohighlight">\(S\)</span> on <span class="math notranslate nohighlight">\(\hat{Y}\)</span>. This idea is
not new. In fact it is specifically mentioned in “Counterfactual
Fairness” <span id="id44">[<a class="reference internal" href="../bib.html#id69"><span>KLRS17</span></a>]</span> that “a decision is unfair
toward an individual if it coincides with the one that would have been
taken in a counterfactual world in which the sensitive attribute were
different”. This assumes that the entire effect of <span class="math notranslate nohighlight">\(S\)</span> on <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is
problematic. The path-specific approach uses the same definition, but
modifies the ending to be “… counterfactual world in which the
sensitive attribute <em>along the unfair pathways</em> were different”. They
achieve this by measuring the effect of <span class="math notranslate nohighlight">\(s\)</span> along unfair pathways and
disregarding it. In the simple case below</p>
<div class="figure align-default" id="path-specific">
<a class="reference internal image-reference" href="../../_images/path-specific.png"><img alt="../../_images/path-specific.png" src="../../_images/path-specific.png" style="height: 200px;" /></a>
</div>
<p>where the direct effect of <span class="math notranslate nohighlight">\(s\)</span> on <span class="math notranslate nohighlight">\(y\)</span> is fair, but the effect of <span class="math notranslate nohighlight">\(s\)</span> via
<span class="math notranslate nohighlight">\(m\)</span> is unfair, then we can think of each variable being created of it’s
own characteristic <span class="math notranslate nohighlight">\(\theta^{\text{variable}}\)</span>, plus the effect of its
parents <span class="math notranslate nohighlight">\(\theta^{\text{variable}}_{\text{induced by}}\)</span>, plus noise
<span class="math notranslate nohighlight">\(\epsilon_{\text{variable}}\)</span>, so</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
S &amp;= \theta^s + \epsilon_s \\
M &amp;= \theta^m + \theta^m_s + \epsilon_m \\
Y &amp;= \theta^y + \theta^y_s + \theta^y_m + \epsilon_y
\end{align*}\end{split}\]</div>
<p>our goal would be to remove the effect of <span class="math notranslate nohighlight">\(s\)</span> along unfair pathways,
giving</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
S &amp;= \theta^s + \epsilon_s \\
M_{\text{fair}} &amp;= \theta^m + \epsilon_m \\
Y_{\text{fair}} &amp;= \theta^y + \theta^y_s + \theta^y_{m_\text{fair}} + \epsilon_y
\end{align*}\end{split}\]</div>
<p>In this case, (and in the case of “Counterfactual Fairness”
<span id="id45">[<a class="reference internal" href="../bib.html#id69"><span>KLRS17</span></a>]</span>). The goal is to achieve fairness in
a counterfactual world as described above. This is a form of individual
fairness, and is often linked to the other seminal work in this area of
Dwork et al. <span id="id46">[<a class="reference internal" href="../bib.html#id49"><span>DHP+12</span></a>]</span>. Whilst one uses a causal
model to determine the effect of group membership, the other uses a
distance measure. Clearly there are strengths and weaknesses to both
approaches.</p>
</div>
</div>
</div>
<div class="section" id="challenges">
<h3>Challenges<a class="headerlink" href="#challenges" title="Permalink to this headline">¶</a></h3>
<p>Need to record <span class="math notranslate nohighlight">\(S\)</span> so that we can be fair with regard to <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p>Distribution shift.
Particularly over time in systems with long feedback loops.</p>
<p>Learning from biased data.
Walter Quijano, a pschologist testifying in the trial of Duane Edward Buck, asserted that “African Americans pose a greater risk of ‘future dangerousness.’”
Without focussing too much on that case, it should be relatively clear that a system trained on data such as this may learn to repeat some of the behaviours.
<a class="reference external" href="https://www.huffingtonpost.co.uk/entry/supreme-court-death-penalty-case-duane-buck_n_1080112">link</a></p>
<p>Optimising without taking fairness metrics into account.</p>
</div>
</div>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<div class="section" id="adversarial-methods">
<h3>Adversarial Methods<a class="headerlink" href="#adversarial-methods" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="distribution-matching">
<h3>Distribution Matching<a class="headerlink" href="#distribution-matching" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id47">
<h3>Counterfactual Fairness<a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="research-question">
<h2>Research Question<a class="headerlink" href="#research-question" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Can we learn a representation of data that makes downstream tasks <strong>fair</strong>?</p>
<ol class="simple">
<li><p>If so, can we understand what the representation <strong>changed</strong>?</p></li>
</ol>
</li>
</ol>
<p>I will answer this question by:</p>
<ul class="simple">
<li><p>demonstrating that fair representations of data can be built in the original data domain without loss of performance with regard to both utility and fairness criterion.</p></li>
<li><p>demonstrating qualitatively that representations in the data domain provide feedback as to what is required to make data “fair”.</p></li>
</ul>
<hr class="docutils" />
<p id="id48"><dl class="citation">
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id15">USN</a></span></dt>
<dd><p>Mobile digest of education statistics, 2017. URL: <a class="reference external" href="https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx">https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx</a>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id15">USC</a></span></dt>
<dd><p>U.s. census bureau quickfacts: united states. URL: <a class="reference external" href="https://www.census.gov/quickfacts/fact/table/US/PST045217">https://www.census.gov/quickfacts/fact/table/US/PST045217</a>.</p>
</dd>
<dt class="label" id="id112"><span class="brackets"><a class="fn-backref" href="#id3">BBC19</a></span></dt>
<dd><p>Police officers raise concerns about 'biased' ai data. Sep 2019. URL: <a class="reference external" href="https://www.bbc.co.uk/news/technology-49717378">https://www.bbc.co.uk/news/technology-49717378</a>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">BHN19</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id19">2</a>)</span></dt>
<dd><p>Solon Barocas, Moritz Hardt, and Arvind Narayanan. <em>Fairness and Machine Learning</em>. fairmlbook.org, 2019. <span><a class="reference external" href="#"></a></span>http://www.fairmlbook.org.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">Chi19</span><span class="fn-backref">(<a href="#id40">1</a>,<a href="#id42">2</a>,<a href="#id43">3</a>)</span></dt>
<dd><p>Silvia Chiappa. Path-specific counterfactual fairness. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 33, 7801–7808. 2019.</p>
</dd>
<dt class="label" id="id113"><span class="brackets"><a class="fn-backref" href="#id12">CK</a></span></dt>
<dd><p>Moustapha Cisse and Sanmi Koyejo. Representation learning and fairness. NeurIPS 2019 Tutorial. URL: <a class="reference external" href="https://neurips.cc/Conferences/2019/Schedule?showEvent=13212">https://neurips.cc/Conferences/2019/Schedule?showEvent=13212</a>.</p>
</dd>
<dt class="label" id="id109"><span class="brackets"><a class="fn-backref" href="#id38">DeD14</a></span></dt>
<dd><p>Simon DeDeo. &quot;wrong side of the tracks&quot;: big data and protected categories. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1412.4643">http://arxiv.org/abs/1412.4643</a>, <a class="reference external" href="https://arxiv.org/abs/1412.4643">arXiv:1412.4643</a>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">DHP+12</span><span class="fn-backref">(<a href="#id34">1</a>,<a href="#id35">2</a>,<a href="#id46">3</a>)</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>, ITCS '12, 214–226. New York, NY, USA, 2012. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>, <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">doi:10.1145/2090236.2090255</a>.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">FFM+15</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id24">2</a>)</span></dt>
<dd><p>Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD '15, 259–268. New York, NY, USA, 2015. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">https://doi.org/10.1145/2783258.2783311</a>, <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">doi:10.1145/2783258.2783311</a>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id28">FIKP20</a></span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. <em>2020 IEEE 36th International Conference on Data Engineering (ICDE)</em>, April 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1109/ICDE48307.2020.00203">http://dx.doi.org/10.1109/ICDE48307.2020.00203</a>, <a class="reference external" href="https://doi.org/10.1109/icde48307.2020.00203">doi:10.1109/icde48307.2020.00203</a>.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">HPS16</span><span class="fn-backref">(<a href="#id18">1</a>,<a href="#id25">2</a>,<a href="#id26">3</a>,<a href="#id27">4</a>,<a href="#id36">5</a>)</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 3315–3323. Curran Associates, Inc., 2016. URL: <a class="reference external" href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id29">HLGK19</a></span></dt>
<dd><p>Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. A moral framework for understanding fair ml through economic models of equality of opportunity. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 181–190. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">https://doi.org/10.1145/3287560.3287584</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">doi:10.1145/3287560.3287584</a>.</p>
</dd>
<dt class="label" id="id110"><span class="brackets"><a class="fn-backref" href="#id1">Hil20</a></span></dt>
<dd><p>Kashmir Hill. Wrongfully accused by an algorithm. June 2020. URL: <a class="reference external" href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</a>.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id31">HCMD18</a></span></dt>
<dd><p>J. Henry Hinnefeld, Peter Cooman, Nat Mammo, and Rupert Deese. Evaluating fairness metrics in the presence of dataset bias. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.09245">http://arxiv.org/abs/1809.09245</a>, <a class="reference external" href="https://arxiv.org/abs/1809.09245">arXiv:1809.09245</a>.</p>
</dd>
<dt class="label" id="id80"><span class="brackets"><a class="fn-backref" href="#id22">Kam11</a></span></dt>
<dd><p>F. Kamiran. Discrimination-aware classification. Technical Report, Technische Universiteit Eindhoven, Eindhoven, 2011. URL: <a class="reference external" href="https://research.tue.nl/en/publications/discrimination-aware-classification">https://research.tue.nl/en/publications/discrimination-aware-classification</a>, <a class="reference external" href="https://doi.org/10.6100/IR717576">doi:10.6100/IR717576</a>.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">KC09</span><span class="fn-backref">(<a href="#id16">1</a>,<a href="#id20">2</a>,<a href="#id22">3</a>)</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Classifying without discriminating. In <em>2009 2nd International Conference on Computer, Control and Communication, IC4 2009</em>. 2009. <a class="reference external" href="https://doi.org/10.1109/IC4.2009.4909197">doi:10.1109/IC4.2009.4909197</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id21">KC12</a></span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1):1–33, 2012. <a class="reference external" href="https://doi.org/10.1007/s10115-011-0463-8">doi:10.1007/s10115-011-0463-8</a>.</p>
</dd>
<dt class="label" id="id116"><span class="brackets">KLRS17</span><span class="fn-backref">(<a href="#id44">1</a>,<a href="#id45">2</a>)</span></dt>
<dd><p>Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 30</em>, pages 4066–4076. Curran Associates, Inc., 2017. URL: <a class="reference external" href="http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf">http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf</a>.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">LDR+18</span><span class="fn-backref">(<a href="#id30">1</a>,<a href="#id36">2</a>)</span></dt>
<dd><p>Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3150–3158. StockholmsmÃ€ssan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/liu18c.html">http://proceedings.mlr.press/v80/liu18c.html</a>.</p>
</dd>
<dt class="label" id="id98"><span class="brackets"><a class="fn-backref" href="#id33">LRT11</a></span></dt>
<dd><p>Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. K-nn as an implementation of situation testing for discrimination discovery and prevention. <em>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11</em>, pages 502, 2011. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?doid=2020408.2020488">http://dl.acm.org/citation.cfm?doid=2020408.2020488</a>, <a class="reference external" href="https://doi.org/10.1145/2020408.2020488">doi:10.1145/2020408.2020488</a>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id37">MCPZ18</a></span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3384–3393. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/madras18a.html">http://proceedings.mlr.press/v80/madras18a.html</a>.</p>
</dd>
<dt class="label" id="id141"><span class="brackets">Mil19</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id9">2</a>,<a href="#id10">3</a>)</span></dt>
<dd><p>Tim Miller. Explanation in artificial intelligence: insights from the social sciences. <em>Artificial Intelligence</em>, 267:1–38, 2019. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0004370218305988">http://www.sciencedirect.com/science/article/pii/S0004370218305988</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2018.07.007">doi:https://doi.org/10.1016/j.artint.2018.07.007</a>.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">Mol18</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Christoph Molnar. <em>Interpretable Machine Learning</em>. https://christophm.github.io/interpretable-ml-book/, 2018.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id39">Pea09</a></span></dt>
<dd><p>Judea Pearl. <em>Causality: Models, Reasoning and Inference</em>. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.</p>
</dd>
<dt class="label" id="id136"><span class="brackets"><a class="fn-backref" href="#id41">PRT08</a></span></dt>
<dd><p>Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Ying Li, Bing Liu, and Sunita Sarawagi, editors, <em>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008</em>, 560–568. ACM, 2008. <a class="reference external" href="https://doi.org/10.1145/1401890.1401959">doi:10.1145/1401890.1401959</a>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets"><a class="fn-backref" href="#id17">QS17</a></span></dt>
<dd><p>Novi Quadrianto and Viktoriia Sharmanska. Recycling privileged learning and distribution matching for fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id111"><span class="brackets"><a class="fn-backref" href="#id2">Reu18</a></span></dt>
<dd><p>Reuters. Amazon ditched ai recruiting tool that favored men for technical jobs. Oct 2018. URL: <a class="reference external" href="https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine">https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine</a>.</p>
</dd>
<dt class="label" id="id81"><span class="brackets"><a class="fn-backref" href="#id4">RSG16</a></span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. &quot;why should i trust you?&quot; explaining the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135–1144. 2016.</p>
</dd>
<dt class="label" id="id107"><span class="brackets"><a class="fn-backref" href="#id5">Rot88</a></span></dt>
<dd><p>Alvin E Roth. <em>The Shapley value: essays in honor of Lloyd S. Shapley</em>. Cambridge University Press, 1988.</p>
</dd>
<dt class="label" id="id144"><span class="brackets"><a class="fn-backref" href="#id29">YT21</a></span></dt>
<dd><p>Samuel Yeom and Michael Carl Tschantz. Avoiding disparity amplification under different worldviews. In <em>ACM Conference on Fairness, Accountability, and Transparency</em>. 2021.</p>
</dd>
<dt class="label" id="id95"><span class="brackets"><a class="fn-backref" href="#id23">ZVGRG19</a></span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness constraints: a flexible approach for fair classification. <em>Journal of Machine Learning Research</em>, 20(75):1–42, 2019. URL: <a class="reference external" href="http://jmlr.org/papers/v20/18-262.html">http://jmlr.org/papers/v20/18-262.html</a>.</p>
</dd>
<dt class="label" id="id124"><span class="brackets"><a class="fn-backref" href="#id37">ZWS+13</a></span></dt>
<dd><p>Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Sanjoy Dasgupta and David McAllester, editors, <em>Proceedings of the 30th International Conference on Machine Learning</em>, volume 28 of Proceedings of Machine Learning Research, 325–333. Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v28/zemel13.html">http://proceedings.mlr.press/v28/zemel13.html</a>.</p>
</dd>
</dl>
</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="footnote"><span class="brackets"><a class="fn-backref" href="#id11">1</a></span></dt>
<dd><p><a class="reference external" href="http://sanmi.cs.illinois.edu/documents/Representation_Learning_Fairness_NeurIPS19_Tutorial.pdf">http://sanmi.cs.illinois.edu/documents/Representation_Learning_Fairness_NeurIPS19_Tutorial.pdf</a></p>
</dd>
<dt class="label" id="id166"><span class="brackets"><a class="fn-backref" href="#id32">5</a></span></dt>
<dd><p>Which due to contractual reasons they are unable to release.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/01_introduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../content.html" title="previous page">Content</a>
    <a class='right-next' id="next-link" href="../02_data_domain_fairness/intro.html" title="next page">Data Domain Fairness</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Oliver Thomas<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-170288604-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>