
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bibliography &#8212; Fair Representations of Biased Data</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="oliverthomas.ml/content/bib.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Summary" href="10_annual_review/summary.html" />
    <link rel="prev" title="Conclusion" href="08_conclusion/conclusion.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/pal-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Fair Representations of Biased Data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   Content
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Thesis
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_introduction/intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_data_domain_fairness/intro.html">
   Data Domain Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_identifying/intro.html">
   An Algorithmic Framework for Positive Action
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_conclusion/conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bibliography
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Annual Review 2020
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="10_annual_review/summary.html">
   Summary
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="10_annual_review/area.html">
     Research Area &amp; Question
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_annual_review/activities_to_date.html">
   Activities to date
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_annual_review/structure.html">
   Thesis Structure
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="09_appendix/published.html">
   Publications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/dfritdd.html">
     Discovering Fair Representations in the Data Domain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/nosinn.html">
     Null-Sampling for Invariant and Interpretable Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/lit_review.html">
     Literature Review (Year 1 Annual Review)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="09_appendix/software.html">
   Software
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/ethicml.html">
     EthicML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/intervene.html">
     Casual Discovery Tool
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/bib.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/olliethomas/thesis/issues/new?title=Issue%20on%20page%20%2Fcontent/bib.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/olliethomas/thesis/edit/master/content/bib.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p id="id1"><dl class="citation">
<dt class="label" id="id27"><span class="brackets">Ide</span></dt>
<dd><p>Ai for recruiting software | high-volume hiring | maximize quality of hire. URL: <a class="reference external" href="https://ideal.com/">https://ideal.com/</a>.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">USN</span></dt>
<dd><p>Mobile digest of education statistics, 2017. URL: <a class="reference external" href="https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx">https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx</a>.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">USC</span></dt>
<dd><p>U.s. census bureau quickfacts: united states. URL: <a class="reference external" href="https://www.census.gov/quickfacts/fact/table/US/PST045217">https://www.census.gov/quickfacts/fact/table/US/PST045217</a>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">BBC19</span></dt>
<dd><p>Police officers raise concerns about 'biased' ai data. Sep 2019. URL: <a class="reference external" href="https://www.bbc.co.uk/news/technology-49717378">https://www.bbc.co.uk/news/technology-49717378</a>.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">AAG+18</span></dt>
<dd><p>Igor Adamski, Robert Adamski, Tomasz Grel, Adam Jędrych, Kamil Kaczmarek, and Henryk Michalewski. Distributed deep reinforcement learning: learn how to play atari games in 21â minutes. In Rio Yokota, Michèle Weiland, David Keyes, and Carsten Trinitis, editors, <em>High Performance Computing</em>, 370–388. Cham, 2018. Springer International Publishing.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">ABD+18a</span></dt>
<dd><p>Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 60–69. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">ABD+18b</span></dt>
<dd><p>Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 60–69. Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">AC18</span></dt>
<dd><p>Mike Ananny and Kate Crawford. Seeing without knowing: limitations of the transparency ideal and its application to algorithmic accountability. <em>new media &amp; society</em>, 20(3):973–989, 2018.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">ALMK16</span></dt>
<dd><p>Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. <em>ProPublica, May</em>, 23(2016):139–159, 2016.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">BH</span></dt>
<dd><p>S Barocas and M Hardt. Fairness in machine learning. URL: <a class="reference external" href="https://nips.cc/Conferences/2017/Schedule?showEvent=8734">https://nips.cc/Conferences/2017/Schedule?showEvent=8734</a>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">BS16</span></dt>
<dd><p>S. Barocas and A. Selbst. Big data's disparate impact. <em>California Law Review</em>, 104(3):671–732, 2016.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">BHN19</span></dt>
<dd><p>Solon Barocas, Moritz Hardt, and Arvind Narayanan. <em>Fairness and Machine Learning</em>. fairmlbook.org, 2019. <span><a class="reference external" href="#"></a></span>http://www.fairmlbook.org.</p>
</dd>
<dt class="label" id="id130"><span class="brackets">Ben19</span></dt>
<dd><p>Jason R Bent. Is algorithmic affirmative action legal. <em>THE GEORGETOWN LAW JOURNAL</em>, 108:803, 2019.</p>
</dd>
<dt class="label" id="id141"><span class="brackets">BM04</span></dt>
<dd><p>Marianne Bertrand and Sendhil Mullainathan. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimination. <em>American economic review</em>, 94(4):991–1013, 2004.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">BCZC17</span></dt>
<dd><p>Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. Data decisions and theoretical implications when adversarially learning fair representations. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1707.00075">http://arxiv.org/abs/1707.00075</a>, <a class="reference external" href="https://arxiv.org/abs/1707.00075">arXiv:1707.00075</a>.</p>
</dd>
<dt class="label" id="id136"><span class="brackets">BYF20</span></dt>
<dd><p>Emily Black, Samuel Yeom, and Matt Fredrikson. Fliptest: fairness testing via optimal transport. In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, FAT* '20, 111–121. New York, NY, USA, 2020. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">https://doi.org/10.1145/3351095.3372845</a>, <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">doi:10.1145/3351095.3372845</a>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">BCZ+16</span></dt>
<dd><p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 4349–4357. 2016.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">BP93</span></dt>
<dd><p>Elizabeth Brown and David I Perrett. What gives a face its gender? <em>Perception</em>, 22(7):829–840, 1993. PMID: 8115240. URL: <a class="reference external" href="https://doi.org/10.1068/p220829">https://doi.org/10.1068/p220829</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1068/p220829">arXiv:https://doi.org/10.1068/p220829</a>, <a class="reference external" href="https://doi.org/10.1068/p220829">doi:10.1068/p220829</a>.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">BS18</span></dt>
<dd><p>N. Brown and T. Sandholm. Superhuman ai for heads-up no-limit poker: libratus beats top professionals. <em>Science</em>, 359:418–424, January 2018. <a class="reference external" href="https://doi.org/10.1126/science.aao1733">doi:10.1126/science.aao1733</a>.</p>
</dd>
<dt class="label" id="id105"><span class="brackets">BBH+93</span></dt>
<dd><p>Vicki Bruce, A Mike Burton, Elias Hanna, Pat Healey, Oli Mason, Anne Coombes, Rick Fright, and Alf Linney. Sex discrimination: how do we tell the difference between male and female faces? <em>Perception</em>, 22(2):131–152, 1993. PMID: 8474840. URL: <a class="reference external" href="https://doi.org/10.1068/p220131">https://doi.org/10.1068/p220131</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1068/p220131">arXiv:https://doi.org/10.1068/p220131</a>, <a class="reference external" href="https://doi.org/10.1068/p220131">doi:10.1068/p220131</a>.</p>
</dd>
<dt class="label" id="id140"><span class="brackets">BG18</span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. PMLR, 2018.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">BSOG18</span></dt>
<dd><p>Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced neighborhoods for multi-sided fairness in recommendation. <em>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</em>, 81(2008):202–214, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v81/burke18a.html">http://proceedings.mlr.press/v81/burke18a.html</a>.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">CV10</span></dt>
<dd><p>Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. <em>Data Mining and Knowledge Discovery</em>, 21(2):277–292, September 2010. URL: <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">https://doi.org/10.1007/s10618-010-0190-x</a>, <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">doi:10.1007/s10618-010-0190-x</a>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">Chi19</span></dt>
<dd><p>Silvia Chiappa. Path-specific counterfactual fairness. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 33, 7801–7808. 2019.</p>
</dd>
<dt class="label" id="id112"><span class="brackets">CCK+18</span></dt>
<dd><p>Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: unified generative adversarial networks for multi-domain image-to-image translation. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. June 2018.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">Cho17</span></dt>
<dd><p>Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. <em>Big Data</em>, 5(2):153–163, 2017. PMID: 28632438. URL: <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1089/big.2016.0047">arXiv:https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">doi:10.1089/big.2016.0047</a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">CK</span></dt>
<dd><p>Moustapha Cisse and Sanmi Koyejo. Representation learning and fairness. NeurIPS 2019 Tutorial. URL: <a class="reference external" href="https://neurips.cc/Conferences/2019/Schedule?showEvent=13212">https://neurips.cc/Conferences/2019/Schedule?showEvent=13212</a>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">CP14</span></dt>
<dd><p>Danielle Keats Citron and Frank Pasquale. The scored society: due process for automated predictions. <em>Wash. L. Rev.</em>, 89:1, 2014.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">CDG</span></dt>
<dd><p>S Corbett-Davies and S Goel. Defining and designing fair algorithms. ICML 2018 Tutorial. URL: <a class="reference external" href="https://icml.cc/Conferences/2018/Schedule?showEvent=1862">https://icml.cc/Conferences/2018/Schedule?showEvent=1862</a>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">DeD14</span></dt>
<dd><p>Simon DeDeo. &quot;wrong side of the tracks&quot;: big data and protected categories. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1412.4643">http://arxiv.org/abs/1412.4643</a>, <a class="reference external" href="https://arxiv.org/abs/1412.4643">arXiv:1412.4643</a>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">Dia14</span></dt>
<dd><p>Nicholas Diakopoulos. Algorithmic accountability reporting: on the investigation of black boxes. <em>NA</em>, 2014.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">DHP+12</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>, ITCS '12, 214–226. New York, NY, USA, 2012. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>, <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">doi:10.1145/2090236.2090255</a>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">ES16</span></dt>
<dd><p>Harrison Edwards and Amos J. Storkey. Censoring representations with an adversary. In Yoshua Bengio and Yann LeCun, editors, <em>4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings</em>. 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1511.05897">http://arxiv.org/abs/1511.05897</a>.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">EG18</span></dt>
<dd><p>Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 11–21. Brussels, Belgium, October 2018. Association for Computational Linguistics. URL: <a class="reference external" href="https://www.aclweb.org/anthology/D18-1002">https://www.aclweb.org/anthology/D18-1002</a>, <a class="reference external" href="https://doi.org/10.18653/v1/D18-1002">doi:10.18653/v1/D18-1002</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">EJJ+19</span></dt>
<dd><p>Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, and Zachary Schutzman. Fair algorithms for learning in allocation problems. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 170–179. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">https://doi.org/10.1145/3287560.3287571</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">doi:10.1145/3287560.3287571</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">FKT+18</span></dt>
<dd><p>Golnoosh Farnadi, Pigi Kouki, Spencer K. Thompson, Sriram Srinivasan, and Lise Getoor. A fairness-aware hybrid recommender system. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.09030">http://arxiv.org/abs/1809.09030</a>, <a class="reference external" href="https://arxiv.org/abs/1809.09030">arXiv:1809.09030</a>.</p>
</dd>
<dt class="label" id="id42"><span class="brackets">FFM+15</span></dt>
<dd><p>Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD '15, 259–268. New York, NY, USA, 2015. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">https://doi.org/10.1145/2783258.2783311</a>, <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">doi:10.1145/2783258.2783311</a>.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">FIKP20</span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. <em>2020 IEEE 36th International Conference on Data Engineering (ICDE)</em>, April 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1109/ICDE48307.2020.00203">http://dx.doi.org/10.1109/ICDE48307.2020.00203</a>, <a class="reference external" href="https://doi.org/10.1109/icde48307.2020.00203">doi:10.1109/icde48307.2020.00203</a>.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">FSV16</span></dt>
<dd><p>Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of fairness. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.07236">http://arxiv.org/abs/1609.07236</a>.</p>
</dd>
<dt class="label" id="id104"><span class="brackets">FHH14</span></dt>
<dd><p>S. Fu, H. He, and Z. Hou. Learning race from face: a survey. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 36(12):2483–2509, 2014. <a class="reference external" href="https://doi.org/10.1109/TPAMI.2014.2321570">doi:10.1109/TPAMI.2014.2321570</a>.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">GUA+16</span></dt>
<dd><p>Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. <em>Journal of Machine Learning Research</em>, 17(59):1–35, 2016. URL: <a class="reference external" href="http://jmlr.org/papers/v17/15-239.html">http://jmlr.org/papers/v17/15-239.html</a>.</p>
</dd>
<dt class="label" id="id124"><span class="brackets">GEB16</span></dt>
<dd><p>Leon Gatys, Alexander Ecker, and Matthias Bethge. A neural algorithm of artistic style. <em>Journal of Vision</em>, 16(12):326, Sep 2016. URL: <a class="reference external" href="http://dx.doi.org/10.1167/16.12.326">http://dx.doi.org/10.1167/16.12.326</a>, <a class="reference external" href="https://doi.org/10.1167/16.12.326">doi:10.1167/16.12.326</a>.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">GGLRe20</span></dt>
<dd><p>Karan Goel, Albert Gu, Yixuan Li, and Christopher Ré. Model patching: closing the subgroup performance gap with data augmentation. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id81"><span class="brackets">GBR+07</span></dt>
<dd><p>Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander J Smola. A kernel approach to comparing distributions. In <em>Proceedings of the 22. AAAI Conference on Artificial Intelligence</em>, 1637–1641. Menlo Park, CA, USA, July 2007. Max-Planck-Gesellschaft, AAAI Press.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">GBR+12</span></dt>
<dd><p>Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. <em>Journal of Machine Learning Research</em>, 13(25):723–773, 2012. URL: <a class="reference external" href="http://jmlr.org/papers/v13/gretton12a.html">http://jmlr.org/papers/v13/gretton12a.html</a>.</p>
</dd>
<dt class="label" id="id113"><span class="brackets">GBSScholkopf05</span></dt>
<dd><p>Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. Measuring statistical dependence with hilbert-schmidt norms. In Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, editors, <em>Algorithmic Learning Theory</em>, 63–77. Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.</p>
</dd>
<dt class="label" id="id114"><span class="brackets">GHRGW18</span></dt>
<dd><p>Nina Grgic-Hlaca, Elissa M. Redmiles, Krishna P. Gummadi, and Adrian Weller. Human perceptions of fairness in algorithmic decision making: a case study of criminal risk prediction. In <em>Proceedings of the 2018 World Wide Web Conference</em>, WWW '18, 903–912. Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee. URL: <a class="reference external" href="https://doi.org/10.1145/3178876.3186138">https://doi.org/10.1145/3178876.3186138</a>, <a class="reference external" href="https://doi.org/10.1145/3178876.3186138">doi:10.1145/3178876.3186138</a>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">GCFW18</span></dt>
<dd><p>Maya R. Gupta, Andrew Cotter, Mahdi Milani Fard, and Serena Wang. Proxy fairness. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1806.11212">http://arxiv.org/abs/1806.11212</a>, <a class="reference external" href="https://arxiv.org/abs/1806.11212">arXiv:1806.11212</a>.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">HPS16</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 3315–3323. Curran Associates, Inc., 2016. URL: <a class="reference external" href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf</a>.</p>
</dd>
<dt class="label" id="id131"><span class="brackets">HW19</span></dt>
<dd><p>Zach Harned and Hanna Wallach. Stretching human laws to apply to machines: the dangers of a “colorblind” computer. <em>Florida State Univeristy Law Review</em>, 47:617, 2019.</p>
</dd>
<dt class="label" id="id119"><span class="brackets">HZRS16</span></dt>
<dd><p>K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–778. 2016. <a class="reference external" href="https://doi.org/10.1109/CVPR.2016.90">doi:10.1109/CVPR.2016.90</a>.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">HLGK19</span></dt>
<dd><p>Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. A moral framework for understanding fair ml through economic models of equality of opportunity. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 181–190. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">https://doi.org/10.1145/3287560.3287584</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">doi:10.1145/3287560.3287584</a>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">Hil20</span></dt>
<dd><p>Kashmir Hill. Wrongfully accused by an algorithm. June 2020. URL: <a class="reference external" href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</a>.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">HCMD18</span></dt>
<dd><p>J. Henry Hinnefeld, Peter Cooman, Nat Mammo, and Rupert Deese. Evaluating fairness metrics in the presence of dataset bias. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.09245">http://arxiv.org/abs/1809.09245</a>, <a class="reference external" href="https://arxiv.org/abs/1809.09245">arXiv:1809.09245</a>.</p>
</dd>
<dt class="label" id="id53"><span class="brackets">HLV16</span></dt>
<dd><p>Mark Holmstrom, Dylan Liu, and Christopher Vo. Machine learning applied to weather forecasting. <em>Stanford University</em>, pages 2–4, 2016.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">HOU18</span></dt>
<dd><p>OF LORDS HOUSE. Select committee on artificial intelligence. <em>AI in the UK: ready, willing and able</em>, 2018.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Hum00</span></dt>
<dd><p>David Hume. <em>An enquiry concerning human understanding: A critical edition</em>. Volume 3. Oxford University Press, 2000.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">Hwa18</span></dt>
<dd><p>Tim Hwang. Computational power and the social impact of artificial intelligence. <em>CoRR</em>, 2018. Available at SSRN 3147971. URL: <a class="reference external" href="http://arxiv.org/abs/1803.08971">http://arxiv.org/abs/1803.08971</a>, <a class="reference external" href="https://arxiv.org/abs/1803.08971">arXiv:1803.08971</a>.</p>
</dd>
<dt class="label" id="id108"><span class="brackets">IZZE17</span></dt>
<dd><p>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. July 2017.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">JN20</span></dt>
<dd><p>Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In Silvia Chiappa and Roberto Calandra, editors, <em>The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]</em>, volume 108 of Proceedings of Machine Learning Research, 702–712. PMLR, 2020.</p>
</dd>
<dt class="label" id="id133"><span class="brackets">JKV+19</span></dt>
<dd><p>Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards realistic individual recourse and actionable explanations in black-box decision making systems. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1907.09615">http://arxiv.org/abs/1907.09615</a>, <a class="reference external" href="https://arxiv.org/abs/1907.09615">arXiv:1907.09615</a>.</p>
</dd>
<dt class="label" id="id88"><span class="brackets">KZ18</span></dt>
<dd><p>Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. In Jennifer G. Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018</em>, volume 80 of Proceedings of Machine Learning Research, 2444–2453. PMLR, 2018.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">Kam11</span></dt>
<dd><p>F. Kamiran. Discrimination-aware classification. Technical Report, Technische Universiteit Eindhoven, Eindhoven, 2011. URL: <a class="reference external" href="https://research.tue.nl/en/publications/discrimination-aware-classification">https://research.tue.nl/en/publications/discrimination-aware-classification</a>, <a class="reference external" href="https://doi.org/10.6100/IR717576">doi:10.6100/IR717576</a>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">KC09</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Classifying without discriminating. In <em>2009 2nd International Conference on Computer, Control and Communication, IC4 2009</em>. 2009. <a class="reference external" href="https://doi.org/10.1109/IC4.2009.4909197">doi:10.1109/IC4.2009.4909197</a>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">KC12</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1):1–33, 2012. <a class="reference external" href="https://doi.org/10.1007/s10115-011-0463-8">doi:10.1007/s10115-011-0463-8</a>.</p>
</dd>
<dt class="label" id="id135"><span class="brackets">KScholkopfV21</span></dt>
<dd><p>Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT '21, 353–362. New York, NY, USA, 2021. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">https://doi.org/10.1145/3442188.3445899</a>, <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">doi:10.1145/3442188.3445899</a>.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">KNRW18</span></dt>
<dd><p>Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 2564–2572. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/kearns18a.html">http://proceedings.mlr.press/v80/kearns18a.html</a>.</p>
</dd>
<dt class="label" id="id126"><span class="brackets">KCQ20</span></dt>
<dd><p>Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto. Tuning fairness by balancing target labels. <em>Frontiers in Artificial Intelligence</em>, 3:33, 2020. URL: <a class="reference external" href="https://www.frontiersin.org/article/10.3389/frai.2020.00033">https://www.frontiersin.org/article/10.3389/frai.2020.00033</a>, <a class="reference external" href="https://doi.org/10.3389/frai.2020.00033">doi:10.3389/frai.2020.00033</a>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">KMR17</span></dt>
<dd><p>Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Christos H. Papadimitriou, editor, <em>8th Innovations in Theoretical Computer Science Conference (ITCS 2017)</em>, volume 67 of Leibniz International Proceedings in Informatics (LIPIcs), 43:1–43:23. Dagstuhl, Germany, 2017. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik. URL: <a class="reference external" href="http://drops.dagstuhl.de/opus/volltexte/2017/8156">http://drops.dagstuhl.de/opus/volltexte/2017/8156</a>, <a class="reference external" href="https://doi.org/10.4230/LIPIcs.ITCS.2017.43">doi:10.4230/LIPIcs.ITCS.2017.43</a>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">KLRS17</span></dt>
<dd><p>Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 30</em>, pages 4066–4076. Curran Associates, Inc., 2017. URL: <a class="reference external" href="http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf">http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf</a>.</p>
</dd>
<dt class="label" id="id107"><span class="brackets">LH15</span></dt>
<dd><p>G. Levi and T. Hassncer. Age and gender classification using convolutional neural networks. In <em>2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 34–42. 2015. <a class="reference external" href="https://doi.org/10.1109/CVPRW.2015.7301352">doi:10.1109/CVPRW.2015.7301352</a>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">LDR+18</span></dt>
<dd><p>Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3150–3158. StockholmsmÃ€ssan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/liu18c.html">http://proceedings.mlr.press/v80/liu18c.html</a>.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">LLWT15</span></dt>
<dd><p>Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In <em>2015 IEEE International Conference on Computer Vision (ICCV)</em>, 3730–3738. 2015. <a class="reference external" href="https://doi.org/10.1109/ICCV.2015.425">doi:10.1109/ICCV.2015.425</a>.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">LSL+16</span></dt>
<dd><p>Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. In <em>ICLR</em>. 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1511.00830">http://arxiv.org/abs/1511.00830</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">LRT11</span></dt>
<dd><p>Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. K-nn as an implementation of situation testing for discrimination discovery and prevention. <em>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11</em>, pages 502, 2011. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?doid=2020408.2020488">http://dl.acm.org/citation.cfm?doid=2020408.2020488</a>, <a class="reference external" href="https://doi.org/10.1145/2020408.2020488">doi:10.1145/2020408.2020488</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">MCPZ18a</span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3384–3393. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/madras18a.html">http://proceedings.mlr.press/v80/madras18a.html</a>.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">MCPZ18b</span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Fairness through causal awareness: learning latent-variable models for biased data. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.02519">http://arxiv.org/abs/1809.02519</a>, <a class="reference external" href="https://arxiv.org/abs/1809.02519">arXiv:1809.02519</a>.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">MPZ18</span></dt>
<dd><p>David Madras, Toni Pitassi, and Richard Zemel. Predict responsibly: improving fairness and accuracy by learning to defer. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 31, 6147–6157. Curran Associates, Inc., 2018. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">Mil19</span></dt>
<dd><p>Tim Miller. Explanation in artificial intelligence: insights from the social sciences. <em>Artificial Intelligence</em>, 267:1–38, 2019. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0004370218305988">http://www.sciencedirect.com/science/article/pii/S0004370218305988</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2018.07.007">doi:https://doi.org/10.1016/j.artint.2018.07.007</a>.</p>
</dd>
<dt class="label" id="id110"><span class="brackets">MO14</span></dt>
<dd><p>Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1411.1784">http://arxiv.org/abs/1411.1784</a>, <a class="reference external" href="https://arxiv.org/abs/1411.1784">arXiv:1411.1784</a>.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">Mol18</span></dt>
<dd><p>Christoph Molnar. <em>Interpretable Machine Learning</em>. https://christophm.github.io/interpretable-ml-book/, 2018.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">MJPScholkopf09</span></dt>
<dd><p>Joris Mooij, Dominik Janzing, Jonas Peters, and Bernhard Schölkopf. Regression by dependence minimization and its application to causal inference in additive noise models. In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, ICML '09, 745–752. New York, NY, USA, 2009. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/1553374.1553470">https://doi.org/10.1145/1553374.1553470</a>, <a class="reference external" href="https://doi.org/10.1145/1553374.1553470">doi:10.1145/1553374.1553470</a>.</p>
</dd>
<dt class="label" id="id132"><span class="brackets">MS20</span></dt>
<dd><p>Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In Hal Daumé III and Aarti Singh, editors, <em>Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of Proceedings of Machine Learning Research, 7076–7087. PMLR, 13–18 Jul 2020. URL: <a class="reference external" href="http://proceedings.mlr.press/v119/mozannar20b.html">http://proceedings.mlr.press/v119/mozannar20b.html</a>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">MSP16</span></dt>
<dd><p>Cecilia Munoz, Megan Smith, and DJ Patil. Big data : a report on algorithmic systems , opportunity , and civil rights big data : a report on algorithmic systems , opportunity , and civil rights. <em>Executive Office of the President of USA</em>, 2016. URL: <a class="reference external" href="https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf">https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf</a>, <a class="reference external" href="https://doi.org/10.1177/0162243915598056">doi:10.1177/0162243915598056</a>.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">NSB02</span></dt>
<dd><p>Ilya Nemenman, F. Shafee, and William Bialek. Entropy and inference, revisited. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, <em>Advances in Neural Information Processing Systems</em>, volume 14. MIT Press, 2002. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf">https://proceedings.neurips.cc/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id129"><span class="brackets">PCC+21</span></dt>
<dd><p>Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, and Hyunjung Shim. Multiple heads are better than one: few-shot font generation with multiple localized experts. 2021. <a class="reference external" href="https://arxiv.org/abs/2104.00887">arXiv:2104.00887</a>.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">Pea09</span></dt>
<dd><p>Judea Pearl. <em>Causality: Models, Reasoning and Inference</em>. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">PRT08</span></dt>
<dd><p>Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Ying Li, Bing Liu, and Sunita Sarawagi, editors, <em>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008</em>, 560–568. ACM, 2008. <a class="reference external" href="https://doi.org/10.1145/1401890.1401959">doi:10.1145/1401890.1401959</a>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">QS17</span></dt>
<dd><p>Novi Quadrianto and Viktoriia Sharmanska. Recycling privileged learning and distribution matching for fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">Reu18</span></dt>
<dd><p>Reuters. Amazon ditched ai recruiting tool that favored men for technical jobs. Oct 2018. URL: <a class="reference external" href="https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine">https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">RSG16</span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. &quot;why should i trust you?&quot; explaining the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135–1144. 2016.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">RR83</span></dt>
<dd><p>Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. <em>Biometrika</em>, 70(1):41–55, 1983.</p>
</dd>
<dt class="label" id="id60"><span class="brackets">Rot88</span></dt>
<dd><p>Alvin E Roth. <em>The Shapley value: essays in honor of Lloyd S. Shapley</em>. Cambridge University Press, 1988.</p>
</dd>
<dt class="label" id="id91"><span class="brackets">Rub90</span></dt>
<dd><p>Donald B Rubin. Formal mode of statistical inference for causal effects. <em>Journal of statistical planning and inference</em>, 25(3):279–292, 1990.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">SKS+18</span></dt>
<dd><p>Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. Aequitas: a bias and fairness audit toolkit. <em>arXiv preprint arXiv:1811.05577</em>, 2018.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">SHCV19</span></dt>
<dd><p>P. Sattigeri, S. C. Hoffman, V. Chenthamarakshan, and K. R. Varshney. Fairness gan: generating datasets with fairness properties using a generative adversarial network. <em>IBM Journal of Research and Development</em>, 63(4/5):3:1–3:9, 2019. <a class="reference external" href="https://doi.org/10.1147/JRD.2019.2945519">doi:10.1147/JRD.2019.2945519</a>.</p>
</dd>
<dt class="label" id="id128"><span class="brackets">SJS17</span></dt>
<dd><p>Uri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In Doina Precup and Yee Whye Teh, editors, <em>Proceedings of the 34th International Conference on Machine Learning</em>, volume 70 of Proceedings of Machine Learning Research, 3076–3085. PMLR, 06–11 Aug 2017. URL: <a class="reference external" href="http://proceedings.mlr.press/v70/shalit17a.html">http://proceedings.mlr.press/v70/shalit17a.html</a>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">SHDQ20</span></dt>
<dd><p>Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive examples for addressing the tyranny of the majority. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">SSS+17</span></dt>
<dd><p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. <em>Nature</em>, 550:354 EP –, 10 2017. URL: <a class="reference external" href="http://dx.doi.org/10.1038/nature24270">http://dx.doi.org/10.1038/nature24270</a>.</p>
</dd>
<dt class="label" id="id127"><span class="brackets">SBW21</span></dt>
<dd><p>Joshua Simons, Sophia Adams Bhatti, and Adrian Weller. Machine learning and the meaning of equal treatment. <em>AIES</em>, 2021.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">SGSS16</span></dt>
<dd><p>Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, and Soumik Sarkar. Machine learning for high-throughput stress phenotyping in plants. <em>Trends in Plant Science</em>, 21(2):110–124, 2016. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1360138515002630">http://www.sciencedirect.com/science/article/pii/S1360138515002630</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.tplants.2015.10.015">doi:https://doi.org/10.1016/j.tplants.2015.10.015</a>.</p>
</dd>
<dt class="label" id="id122"><span class="brackets">SSG+12</span></dt>
<dd><p>Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via dependence maximization. <em>Journal of Machine Learning Research</em>, 13(47):1393–1434, 2012. URL: <a class="reference external" href="http://jmlr.org/papers/v13/song12a.html">http://jmlr.org/papers/v13/song12a.html</a>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">Swe13</span></dt>
<dd><p>Latanya Sweeney. Discrimination in online ad delivery. <em>Commun. ACM</em>, 56(5):44–54, May 2013. URL: <a class="reference external" href="https://doi.org/10.1145/2447976.2447990">https://doi.org/10.1145/2447976.2447990</a>, <a class="reference external" href="https://doi.org/10.1145/2447976.2447990">doi:10.1145/2447976.2447990</a>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">Tol19</span></dt>
<dd><p>Songül Tolan. Fair and unbiased algorithmic decision making: current state and future challenges. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1901.04730">http://arxiv.org/abs/1901.04730</a>, <a class="reference external" href="https://arxiv.org/abs/1901.04730">arXiv:1901.04730</a>.</p>
</dd>
<dt class="label" id="id134"><span class="brackets">USL19</span></dt>
<dd><p>Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 10–19. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">https://doi.org/10.1145/3287560.3287566</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">doi:10.1145/3287560.3287566</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">Vin17</span></dt>
<dd><p>James Vincent. Deepmind's ai became a superhuman chess player in a few hours, just for fun. December 2017. URL: <a class="reference external" href="https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go">https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go</a>.</p>
</dd>
<dt class="label" id="id61"><span class="brackets">WMF17</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Luciano Floridi. Transparent, explainable, and accountable ai for robotics. <em>NA</em>, 2017.</p>
</dd>
<dt class="label" id="id117"><span class="brackets">WMR18</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: automated decisions and the gdpr. <em>Harvard Journal of Law &amp; Technology</em>, 2018.</p>
</dd>
<dt class="label" id="id92"><span class="brackets">WMR20</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why fairness cannot be automated: bridging the gap between eu non-discrimination law and ai. <em>SSRN Electronic Journal</em>, 2020. URL: <a class="reference external" href="http://dx.doi.org/10.2139/ssrn.3547922">http://dx.doi.org/10.2139/ssrn.3547922</a>, <a class="reference external" href="https://doi.org/10.2139/ssrn.3547922">doi:10.2139/ssrn.3547922</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">WVP18</span></dt>
<dd><p>Christina Wadsworth, Francesca Vera, and Chris Piech. Achieving fairness through adversarial learning: an application to recidivism prediction. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1807.00199">http://arxiv.org/abs/1807.00199</a>, <a class="reference external" href="https://arxiv.org/abs/1807.00199">arXiv:1807.00199</a>.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">WPT19</span></dt>
<dd><p>Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan. Unlocking fairness: a trade-off revisited. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle  Alché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 32</em>, pages 8783–8792. Curran Associates, Inc., 2019. URL: <a class="reference external" href="http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf">http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf</a>.</p>
</dd>
<dt class="label" id="id125"><span class="brackets">Xia21</span></dt>
<dd><p>Alice Xiang. Reconciling legal and technical approaches to algorithmic bias. <em>Tennessee Law Review</em>, 2021. URL: <a class="reference external" href="https://ssrn.com/abstract=3650635">https://ssrn.com/abstract=3650635</a>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">XYZW18</span></dt>
<dd><p>Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: fairness-aware generative adversarial networks. In <em>2018 IEEE International Conference on Big Data (Big Data)</em>, 570–575. IEEE, 2018.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">YH17</span></dt>
<dd><p>Sirui Yao and Bert Huang. Beyond parity: fairness objectives for collaborative filtering. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id97"><span class="brackets">YT21</span></dt>
<dd><p>Samuel Yeom and Michael Carl Tschantz. Avoiding disparity amplification under different worldviews. In <em>ACM Conference on Fairness, Accountability, and Transparency</em>. 2021.</p>
</dd>
<dt class="label" id="id116"><span class="brackets">ZVGRG17</span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment &amp; disparate impact: learning classification without disparate mistreatment. In <em>Proceedings of the 26th International Conference on World Wide Web</em>, WWW '17, 1171–1180. Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee. URL: <a class="reference external" href="https://doi.org/10.1145/3038912.3052660">https://doi.org/10.1145/3038912.3052660</a>, <a class="reference external" href="https://doi.org/10.1145/3038912.3052660">doi:10.1145/3038912.3052660</a>.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">ZVGRG19</span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness constraints: a flexible approach for fair classification. <em>Journal of Machine Learning Research</em>, 20(75):1–42, 2019. URL: <a class="reference external" href="http://jmlr.org/papers/v20/18-262.html">http://jmlr.org/papers/v20/18-262.html</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">ZBC+17</span></dt>
<dd><p>Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. Fa*ir: a fair top-k ranking algorithm. In <em>Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</em>, CIKM '17, 1569–1578. New York, NY, USA, 2017. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">https://doi.org/10.1145/3132847.3132938</a>, <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">doi:10.1145/3132847.3132938</a>.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">ZWS+13</span></dt>
<dd><p>Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Sanjoy Dasgupta and David McAllester, editors, <em>Proceedings of the 30th International Conference on Machine Learning</em>, volume 28 of Proceedings of Machine Learning Research, 325–333. Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v28/zemel13.html">http://proceedings.mlr.press/v28/zemel13.html</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">ZLM18</span></dt>
<dd><p>Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In <em>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</em>, AIES '18, 335–340. New York, NY, USA, 2018. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">https://doi.org/10.1145/3278721.3278779</a>, <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">doi:10.1145/3278721.3278779</a>.</p>
</dd>
<dt class="label" id="id111"><span class="brackets">ZPIE17</span></dt>
<dd><p>J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, 2242–2251. 2017. <a class="reference external" href="https://doi.org/10.1109/ICCV.2017.244">doi:10.1109/ICCV.2017.244</a>.</p>
</dd>
<dt class="label" id="id109"><span class="brackets">GlobalFCoHR20161818</span></dt>
<dd><p>Global Future Council on Human Rights 2016-18. How to prevent discriminatory outcomes in machine learning. Technical Report, World Economic Forum, 2018.</p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="08_conclusion/conclusion.html" title="previous page">Conclusion</a>
    <a class='right-next' id="next-link" href="10_annual_review/summary.html" title="next page">Summary</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Oliver Thomas<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-170288604-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>