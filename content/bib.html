
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bibliography &#8212; Fair Representations of Biased Data</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="oliverthomas.ml/content/bib.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Summary" href="10_annual_review/summary.html" />
    <link rel="prev" title="Conclusion" href="08_conclusion/conclusion.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="oliverthomas.ml/content/bib.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Bibliography" />
<meta property="og:description" content="Bibliography  Ide  Ai for recruiting software | high-volume hiring | maximize quality of hire. URL: https://ideal.com/.USN  Mobile digest of education statistic" />
<meta property="og:image"       content="oliverthomas.ml/_static/pal-logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/pal-logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Fair Representations of Biased Data</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   Content
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Thesis
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="00_introduction/intro.html">
   Introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="00_introduction/00-1_overview/overview.html">
     Overview
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3 collapsible-parent">
      <a class="reference internal" href="00_introduction/00-1_overview/fair_ml.html">
       Fair Machine Learning
      </a>
      <ul class="collapse-ul">
       <li class="toctree-l4">
        <a class="reference internal" href="00_introduction/00-1_overview/definitions.html">
         Definitions
        </a>
       </li>
      </ul>
      <i class="fas fa-chevron-down">
      </i>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="00_introduction/00-1_overview/challenges.html">
       Challenges
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="00_introduction/00-2_background/background.html">
     Background
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="00_introduction/00-2_background/adversarial_methods.html">
       Adversarial Methods
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="00_introduction/00-2_background/dist_matching.html">
       Distribution Matching
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="00_introduction/00-2_background/counterfactual_fairness.html">
       Counterfactual Fairness
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_fair_representations/intro.html">
   Fair Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_data_domain_fairness/intro.html">
   Data Domain Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_identifying/intro.html">
   Identifying At-Risk Individuals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_conclusion/conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bibliography
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Annual Review 2020
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="10_annual_review/summary.html">
   Summary
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="10_annual_review/area.html">
     Research Area &amp; Question
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_annual_review/activities_to_date.html">
   Activities to date
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_annual_review/structure.html">
   Thesis Structure
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="09_appendix/published.html">
   Publications
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/dfritdd.html">
     Discovering Fair Representations in the Data Domain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/framework_for_positive_action.html">
     An Algorithmic Framework for Positive Action
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/framework_for_positive_action.html#demonstration">
     Demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/imagined.html">
     Imagined Examples for Fairness: Sampling Bias and Proxy Labels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/nosinn.html">
     Null-Sampling for Invariant and Interpretable Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/lit_review.html">
     Literature Review (Year 1 Annual Review)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="09_appendix/software.html">
   Software
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/ethicml.html">
     EthicML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/intervene.html">
     Casual Discovery Tool
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/bib.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/olliethomas/thesis/issues/new?title=Issue%20on%20page%20%2Fcontent/bib.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/olliethomas/thesis/edit/master/content/bib.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p id="id1"><dl class="citation">
<dt class="label" id="id27"><span class="brackets">Ide</span></dt>
<dd><p>Ai for recruiting software | high-volume hiring | maximize quality of hire. URL: <a class="reference external" href="https://ideal.com/">https://ideal.com/</a>.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">USN</span></dt>
<dd><p>Mobile digest of education statistics, 2017. URL: <a class="reference external" href="https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx">https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx</a>.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">USC</span></dt>
<dd><p>U.s. census bureau quickfacts: united states. URL: <a class="reference external" href="https://www.census.gov/quickfacts/fact/table/US/PST045217">https://www.census.gov/quickfacts/fact/table/US/PST045217</a>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">BBC19</span></dt>
<dd><p>Police officers raise concerns about 'biased' ai data. Sep 2019. URL: <a class="reference external" href="https://www.bbc.co.uk/news/technology-49717378">https://www.bbc.co.uk/news/technology-49717378</a>.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">AAG+18</span></dt>
<dd><p>Igor Adamski, Robert Adamski, Tomasz Grel, Adam Jędrych, Kamil Kaczmarek, and Henryk Michalewski. Distributed deep reinforcement learning: learn how to play atari games in 21â minutes. In Rio Yokota, Michèle Weiland, David Keyes, and Carsten Trinitis, editors, <em>High Performance Computing</em>, 370–388. Cham, 2018. Springer International Publishing.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">AVGW19</span></dt>
<dd><p>Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. One-network adversarial fairness. In <em>Thirty-Third AAAI Conference on Artificial Intelligence</em>. 2019.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">ABD+18</span></dt>
<dd><p>Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 60–69. Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">AC18</span></dt>
<dd><p>Mike Ananny and Kate Crawford. Seeing without knowing: limitations of the transparency ideal and its application to algorithmic accountability. <em>new media &amp; society</em>, 20(3):973–989, 2018.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">ALMK16</span></dt>
<dd><p>Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. <em>ProPublica, May</em>, 23(2016):139–159, 2016.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">BH</span></dt>
<dd><p>S Barocas and M Hardt. Fairness in machine learning. URL: <a class="reference external" href="https://nips.cc/Conferences/2017/Schedule?showEvent=8734">https://nips.cc/Conferences/2017/Schedule?showEvent=8734</a>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">BS16</span></dt>
<dd><p>S. Barocas and A. Selbst. Big data's disparate impact. <em>California Law Review</em>, 104(3):671–732, 2016.</p>
</dd>
<dt class="label" id="id84"><span class="brackets">BCSW17</span></dt>
<dd><p>Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. The problem with bias: from allocative to representational harms in machine learning. In <em>Special Interest Group for Computing, Information and Society (SIGCIS)</em>. 2017.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">BHN19</span></dt>
<dd><p>Solon Barocas, Moritz Hardt, and Arvind Narayanan. <em>Fairness and Machine Learning</em>. fairmlbook.org, 2019. <span><a class="reference external" href="#"></a></span>http://www.fairmlbook.org.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">BCZC17</span></dt>
<dd><p>Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. Data decisions and theoretical implications when adversarially learning fair representations. <em>CoRR</em>, 2017. URL: <a class="reference external" href="http://arxiv.org/abs/1707.00075">http://arxiv.org/abs/1707.00075</a>, <a class="reference external" href="https://arxiv.org/abs/1707.00075">arXiv:1707.00075</a>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">BS20</span></dt>
<dd><p>Avrim Blum and Kevin Stangl. Recovering from biased data: can fairness constraints improve accuracy? In Aaron Roth, editor, <em>1st Symposium on Foundations of Responsible Computing, FORC 2020, June 1-3, 2020, Harvard University, Cambridge, MA, USA (virtual conference)</em>, volume 156 of LIPIcs, 3:1–3:20. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">BCZ+16</span></dt>
<dd><p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 4349–4357. 2016.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">BS18</span></dt>
<dd><p>N. Brown and T. Sandholm. Superhuman ai for heads-up no-limit poker: libratus beats top professionals. <em>Science</em>, 359:418–424, January 2018. <a class="reference external" href="https://doi.org/10.1126/science.aao1733">doi:10.1126/science.aao1733</a>.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">BSOG18</span></dt>
<dd><p>Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced neighborhoods for multi-sided fairness in recommendation. <em>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</em>, 81(2008):202–214, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v81/burke18a.html">http://proceedings.mlr.press/v81/burke18a.html</a>.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">CV10</span></dt>
<dd><p>Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. <em>Data Mining and Knowledge Discovery</em>, 21(2):277–292, September 2010. URL: <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">https://doi.org/10.1007/s10618-010-0190-x</a>, <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">doi:10.1007/s10618-010-0190-x</a>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">Chi19</span></dt>
<dd><p>Silvia Chiappa. Path-specific counterfactual fairness. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 33, 7801–7808. 2019.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">Cho17</span></dt>
<dd><p>Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. <em>Big Data</em>, 5(2):153–163, 2017. PMID: 28632438. URL: <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1089/big.2016.0047">arXiv:https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">doi:10.1089/big.2016.0047</a>.</p>
</dd>
<dt class="label" id="id82"><span class="brackets">CR20</span></dt>
<dd><p>Alexandra Chouldechova and Aaron Roth. A snapshot of the frontiers of fairness in machine learning. <em>Commun. ACM</em>, 63(5):82–89, 2020.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">CK</span></dt>
<dd><p>Moustapha Cisse and Sanmi Koyejo. Representation learning and fairness. NeurIPS 2019 Tutorial. URL: <a class="reference external" href="https://neurips.cc/Conferences/2019/Schedule?showEvent=13212">https://neurips.cc/Conferences/2019/Schedule?showEvent=13212</a>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">CP14</span></dt>
<dd><p>Danielle Keats Citron and Frank Pasquale. The scored society: due process for automated predictions. <em>Wash. L. Rev.</em>, 89:1, 2014.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">CDG</span></dt>
<dd><p>S Corbett-Davies and S Goel. Defining and designing fair algorithms. ICML 2018 Tutorial. URL: <a class="reference external" href="https://icml.cc/Conferences/2018/Schedule?showEvent=1862">https://icml.cc/Conferences/2018/Schedule?showEvent=1862</a>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">DeD14</span></dt>
<dd><p>Simon DeDeo. &quot;wrong side of the tracks&quot;: big data and protected categories. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1412.4643">http://arxiv.org/abs/1412.4643</a>, <a class="reference external" href="https://arxiv.org/abs/1412.4643">arXiv:1412.4643</a>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">Dia14</span></dt>
<dd><p>Nicholas Diakopoulos. Algorithmic accountability reporting: on the investigation of black boxes. <em>NA</em>, 2014.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">DG17</span></dt>
<dd><p>Dheeru Dua and Casey Graff. UCI machine learning repository. 2017. URL: <a class="reference external" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">DHP+12</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>, ITCS '12, 214–226. New York, NY, USA, 2012. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>, <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">doi:10.1145/2090236.2090255</a>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">ES16</span></dt>
<dd><p>Harrison Edwards and Amos J. Storkey. Censoring representations with an adversary. In Yoshua Bengio and Yann LeCun, editors, <em>4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings</em>. 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1511.05897">http://arxiv.org/abs/1511.05897</a>.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">EG18</span></dt>
<dd><p>Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 11–21. Brussels, Belgium, October 2018. Association for Computational Linguistics. URL: <a class="reference external" href="https://www.aclweb.org/anthology/D18-1002">https://www.aclweb.org/anthology/D18-1002</a>, <a class="reference external" href="https://doi.org/10.18653/v1/D18-1002">doi:10.18653/v1/D18-1002</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">EJJ+19</span></dt>
<dd><p>Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, and Zachary Schutzman. Fair algorithms for learning in allocation problems. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 170–179. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">https://doi.org/10.1145/3287560.3287571</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">doi:10.1145/3287560.3287571</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">FKT+18</span></dt>
<dd><p>Golnoosh Farnadi, Pigi Kouki, Spencer K. Thompson, Sriram Srinivasan, and Lise Getoor. A fairness-aware hybrid recommender system. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.09030">http://arxiv.org/abs/1809.09030</a>, <a class="reference external" href="https://arxiv.org/abs/1809.09030">arXiv:1809.09030</a>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets">FGU21</span></dt>
<dd><p>Till Feier, Jan Gogoll, and Matthias Uhl. Hiding behind machines: when blame is shifted to artificial agents. 2021. <a class="reference external" href="https://arxiv.org/abs/2101.11465">arXiv:2101.11465</a>.</p>
</dd>
<dt class="label" id="id42"><span class="brackets">FFM+15</span></dt>
<dd><p>Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD '15, 259–268. New York, NY, USA, 2015. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">https://doi.org/10.1145/2783258.2783311</a>, <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">doi:10.1145/2783258.2783311</a>.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">FIKP20</span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. <em>2020 IEEE 36th International Conference on Data Engineering (ICDE)</em>, April 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1109/ICDE48307.2020.00203">http://dx.doi.org/10.1109/ICDE48307.2020.00203</a>, <a class="reference external" href="https://doi.org/10.1109/icde48307.2020.00203">doi:10.1109/icde48307.2020.00203</a>.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">FSV16</span></dt>
<dd><p>Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility of fairness. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1609.07236">http://arxiv.org/abs/1609.07236</a>.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">GUA+16</span></dt>
<dd><p>Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. <em>Journal of Machine Learning Research</em>, 17(59):1–35, 2016. URL: <a class="reference external" href="http://jmlr.org/papers/v17/15-239.html">http://jmlr.org/papers/v17/15-239.html</a>.</p>
</dd>
<dt class="label" id="id2"><span class="brackets">GSR+18</span></dt>
<dd><p>Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. <em>arXiv preprint arXiv:1807.01622</em>, 2018.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">GGLRe20</span></dt>
<dd><p>Karan Goel, Albert Gu, Yixuan Li, and Christopher Ré. Model patching: closing the subgroup performance gap with data augmentation. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">GCFW18</span></dt>
<dd><p>Maya R. Gupta, Andrew Cotter, Mahdi Milani Fard, and Serena Wang. Proxy fairness. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1806.11212">http://arxiv.org/abs/1806.11212</a>, <a class="reference external" href="https://arxiv.org/abs/1806.11212">arXiv:1806.11212</a>.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">HPS16</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 29</em>, 3315–3323. Curran Associates, Inc., 2016. URL: <a class="reference external" href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf</a>.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">HLGK19</span></dt>
<dd><p>Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. A moral framework for understanding fair ml through economic models of equality of opportunity. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, FAT* '19, 181–190. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">https://doi.org/10.1145/3287560.3287584</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287584">doi:10.1145/3287560.3287584</a>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">Hil20</span></dt>
<dd><p>Kashmir Hill. Wrongfully accused by an algorithm. June 2020. URL: <a class="reference external" href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</a>.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">HCMD18</span></dt>
<dd><p>J. Henry Hinnefeld, Peter Cooman, Nat Mammo, and Rupert Deese. Evaluating fairness metrics in the presence of dataset bias. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.09245">http://arxiv.org/abs/1809.09245</a>, <a class="reference external" href="https://arxiv.org/abs/1809.09245">arXiv:1809.09245</a>.</p>
</dd>
<dt class="label" id="id53"><span class="brackets">HLV16</span></dt>
<dd><p>Mark Holmstrom, Dylan Liu, and Christopher Vo. Machine learning applied to weather forecasting. <em>Stanford University</em>, pages 2–4, 2016.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">HOU18</span></dt>
<dd><p>OF LORDS HOUSE. Select committee on artificial intelligence. <em>AI in the UK: ready, willing and able</em>, 2018.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Hum00</span></dt>
<dd><p>David Hume. <em>An enquiry concerning human understanding: A critical edition</em>. Volume 3. Oxford University Press, 2000.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">Hwa18</span></dt>
<dd><p>Tim Hwang. Computational power and the social impact of artificial intelligence. <em>CoRR</em>, 2018. Available at SSRN 3147971. URL: <a class="reference external" href="http://arxiv.org/abs/1803.08971">http://arxiv.org/abs/1803.08971</a>, <a class="reference external" href="https://arxiv.org/abs/1803.08971">arXiv:1803.08971</a>.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">JN20</span></dt>
<dd><p>Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In Silvia Chiappa and Roberto Calandra, editors, <em>The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]</em>, volume 108 of Proceedings of Machine Learning Research, 702–712. PMLR, 2020.</p>
</dd>
<dt class="label" id="id88"><span class="brackets">KZ18</span></dt>
<dd><p>Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. In Jennifer G. Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018</em>, volume 80 of Proceedings of Machine Learning Research, 2444–2453. PMLR, 2018.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">Kam11</span></dt>
<dd><p>F. Kamiran. Discrimination-aware classification. Technical Report, Technische Universiteit Eindhoven, Eindhoven, 2011. URL: <a class="reference external" href="https://research.tue.nl/en/publications/discrimination-aware-classification">https://research.tue.nl/en/publications/discrimination-aware-classification</a>, <a class="reference external" href="https://doi.org/10.6100/IR717576">doi:10.6100/IR717576</a>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">KC09</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Classifying without discriminating. In <em>2009 2nd International Conference on Computer, Control and Communication, IC4 2009</em>. 2009. <a class="reference external" href="https://doi.org/10.1109/IC4.2009.4909197">doi:10.1109/IC4.2009.4909197</a>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">KC12</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1):1–33, 2012. <a class="reference external" href="https://doi.org/10.1007/s10115-011-0463-8">doi:10.1007/s10115-011-0463-8</a>.</p>
</dd>
<dt class="label" id="id98"><span class="brackets">KZC13</span></dt>
<dd><p>Faisal Kamiran, Indrė Žliobaitė, and Toon Calders. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. <em>Knowl Inf Syst</em>, 2013. URL: <a class="reference external" href="https://doi.org/10.1007/s10115-012-0584-8">https://doi.org/10.1007/s10115-012-0584-8</a>.</p>
</dd>
<dt class="label" id="id85"><span class="brackets">KAAS12</span></dt>
<dd><p>Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. In <em>European conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</em>, 35–50. 2012.</p>
</dd>
<dt class="label" id="id3"><span class="brackets">KCQ18</span></dt>
<dd><p>Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto. Interpretable fairness via target labels in gaussian process models. <em>arXiv preprint arXiv:1810.05598</em>, 2018.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">KBK+19</span></dt>
<dd><p>Niki Kilbertus, Philip J. Ball, Matt J. Kusner, Adrian Weller, and Ricardo Silva. The sensitivity of counterfactual fairness to unmeasured confounding. In <em>Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI</em>. 2019.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">KMR17</span></dt>
<dd><p>Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In <em>Innovations in Theoretical Computer Science Conference</em>, 43:1–43:23. 2017.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">KLRS17</span></dt>
<dd><p>Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 30</em>, pages 4066–4076. Curran Associates, Inc., 2017. URL: <a class="reference external" href="http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf">http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf</a>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">LDR+18</span></dt>
<dd><p>Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3150–3158. StockholmsmÃ€ssan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/liu18c.html">http://proceedings.mlr.press/v80/liu18c.html</a>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">LSSW19</span></dt>
<dd><p>Christos Louizos, Xiahan Shi, Klamer Schutte, and Max Welling. The functional neural process. <em>arXiv preprint arXiv:1906.08324</em>, 2019.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">LSL+16</span></dt>
<dd><p>Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. In <em>ICLR</em>. 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1511.00830">http://arxiv.org/abs/1511.00830</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">LRT11</span></dt>
<dd><p>Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. K-nn as an implementation of situation testing for discrimination discovery and prevention. <em>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11</em>, pages 502, 2011. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?doid=2020408.2020488">http://dl.acm.org/citation.cfm?doid=2020408.2020488</a>, <a class="reference external" href="https://doi.org/10.1145/2020408.2020488">doi:10.1145/2020408.2020488</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">MCPZ18a</span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Jennifer Dy and Andreas Krause, editors, <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of Proceedings of Machine Learning Research, 3384–3393. PMLR, 10–15 Jul 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/madras18a.html">http://proceedings.mlr.press/v80/madras18a.html</a>.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">MCPZ18b</span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Fairness through causal awareness: learning latent-variable models for biased data. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1809.02519">http://arxiv.org/abs/1809.02519</a>, <a class="reference external" href="https://arxiv.org/abs/1809.02519">arXiv:1809.02519</a>.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">Mil19</span></dt>
<dd><p>Tim Miller. Explanation in artificial intelligence: insights from the social sciences. <em>Artificial Intelligence</em>, 267:1–38, 2019. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0004370218305988">http://www.sciencedirect.com/science/article/pii/S0004370218305988</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2018.07.007">doi:https://doi.org/10.1016/j.artint.2018.07.007</a>.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">Mol18</span></dt>
<dd><p>Christoph Molnar. <em>Interpretable Machine Learning</em>. https://christophm.github.io/interpretable-ml-book/, 2018.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">MSP16</span></dt>
<dd><p>Cecilia Munoz, Megan Smith, and DJ Patil. Big data : a report on algorithmic systems , opportunity , and civil rights big data : a report on algorithmic systems , opportunity , and civil rights. <em>Executive Office of the President of USA</em>, 2016. URL: <a class="reference external" href="https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf">https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf</a>, <a class="reference external" href="https://doi.org/10.1177/0162243915598056">doi:10.1177/0162243915598056</a>.</p>
</dd>
<dt class="label" id="id72"><span class="brackets">NS18</span></dt>
<dd><p>Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In <em>Proceedings of the Thirty Second Conference on Association for the Advancement of Artificial Intelligence (AAAI-32nd)</em>. AAAI Press, 2018.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">Pea09</span></dt>
<dd><p>Judea Pearl. <em>Causality: Models, Reasoning and Inference</em>. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">PRT08</span></dt>
<dd><p>Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Ying Li, Bing Liu, and Sunita Sarawagi, editors, <em>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008</em>, 560–568. ACM, 2008. <a class="reference external" href="https://doi.org/10.1145/1401890.1401959">doi:10.1145/1401890.1401959</a>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">QS17</span></dt>
<dd><p>Novi Quadrianto and Viktoriia Sharmanska. Recycling privileged learning and distribution matching for fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">QST19</span></dt>
<dd><p>Novi Quadrianto, Viktoriia Sharmanska, and Oliver Thomas. Discovering fair representations in the data domain. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 8227–8236. 2019.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">Reu18</span></dt>
<dd><p>Reuters. Amazon ditched ai recruiting tool that favored men for technical jobs. Oct 2018. URL: <a class="reference external" href="https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine">https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">RSG16</span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. &quot;why should i trust you?&quot; explaining the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135–1144. 2016.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">RR83</span></dt>
<dd><p>Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. <em>Biometrika</em>, 70(1):41–55, 1983.</p>
</dd>
<dt class="label" id="id60"><span class="brackets">Rot88</span></dt>
<dd><p>Alvin E Roth. <em>The Shapley value: essays in honor of Lloyd S. Shapley</em>. Cambridge University Press, 1988.</p>
</dd>
<dt class="label" id="id91"><span class="brackets">Rub90</span></dt>
<dd><p>Donald B Rubin. Formal mode of statistical inference for causal effects. <em>Journal of statistical planning and inference</em>, 25(3):279–292, 1990.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">RKLS17</span></dt>
<dd><p>Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva. When worlds collide: integrating different counterfactual assumptions in fairness. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 30</em>, 6414–6423. 2017.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">SHCV18</span></dt>
<dd><p>Prasanna Sattigeri, Samuel C Hoffman, Vijil Chenthamarakshan, and Kush R Varshney. Fairness gan. <em>arXiv preprint arXiv:1805.09910</em>, 2018.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">SHDQ20</span></dt>
<dd><p>Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive examples for addressing the tyranny of the majority. <em>CoRR</em>, 2020.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">SSS+17</span></dt>
<dd><p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. <em>Nature</em>, 550:354 EP –, 10 2017. URL: <a class="reference external" href="http://dx.doi.org/10.1038/nature24270">http://dx.doi.org/10.1038/nature24270</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">SGSS16</span></dt>
<dd><p>Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, and Soumik Sarkar. Machine learning for high-throughput stress phenotyping in plants. <em>Trends in Plant Science</em>, 21(2):110–124, 2016. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1360138515002630">http://www.sciencedirect.com/science/article/pii/S1360138515002630</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.tplants.2015.10.015">doi:https://doi.org/10.1016/j.tplants.2015.10.015</a>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">Swe13</span></dt>
<dd><p>Latanya Sweeney. Discrimination in online ad delivery. <em>Commun. ACM</em>, 56(5):44–54, May 2013. URL: <a class="reference external" href="https://doi.org/10.1145/2447976.2447990">https://doi.org/10.1145/2447976.2447990</a>, <a class="reference external" href="https://doi.org/10.1145/2447976.2447990">doi:10.1145/2447976.2447990</a>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">Tol19</span></dt>
<dd><p>Songül Tolan. Fair and unbiased algorithmic decision making: current state and future challenges. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1901.04730">http://arxiv.org/abs/1901.04730</a>, <a class="reference external" href="https://arxiv.org/abs/1901.04730">arXiv:1901.04730</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">Vin17</span></dt>
<dd><p>James Vincent. Deepmind's ai became a superhuman chess player in a few hours, just for fun. December 2017. URL: <a class="reference external" href="https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go">https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go</a>.</p>
</dd>
<dt class="label" id="id61"><span class="brackets">WMF17</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Luciano Floridi. Transparent, explainable, and accountable ai for robotics. <em>NA</em>, 2017.</p>
</dd>
<dt class="label" id="id92"><span class="brackets">WMR20</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why fairness cannot be automated: bridging the gap between eu non-discrimination law and ai. <em>SSRN Electronic Journal</em>, 2020. URL: <a class="reference external" href="http://dx.doi.org/10.2139/ssrn.3547922">http://dx.doi.org/10.2139/ssrn.3547922</a>, <a class="reference external" href="https://doi.org/10.2139/ssrn.3547922">doi:10.2139/ssrn.3547922</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">WVP18</span></dt>
<dd><p>Christina Wadsworth, Francesca Vera, and Chris Piech. Achieving fairness through adversarial learning: an application to recidivism prediction. <em>CoRR</em>, 2018. URL: <a class="reference external" href="http://arxiv.org/abs/1807.00199">http://arxiv.org/abs/1807.00199</a>, <a class="reference external" href="https://arxiv.org/abs/1807.00199">arXiv:1807.00199</a>.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">WPT19</span></dt>
<dd><p>Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan. Unlocking fairness: a trade-off revisited. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle  Alché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems 32</em>, pages 8783–8792. Curran Associates, Inc., 2019. URL: <a class="reference external" href="http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf">http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf</a>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">XYZW18</span></dt>
<dd><p>Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: fairness-aware generative adversarial networks. In <em>2018 IEEE International Conference on Big Data (Big Data)</em>, 570–575. IEEE, 2018.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">YH17</span></dt>
<dd><p>Sirui Yao and Bert Huang. Beyond parity: fairness objectives for collaborative filtering. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id97"><span class="brackets">YT21</span></dt>
<dd><p>Samuel Yeom and Michael Carl Tschantz. Avoiding disparity amplification under different worldviews. In <em>ACM Conference on Fairness, Accountability, and Transparency</em>. 2021.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">ZVGRG19</span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness constraints: a flexible approach for fair classification. <em>Journal of Machine Learning Research</em>, 20(75):1–42, 2019. URL: <a class="reference external" href="http://jmlr.org/papers/v20/18-262.html">http://jmlr.org/papers/v20/18-262.html</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">ZBC+17</span></dt>
<dd><p>Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. Fa*ir: a fair top-k ranking algorithm. In <em>Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</em>, CIKM '17, 1569–1578. New York, NY, USA, 2017. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">https://doi.org/10.1145/3132847.3132938</a>, <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">doi:10.1145/3132847.3132938</a>.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">ZWS+13</span></dt>
<dd><p>Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Sanjoy Dasgupta and David McAllester, editors, <em>Proceedings of the 30th International Conference on Machine Learning</em>, volume 28 of Proceedings of Machine Learning Research, 325–333. Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v28/zemel13.html">http://proceedings.mlr.press/v28/zemel13.html</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">ZLM18</span></dt>
<dd><p>Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In <em>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</em>, AIES '18, 335–340. New York, NY, USA, 2018. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">https://doi.org/10.1145/3278721.3278779</a>, <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">doi:10.1145/3278721.3278779</a>.</p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="08_conclusion/conclusion.html" title="previous page">Conclusion</a>
    <a class='right-next' id="next-link" href="10_annual_review/summary.html" title="next page">Summary</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Oliver Thomas<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-170288604-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>