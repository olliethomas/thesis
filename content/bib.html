
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bibliography &#8212; Fair Representations in the Data Domain</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="oliverthomas.ml/content/bib.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Publications" href="09_appendix/published.html" />
    <link rel="prev" title="Appendix" href="00_thesis_pages/appendix.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-170288604-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/pal-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Fair Representations in the Data Domain</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Thesis
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_thesis_pages/front_page.html">
   Front Matter
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="00_thesis_pages/preliminaries.html">
   Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="01_introduction/intro.html">
     Chapter 1: Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_background/background.html">
     Chapter 2: Related Work
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="00_thesis_pages/content.html">
     Chapter 3: Content
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="00_thesis_pages/publications.html">
   Publications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="03_positive_action/intro.html">
     Chapter 4: An Algorithmic Framework for Positive Action
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_data_domain_fairness/intro.html">
     Chapter 5: Data Domain Fairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05_null_sampling/intro.html">
     Chapter 6: Null-sampling for Fair and Invariant Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="00_thesis_pages/conclusion.html">
   Conclusion
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="08_conclusion/conclusion.html">
     Chapter 7: Conclusion
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="00_thesis_pages/appendix.html">
   Appendix
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Bibliography
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="09_appendix/published.html">
   Publications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/positive_action.html">
     An Algorithmic Framework for Positive Action
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/dfritdd.html">
     Discovering Fair Representations in the Data Domain
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/nosinn.html">
     Null-Sampling for Invariant and Interpretable Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/publications/lit_review.html">
     Literature Review (Year 1 Annual Review)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="09_appendix/software.html">
   Software
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/ethicml.html">
     EthicML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/intervene.html">
     Casual Discovery Tool
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_appendix/software/conduit.html">
     Conduit
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/bib.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/olliethomas/thesis/issues/new?title=Issue%20on%20page%20%2Fcontent/bib.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/olliethomas/thesis/edit/master/content/bib.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Bibliography</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p id="id1"><dl class="citation">
<dt class="label" id="id2"><span class="brackets">AAG+18</span></dt>
<dd><p>Igor Adamski, Robert Adamski, Tomasz Grel, Adam Jędrych, Kamil Kaczmarek, and Henryk Michalewski. Distributed deep reinforcement learning: learn how to play atari games in 21Â minutes. In Rio Yokota, Michèle Weiland, David Keyes, and Carsten Trinitis, editors, <em>High Performance Computing</em>, 370–388. Cham, 2018. Springer International Publishing.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">ABDudik+18</span></dt>
<dd><p>Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\'ık, John Langford, and Hanna M. Wallach. A reductions approach to fair classification. In Jennifer G. Dy and Andreas Krause, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 80 of Proceedings of Machine Learning Research, 60–69. Pmlr, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">BH17</span></dt>
<dd><p>S Barocas and M Hardt. Fairness in machine learning. 2017. URL: <a class="reference external" href="https://nips.cc/Conferences/2017/Schedule?showEvent=8734">https://nips.cc/Conferences/2017/Schedule?showEvent=8734</a>.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">BS16</span></dt>
<dd><p>S. Barocas and A. Selbst. Big data's disparate impact. <em>California Law Review</em>, 104(3):671–732, 2016.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">BHN19</span></dt>
<dd><p>Solon Barocas, Moritz Hardt, and Arvind Narayanan. <em>Fairness and Machine Learning</em>. fairmlbook.org, 2019. <span><a class="reference external" href="#"></a></span>http://www.fairmlbook.org.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">Ben19</span></dt>
<dd><p>Jason R Bent. Is algorithmic affirmative action legal. <em>The Georgetown Law Journal</em>, 108:803, 2019.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">BM04</span></dt>
<dd><p>Marianne Bertrand and Sendhil Mullainathan. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimination. <em>American economic review</em>, 94(4):991–1013, 2004.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">BCZC17</span></dt>
<dd><p>Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. Data decisions and theoretical implications when adversarially learning fair representations. <em>ArXiv preprint</em>, 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1707.00075">https://arxiv.org/abs/1707.00075</a>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">BYF20</span></dt>
<dd><p>Emily Black, Samuel Yeom, and Matt Fredrikson. Fliptest: fairness testing via optimal transport. In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, Fat* '20, 111–121. New York, NY, USA, 2020. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">https://doi.org/10.1145/3351095.3372845</a>, <a class="reference external" href="https://doi.org/10.1145/3351095.3372845">doi:10.1145/3351095.3372845</a>.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">BCZ+16</span></dt>
<dd><p>Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NIPS)</em>, 4349–4357. 2016. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">BKN+17</span></dt>
<dd><p><strong>missing booktitle in BowKitNisStraVarVen17</strong></p>
</dd>
<dt class="label" id="id18"><span class="brackets">BP93</span></dt>
<dd><p>Elizabeth Brown and David I Perrett. What gives a face its gender? <em>Perception</em>, 22(7):829–840, 1993.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">BBH+93</span></dt>
<dd><p>Vicki Bruce, A Mike Burton, Elias Hanna, Pat Healey, Oli Mason, Anne Coombes, Rick Fright, and Alf Linney. Sex discrimination: how do we tell the difference between male and female faces? <em>Perception</em>, 22(2):131–152, 1993. Pmid: 8474840. URL: <a class="reference external" href="https://doi.org/10.1068/p220131">https://doi.org/10.1068/p220131</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1068/p220131">arXiv:https://doi.org/10.1068/p220131</a>, <a class="reference external" href="https://doi.org/10.1068/p220131">doi:10.1068/p220131</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">BG18</span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. Pmlr, 2018.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">BSOG18</span></dt>
<dd><p>Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced Neighborhoods for Multi-sided Fairness in Recommendation. <em>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</em>, 81(2008):202–214, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v81/burke18a.html">http://proceedings.mlr.press/v81/burke18a.html</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">CV10</span></dt>
<dd><p>Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. <em>Data Mining and Knowledge Discovery</em>, 21(2):277–292, 2010. URL: <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">https://doi.org/10.1007/s10618-010-0190-x</a>, <a class="reference external" href="https://doi.org/10.1007/s10618-010-0190-x">doi:10.1007/s10618-010-0190-x</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">Chi19</span></dt>
<dd><p>Silvia Chiappa. Path-specific counterfactual fairness. In <em>AAAI Conference on Artificial Intelligence</em>, 7801–7808. AAAI Press, 2019. URL: <a class="reference external" href="https://doi.org/10.1609/aaai.v33i01.33017801">https://doi.org/10.1609/aaai.v33i01.33017801</a>, <a class="reference external" href="https://doi.org/10.1609/aaai.v33i01.33017801">doi:10.1609/aaai.v33i01.33017801</a>.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">CCK+18</span></dt>
<dd><p>Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: unified generative adversarial networks for multi-domain image-to-image translation. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 8789–8797. IEEE Computer Society, 2018. URL: <a class="reference external" href="http://openaccess.thecvf.com/content\%5Fcvpr\%5F2018/html/Choi\%5FStarGAN\%5FUnified\%5FGenerative\%5FCVPR\%5F2018\%5Fpaper.html">http://openaccess.thecvf.com/content\%5Fcvpr\%5F2018/html/Choi\%5FStarGAN\%5FUnified\%5FGenerative\%5FCVPR\%5F2018\%5Fpaper.html</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr.2018.00916">doi:10.1109/cvpr.2018.00916</a>.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">Cho17</span></dt>
<dd><p>Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. <em>Big Data</em>, 5(2):153–163, 2017. Pmid: 28632438. URL: <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1089/big.2016.0047">arXiv:https://doi.org/10.1089/big.2016.0047</a>, <a class="reference external" href="https://doi.org/10.1089/big.2016.0047">doi:10.1089/big.2016.0047</a>.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">CP14</span></dt>
<dd><p>Danielle Keats Citron and Frank Pasquale. The scored society: due process for automated predictions. <em>Wash. L. Rev.</em>, 89:1, 2014.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">CDG18</span></dt>
<dd><p>S Corbett-Davies and S Goel. Defining and designing fair algorithms. 2018. ICML 2018 Tutorial. URL: <a class="reference external" href="https://icml.cc/Conferences/2018/Schedule?showEvent=1862">https://icml.cc/Conferences/2018/Schedule?showEvent=1862</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">Dia14</span></dt>
<dd><p><strong>missing booktitle in Dia14</strong></p>
</dd>
<dt class="label" id="id42"><span class="brackets">DHP+12</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>, Itcs '12, 214–226. New York, NY, USA, 2012. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>, <a class="reference external" href="https://doi.org/10.1145/2090236.2090255">doi:10.1145/2090236.2090255</a>.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">DI19</span></dt>
<dd><p>Cynthia Dwork and Christina Ilvento. Fairness under composition. In <em>Innovations in Theoretical Computer Science (ITCS)</em>. 2019.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">ES16</span></dt>
<dd><p>Harrison Edwards and Amos J. Storkey. Censoring representations with an adversary. In Yoshua Bengio and Yann LeCun, editors, <em>International Conference on Learning Representations (ICLR)</em>. 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1511.05897">http://arxiv.org/abs/1511.05897</a>.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">EG18</span></dt>
<dd><p>Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In <em>Proc. of EMNLP</em>, 11–21. Brussels, Belgium, 2018. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/D18-1002">https://aclanthology.org/D18-1002</a>, <a class="reference external" href="https://doi.org/10.18653/v1/D18-1002">doi:10.18653/v1/D18-1002</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">EJJ+19</span></dt>
<dd><p>Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, and Zachary Schutzman. Fair algorithms for learning in allocation problems. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, Fat\textasteriskcentered '19, 170–179. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">https://doi.org/10.1145/3287560.3287571</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287571">doi:10.1145/3287560.3287571</a>.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">FFM+15</span></dt>
<dd><p>Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Longbing Cao, Chengqi Zhang, Thorsten Joachims, Geoffrey I. Webb, Dragos D. Margineantu, and Graham Williams, editors, <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015</em>, 259–268. Acm, 2015. URL: <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">https://doi.org/10.1145/2783258.2783311</a>, <a class="reference external" href="https://doi.org/10.1145/2783258.2783311">doi:10.1145/2783258.2783311</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">FIKP20</span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. <em>Proc. of ICDE</em>, 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1109/ICDE48307.2020.00203">http://dx.doi.org/10.1109/ICDE48307.2020.00203</a>, <a class="reference external" href="https://doi.org/10.1109/icde48307.2020.00203">doi:10.1109/icde48307.2020.00203</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">FSV+18</span></dt>
<dd><p>Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P. Hamilton, and Derek Roth. A comparative study of fairness-enhancing interventions in machine learning. <em>ArXiv preprint</em>, 2018. URL: <a class="reference external" href="https://arxiv.org/abs/1802.04422">https://arxiv.org/abs/1802.04422</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">FHH14</span></dt>
<dd><p>S. Fu, H. He, and Z. Hou. Learning race from face: a survey. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 36(12):2483–2509, 2014. <a class="reference external" href="https://doi.org/10.1109/tpami.2014.2321570">doi:10.1109/tpami.2014.2321570</a>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">GBR+07</span></dt>
<dd><p>Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander J Smola. A kernel approach to comparing distributions. In <em>Proceedings of the 22. AAAI Conference on Artificial Intelligence</em>, 1637–1641. Menlo Park, CA, USA, 2007. Max-Planck-Gesellschaft, AAAI Press.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">GBSScholkopf05</span></dt>
<dd><p>Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. Measuring statistical dependence with hilbert-schmidt norms. In Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, editors, <em>Algorithmic Learning Theory</em>, 63–77. Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">GHRGW18</span></dt>
<dd><p>Nina Grgic-Hlaca, Elissa M. Redmiles, Krishna P. Gummadi, and Adrian Weller. Human perceptions of fairness in algorithmic decision making: a case study of criminal risk prediction. In Pierre-Antoine Champin, Fabien L. Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis, editors, <em>Proceedings of the 2018 World Wide Web Conference</em>, Www '18, 903–912. Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee. URL: <a class="reference external" href="https://doi.org/10.1145/3178876.3186138">https://doi.org/10.1145/3178876.3186138</a>, <a class="reference external" href="https://doi.org/10.1145/3178876.3186138">doi:10.1145/3178876.3186138</a>.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">HPS16</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NIPS)</em>, 3315–3323. 2016. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">HW19</span></dt>
<dd><p>Zach Harned and Hanna Wallach. Stretching human laws to apply to machines: the dangers of a “colorblind” computer. <em>Florida State Univeristy Law Review</em>, 47:617, 2019.</p>
</dd>
<dt class="label" id="id71"><span class="brackets">HZRS16</span></dt>
<dd><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–778. IEEE Computer Society, 2016. URL: <a class="reference external" href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr.2016.90">doi:10.1109/cvpr.2016.90</a>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">Hum00</span></dt>
<dd><p>David Hume. <em>An enquiry concerning human understanding: A critical edition</em>. Volume 3. Oxford University Press, 2000.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">IZZE17</span></dt>
<dd><p>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 5967–5976. IEEE Computer Society, 2017. URL: <a class="reference external" href="https://doi.org/10.1109/CVPR.2017.632">https://doi.org/10.1109/CVPR.2017.632</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr.2017.632">doi:10.1109/cvpr.2017.632</a>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">JN20</span></dt>
<dd><p>Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In Silvia Chiappa and Roberto Calandra, editors, <em>The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]</em>, volume 108 of Proceedings of Machine Learning Research, 702–712. Pmlr, 2020. URL: <a class="reference external" href="http://proceedings.mlr.press/v108/jiang20a.html">http://proceedings.mlr.press/v108/jiang20a.html</a>.</p>
</dd>
<dt class="label" id="id85"><span class="brackets">JAFF16</span></dt>
<dd><p>Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In <em>European Conference on Computer Vision (ECCV)</em>. 2016.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">Jos19</span></dt>
<dd><p>Shalmali an Joshi. Towards realistic individual recourse and actionable explanation. <em>ArXiv preprint</em>, 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">KZ18</span></dt>
<dd><p>Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. In Jennifer G. Dy and Andreas Krause, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 80 of Proceedings of Machine Learning Research, 2444–2453. Pmlr, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/kallus18a.html">http://proceedings.mlr.press/v80/kallus18a.html</a>.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">KC09</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Classifying without discriminating. In <em>2009 2nd International Conference on Computer, Control and Communication, IC4 2009</em>. 2009. <a class="reference external" href="https://doi.org/10.1109/ic4.2009.4909197">doi:10.1109/ic4.2009.4909197</a>.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">KC12</span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1):1–33, 2012. <a class="reference external" href="https://doi.org/10.1007/s10115-011-0463-8">doi:10.1007/s10115-011-0463-8</a>.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">KScholkopfV21</span></dt>
<dd><p>Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT '21, 353–362. New York, NY, USA, 2021. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">https://doi.org/10.1145/3442188.3445899</a>, <a class="reference external" href="https://doi.org/10.1145/3442188.3445899">doi:10.1145/3442188.3445899</a>.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">KNRW18</span></dt>
<dd><p>Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In Jennifer G. Dy and Andreas Krause, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 80 of Proceedings of Machine Learning Research, 2569–2577. Pmlr, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/kearns18a.html">http://proceedings.mlr.press/v80/kearns18a.html</a>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">KCQ20</span></dt>
<dd><p>Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto. Tuning fairness by balancing target labels. <em>Frontiers in Artificial Intelligence</em>, 3:33, 2020. URL: <a class="reference external" href="https://www.frontiersin.org/article/10.3389/frai.2020.00033">https://www.frontiersin.org/article/10.3389/frai.2020.00033</a>, <a class="reference external" href="https://doi.org/10.3389/frai.2020.00033">doi:10.3389/frai.2020.00033</a>.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Keh21</span></dt>
<dd><p>Thomas Maximilian Kehrenberg. <em>Learning with biased data: invariant representations and target labels</em>. PhD thesis, University of Sussex, September 2021. URL: <a class="reference external" href="http://sro.sussex.ac.uk/id/eprint/101574/">http://sro.sussex.ac.uk/id/eprint/101574/</a>.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">KB15</span></dt>
<dd><p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, <em>International Conference on Learning Representations (ICLR)</em>. 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>.</p>
</dd>
<dt class="label" id="id104"><span class="brackets">KLRS17</span></dt>
<dd><p>Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NIPS)</em>, 4066–4076. 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">LH15</span></dt>
<dd><p>G. Levi and T. Hassncer. Age and gender classification using convolutional neural networks. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 34–42. 2015. <a class="reference external" href="https://doi.org/10.1109/cvprw.2015.7301352">doi:10.1109/cvprw.2015.7301352</a>.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">MCPZ18</span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Learning adversarially fair and transferable representations. In Jennifer G. Dy and Andreas Krause, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 80 of Proceedings of Machine Learning Research, 3381–3390. Pmlr, 2018. URL: <a class="reference external" href="http://proceedings.mlr.press/v80/madras18a.html">http://proceedings.mlr.press/v80/madras18a.html</a>.</p>
</dd>
<dt class="label" id="id117"><span class="brackets">MPZ18</span></dt>
<dd><p>David Madras, Toniann Pitassi, and Richard S. Zemel. Predict responsibly: improving fairness and accuracy by learning to defer. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 6150–6160. 2018. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html">https://proceedings.neurips.cc/paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">MV15</span></dt>
<dd><p>Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 5188–5196. IEEE Computer Society, 2015. URL: <a class="reference external" href="https://doi.org/10.1109/CVPR.2015.7299155">https://doi.org/10.1109/CVPR.2015.7299155</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr.2015.7299155">doi:10.1109/cvpr.2015.7299155</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">MRSFS19</span></dt>
<dd><p>Michele Merler, Nalini K. Ratha, Rogério Schmidt Feris, and John R. Smith. Diversity in faces. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1901.10436">http://arxiv.org/abs/1901.10436</a>.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">Mil19</span></dt>
<dd><p>Tim Miller. Explanation in artificial intelligence: insights from the social sciences. <em>Artificial Intelligence</em>, 267:1–38, 2019. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0004370218305988">http://www.sciencedirect.com/science/article/pii/S0004370218305988</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2018.07.007">doi:https://doi.org/10.1016/j.artint.2018.07.007</a>.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">MO14</span></dt>
<dd><p>Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. <em>ArXiv preprint</em>, 2014. URL: <a class="reference external" href="https://arxiv.org/abs/1411.1784">https://arxiv.org/abs/1411.1784</a>.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">MJPScholkopf09</span></dt>
<dd><p>Joris M. Mooij, Dominik Janzing, Jonas Peters, and Bernhard Schölkopf. Regression by dependence minimization and its application to causal inference in additive noise models. In Andrea Pohoreckyj Danyluk, Léon Bottou, and Michael L. Littman, editors, <em>Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009</em>, volume 382 of ACM International Conference Proceeding Series, 745–752. Acm, 2009. URL: <a class="reference external" href="https://doi.org/10.1145/1553374.1553470">https://doi.org/10.1145/1553374.1553470</a>, <a class="reference external" href="https://doi.org/10.1145/1553374.1553470">doi:10.1145/1553374.1553470</a>.</p>
</dd>
<dt class="label" id="id124"><span class="brackets">MS20</span></dt>
<dd><p>Hussein Mozannar and David A. Sontag. Consistent estimators for learning to defer to an expert. In <em>International Conference on Machine Learning (ICML)</em>, volume 119 of Proceedings of Machine Learning Research, 7076–7087. Pmlr, 2020. URL: <a class="reference external" href="http://proceedings.mlr.press/v119/mozannar20b.html">http://proceedings.mlr.press/v119/mozannar20b.html</a>.</p>
</dd>
<dt class="label" id="id128"><span class="brackets">PCC+21</span></dt>
<dd><p>Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, and Hyunjung Shim. Multiple heads are better than one: few-shot font generation with multiple localized experts. 2021. <a class="reference external" href="https://arxiv.org/abs/2104.00887">arXiv:2104.00887</a>.</p>
</dd>
<dt class="label" id="id129"><span class="brackets">Pea09</span></dt>
<dd><p>Judea Pearl. <em>Causality</em>. Cambridge University Press, Cambridge, 2 edition, 2009. <a class="reference external" href="https://doi.org/10.1017/cbo9780511803161">doi:10.1017/cbo9780511803161</a>.</p>
</dd>
<dt class="label" id="id130"><span class="brackets">PRT08</span></dt>
<dd><p>Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Ying Li, Bing Liu, and Sunita Sarawagi, editors, <em>Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08</em>, 560–568. Acm, 2008. <a class="reference external" href="https://doi.org/10.1145/1401890.1401959">doi:10.1145/1401890.1401959</a>.</p>
</dd>
<dt class="label" id="id132"><span class="brackets">QS17</span></dt>
<dd><p>Novi Quadrianto and Viktoriia Sharmanska. Recycling privileged learning and distribution matching for fairness. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NIPS)</em>, 677–688. 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id133"><span class="brackets">RR07</span></dt>
<dd><p>Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, <em>Advances in Neural Information Processing Systems (NIPS)</em>, 1177–1184. Curran Associates, Inc., 2007. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html">https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id135"><span class="brackets">RR83</span></dt>
<dd><p>Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. <em>Biometrika</em>, 70(1):41–55, 1983.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">Rub90</span></dt>
<dd><p>Donald B Rubin. Formal mode of statistical inference for causal effects. <em>Journal of statistical planning and inference</em>, 25(3):279–292, 1990.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">Sal18</span></dt>
<dd><p>Pedro an Saleiro. Aequitas: a bias and fairness audit toolkit. <em>ArXiv preprint</em>, 2018. URL: <a class="reference external" href="https://arxiv.org/abs/1811.05577">https://arxiv.org/abs/1811.05577</a>.</p>
</dd>
<dt class="label" id="id146"><span class="brackets">SJS17</span></dt>
<dd><p>Uri Shalit, Fredrik D. Johansson, and David A. Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In Doina Precup and Yee Whye Teh, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 70 of Proceedings of Machine Learning Research, 3076–3085. Pmlr, 2017. URL: <a class="reference external" href="http://proceedings.mlr.press/v70/shalit17a.html">http://proceedings.mlr.press/v70/shalit17a.html</a>.</p>
</dd>
<dt class="label" id="id145"><span class="brackets">SHDQ20</span></dt>
<dd><p>Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive examples for addressing the tyranny of the majority. <em>ArXiv preprint</em>, 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2004.06524">https://arxiv.org/abs/2004.06524</a>.</p>
</dd>
<dt class="label" id="id148"><span class="brackets">SBW21</span></dt>
<dd><p>Joshua Simons, Sophia Adams Bhatti, and Adrian Weller. Machine learning and the meaning of equal treatment. <em>Aies</em>, 2021.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">SZ15</span></dt>
<dd><p>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, <em>International Conference on Learning Representations (ICLR)</em>. 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</p>
</dd>
<dt class="label" id="id150"><span class="brackets">SGSS16</span></dt>
<dd><p>Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, and Soumik Sarkar. Machine learning for high-throughput stress phenotyping in plants. <em>Trends in Plant Science</em>, 21(2):110–124, 2016. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1360138515002630">http://www.sciencedirect.com/science/article/pii/S1360138515002630</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.tplants.2015.10.015">doi:https://doi.org/10.1016/j.tplants.2015.10.015</a>.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">Tol19</span></dt>
<dd><p>Songül Tolan. Fair and unbiased algorithmic decision making: current state and future challenges. <em>ArXiv preprint</em>, 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1901.04730">https://arxiv.org/abs/1901.04730</a>.</p>
</dd>
<dt class="label" id="id157"><span class="brackets">ULVL16</span></dt>
<dd><p>Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S. Lempitsky. Texture networks: feed-forward synthesis of textures and stylized images. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, <em>International Conference on Machine Learning (ICML)</em>, volume 48 of JMLR Workshop and Conference Proceedings, 1349–1357. JMLR.org, 2016. URL: <a class="reference external" href="http://proceedings.mlr.press/v48/ulyanov16.html">http://proceedings.mlr.press/v48/ulyanov16.html</a>.</p>
</dd>
<dt class="label" id="id156"><span class="brackets">UVL17</span></dt>
<dd><p>Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 4105–4113. IEEE Computer Society, 2017. URL: <a class="reference external" href="http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.437">http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.437</a>, <a class="reference external" href="https://doi.org/10.1109/cvpr.2017.437">doi:10.1109/cvpr.2017.437</a>.</p>
</dd>
<dt class="label" id="id158"><span class="brackets">USL19</span></dt>
<dd><p>Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, Fat\textasteriskcentered '19, 10–19. New York, NY, USA, 2019. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">https://doi.org/10.1145/3287560.3287566</a>, <a class="reference external" href="https://doi.org/10.1145/3287560.3287566">doi:10.1145/3287560.3287566</a>.</p>
</dd>
<dt class="label" id="id160"><span class="brackets">Vin17</span></dt>
<dd><p>James Vincent. Deepmind's ai became a superhuman chess player in a few hours, just for fun. 2017. URL: <a class="reference external" href="https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go">https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go</a>.</p>
</dd>
<dt class="label" id="id162"><span class="brackets">WMR18</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: automated decisions and the gdpr. <em>Harvard Journal of Law &amp; Technology</em>, 2018.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">WMR20</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why fairness cannot be automated: bridging the gap between eu non-discrimination law and ai. <em>SSRN Electronic Journal</em>, 2020. URL: <a class="reference external" href="http://dx.doi.org/10.2139/ssrn.3547922">http://dx.doi.org/10.2139/ssrn.3547922</a>, <a class="reference external" href="https://doi.org/10.2139/ssrn.3547922">doi:10.2139/ssrn.3547922</a>.</p>
</dd>
<dt class="label" id="id165"><span class="brackets">WPT19</span></dt>
<dd><p>Michael L. Wick, Swetasudha Panda, and Jean-Baptiste Tristan. Unlocking fairness: a trade-off revisited. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 8780–8789. 2019. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html">https://proceedings.neurips.cc/paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html</a>.</p>
</dd>
<dt class="label" id="id166"><span class="brackets">Xia21</span></dt>
<dd><p>Alice Xiang. Reconciling legal and technical approaches to algorithmic bias. <em>Tennessee Law Review</em>, 2021. URL: <a class="reference external" href="https://ssrn.com/abstract=3650635">https://ssrn.com/abstract=3650635</a>.</p>
</dd>
<dt class="label" id="id173"><span class="brackets">ZVGomezRodriguezG17</span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment &amp; disparate impact: learning classification without disparate mistreatment. In Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich, editors, <em>Proceedings of the 26th International Conference on World Wide Web</em>, Www '17, 1171–1180. Republic and Canton of Geneva, CHE, 2017. Acm. URL: <a class="reference external" href="https://doi.org/10.1145/3038912.3052660">https://doi.org/10.1145/3038912.3052660</a>, <a class="reference external" href="https://doi.org/10.1145/3038912.3052660">doi:10.1145/3038912.3052660</a>.</p>
</dd>
<dt class="label" id="id175"><span class="brackets">ZBC+17</span></dt>
<dd><p>Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. Fa*ir: A fair top-k ranking algorithm. In Ee-Peng Lim, Marianne Winslett, Mark Sanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C. Ho, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun, Vincent S. Tseng, and Chenliang Li, editors, <em>Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017</em>, 1569–1578. Acm, 2017. URL: <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">https://doi.org/10.1145/3132847.3132938</a>, <a class="reference external" href="https://doi.org/10.1145/3132847.3132938">doi:10.1145/3132847.3132938</a>.</p>
</dd>
<dt class="label" id="id177"><span class="brackets">ZLM18</span></dt>
<dd><p>Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In <em>Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</em>, Aies '18, 335–340. New York, NY, USA, 2018. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">https://doi.org/10.1145/3278721.3278779</a>, <a class="reference external" href="https://doi.org/10.1145/3278721.3278779">doi:10.1145/3278721.3278779</a>.</p>
</dd>
<dt class="label" id="id180"><span class="brackets">ZPIE17</span></dt>
<dd><p>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In <em>IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017</em>, 2242–2251. IEEE Computer Society, 2017. URL: <a class="reference external" href="https://doi.org/10.1109/ICCV.2017.244">https://doi.org/10.1109/ICCV.2017.244</a>, <a class="reference external" href="https://doi.org/10.1109/iccv.2017.244">doi:10.1109/iccv.2017.244</a>.</p>
</dd>
<dt class="label" id="id164"><span class="brackets">GlobalFCoHR20161818</span></dt>
<dd><p>Global Future Council on Human Rights 2016-18. How to prevent discriminatory outcomes in machine learning. Technical Report, World Economic Forum, 2018.</p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="00_thesis_pages/appendix.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Appendix</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="09_appendix/published.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Publications</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Oliver Thomas<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>