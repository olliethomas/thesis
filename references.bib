@inproceedings{AdaAdaGreJedKacMic18,
  title          = {Distributed Deep Reinforcement Learning: Learn How to Play Atari Games in 21\^{A}~minutes},
  author         = {
    Adamski, Igor and Adamski, Robert and Grel, Tomasz and J{\k{e}}drych, Adam and Kaczmarek, Kamil
    and Michalewski, Henryk
  },
  year           = 2018,
  booktitle      = {High Performance Computing},
  publisher      = {Springer International Publishing},
  address        = {Cham},
  pages          = {370--388},
  isbn           = {978-3-319-92040-5},
  abstract       = {
    We present a study in Distributed Deep Reinforcement Learning (DDRL) focused on scalability of
    a state-of-the-art Deep Reinforcement Learning algorithm known as Batch Asynchronous Advantage
    Actor-Critic (BA3C). We show that using the Adam optimization algorithm with a batch size of
    up\^{A}~to 2048 is a viable choice for carrying out large scale machine learning computations.
    This, combined with careful reexamination of the optimizer's hyperparameters, using synchronous
    training on the node level (while keeping the local, single node part of the algorithm
    asynchronous) and minimizing the model's memory footprint, allowed us to achieve linear scaling
    for up\^{A}~to 64 CPU nodes. This corresponds to a training time of 21\^{A}~min on 768 CPU
    cores, as opposed to the 10\^{A}~h required when using a single node with 24 cores achieved by
    a baseline single-node implementation.
  },
  editor         = {Yokota, Rio and Weiland, Mich{\`e}le and Keyes, David and Trinitis, Carsten}
}
@inproceedings{AdeGhaWel18,
  title          = {Discovering Interpretable Representations for Both Deep Generative and Discriminative Models},
  author         = {Tameem Adel and Zoubin Ghahramani and Adrian Weller},
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {50--59},
  url            = {http://proceedings.mlr.press/v80/adel18a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/AdelGW18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@inproceedings{AgaBeyDudLanWal18,
  title          = {A Reductions Approach to Fair Classification},
  author         = {
    Alekh Agarwal and Alina Beygelzimer and Miroslav Dud{\'{\i}}k and John Langford and Hanna M.
    Wallach
  },
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {60--69},
  url            = {http://proceedings.mlr.press/v80/agarwal18a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/AgarwalBD0W18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@article{Ana18,
  title          = {
    {Seeing without knowing : Limitations of the transparency ideal and its application to
    algorithmic accountability}
  },
  author         = {Ananny, Mike},
  year           = 2018,
  doi            = {10.1177/1461444816676645},
  keywords       = {accountability, algorithms, critical infrastructure studies, platform governance}
}
@misc{AngLarKirMat16,
  title          = {Machine Bias},
  author         = {Angwin, Julia and Larson, Jeff and Kirchner, Lauren and Mattu, Surya},
  year           = 2016,
  journal        = {ProPublica},
  url            = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}
}
@article{ArdLutKruRotKot19,
  title          = {Guided Image Generation with Conditional Invertible Neural Networks},
  author         = {
    Lynton Ardizzone and Carsten L{\"{u}}th and Jakob Kruse and Carsten Rother and Ullrich
    K{\"{o}}the
  },
  year           = 2019,
  journal        = {CoRR},
  volume         = {abs/1907.02392},
  url            = {http://arxiv.org/abs/1907.02392},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1907-02392.bib},
  eprint         = {1907.02392},
  eprinttype     = {arXiv},
  timestamp      = {Mon, 08 Jul 2019 14:12:33 +0200}
}
@article{ArjBotGulLop19,
  title          = {Invariant Risk Minimization},
  author         = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  year           = 2019,
  eprint         = {1907.02893},
  journal        = {arXiv}
}
@misc{BarHar17,
  title          = {Fairness in Machine Learning},
  author         = {Barocas, S and Hardt, M},
  journal        = {NeurIPS 2017},
  publisher      = {NeurIPS},
  url            = {https://nips.cc/Conferences/2017/Schedule?showEvent=8734},
  year           = {2017}
}
@book{BarHarNar19,
  title          = {Fairness and Machine Learning},
  author         = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year           = 2019,
  publisher      = {fairmlbook.org},
  note           = {\url{http://www.fairmlbook.org}}
}
@article{BarSel16,
  title          = {Big data's disparate impact},
  author         = {Barocas, S. and Selbst, A.},
  year           = 2016,
  journal        = {California Law Review},
  volume         = 104,
  number         = 3,
  pages          = {671--732}
}
@article{Ben19,
  title          = {Is algorithmic affirmative action legal},
  author         = {Bent, Jason R},
  year           = 2019,
  journal        = {The Georgetown Law Journal},
  publisher      = {HeinOnline},
  volume         = 108,
  pages          = 803
}
@article{BerMul04,
  title          = {
    Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market
    discrimination
  },
  author         = {Bertrand, Marianne and Mullainathan, Sendhil},
  year           = 2004,
  journal        = {American economic review},
  volume         = 94,
  number         = 4,
  pages          = {991--1013}
}
@article{BeuCheZhaChi17,
  title          = {Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations},
  author         = {Beutel, Alex and Chen, Jilin and Zhao, Zhe and Chi, Ed H.},
  year           = 2017,
  journal        = {ArXiv preprint},
  booktitle      = {Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML)},
  volume         = {abs/1707.00075},
  url            = {https://arxiv.org/abs/1707.00075}
}
@inproceedings{BlaYeoFre20,
  title          = {FlipTest: Fairness Testing via Optimal Transport},
  author         = {Black, Emily and Yeom, Samuel and Fredrikson, Matt},
  year           = 2020,
  booktitle      = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  location       = {Barcelona, Spain},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Fat* '20},
  pages          = {111--121},
  doi            = {10.1145/3351095.3372845},
  isbn           = 9781450369367,
  url            = {https://doi.org/10.1145/3351095.3372845},
  abstract       = {
    We present FlipTest, a black-box technique for uncovering discrimination in classifiers.
    FlipTest is motivated by the intuitive question: had an individual been of a different
    protected status, would the model have treated them differently? Rather than relying on causal
    information to answer this question, FlipTest leverages optimal transport to match individuals
    in different protected groups, creating similar pairs of in-distribution samples. We show how
    to use these instances to detect discrimination by constructing a flipset: the set of
    individuals whose classifier output changes post-translation, which corresponds to the set of
    people who may be harmed because of their group membership. To shed light on why the model
    treats a given subgroup differently, FlipTest produces a transparency report: a ranking of
    features that are most associated with the model's behavior on the flipset. Evaluating the
    approach on three case studies, we show that this provides a computationally inexpensive way to
    identify subgroups that may be harmed by model discrimination, including in cases where the
    model satisfies group fairness criteria.
  },
  keywords       = {fairness, disparate impact, optimal transport, machine learning},
  numpages       = 11
}
@inproceedings{BolChaZouSalKal16,
  title          = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author         = {
    Tolga Bolukbasi and Kai{-}Wei Chang and James Y. Zou and Venkatesh Saligrama and Adam Tauman
    Kalai
  },
  year           = 2016,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {4349--4357},
  url            = {https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/BolukbasiCZSK16.bib},
  editor         = {Daniel D. Lee and Masashi Sugiyama and Ulrike von Luxburg and Isabelle Guyon and Roman Garnett},
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{BowKitNisStraVarVen17,
author           = {Bower, Amanda and
                    Kitchen, Sarah~N. and
                    Niss, Laura and
                    Strauss, Martin~J. and
                    Vargo, Alexander and
                    Venkatasubramanian, Suresh},
title            = {Fair Pipelines},
year             = {2017},
url              = {https://arxiv.org/abs/1707.00391},
numpages         = {5},
location         = {Halifax, NS, Canada},
series           = {FAT/ML '17}
}
@article{BroPer93,
  title          = {What gives a face its gender?},
  author         = {Brown, Elizabeth and
                    Perrett, David~I},
  year           = 1993,
  journal        = {Perception},
  volume         = 22,
  number         = 7,
  pages          = {829--840}
}
@article{BroSan20,
  author         = {Brown, Noam and
                   Sandholm, Tuomas},
  title          = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  journal        = {Science},
  volume         = {359},
  number         = {6374},
  pages          = {418-424},
  year           = {2018},
  doi            = {10.1126/science.aao1733},
  URL            = {https://www.science.org/doi/abs/10.1126/science.aao1733},
  eprint         = {https://www.science.org/doi/pdf/10.1126/science.aao1733},
  abstract       = {Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent. Science, this issue p. 418 An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals. No-limit Texas hold’em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold’em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy. }
}
@article{BruBurHan93,
  title          = {Sex Discrimination: How Do We Tell the Difference between Male and Female Faces?},
  author         = {
    Bruce, Vicki and Burton, A~Mike and Hanna, Elias and Healey, Pat and Mason, Oli and Coombes,
    Anne and Fright, Rick and Linney, Alf
  },
  year           = 1993,
  journal        = {Perception},
  volume         = 22,
  number         = 2,
  pages          = {131--152},
  doi            = {10.1068/p220131},
  url            = {https://doi.org/10.1068/p220131},
  note           = {Pmid: 8474840},
  abstract       = {
    People are remarkably accurate (approaching ceiling) at deciding whether faces are male or
    female, even when cues from hairstyle, makeup, and facial hair are minimised. Experiments
    designed to explore the perceptual basis of our ability to categorise the sex of faces are
    reported. Subjects were considerably less accurate when asked to judge the sex of
    three-dimensional (3-D) representations of faces obtained by laser-scanning, compared with a
    condition where photographs were taken with hair concealed and eyes closed. This suggests that
    cues from features such as eyebrows, and skin texture, play an important role in
    decisionmaking. Performance with the laser-scanned heads remained quite high with 3/4-view
    faces, where the 3-D shape of the face should be easiest to see, suggesting that the 3-D
    structure of the face is a further source of information contributing to the classification of
    its sex. Performance at judging the sex from photographs (with hair concealed) was disrupted if
    the photographs were inverted, which implies that the superficial cues contributing to the
    decision are not processed in a purely `local' way. Performance was also disrupted if the faces
    were shown in photographic negatives, which is consistent with the use of 3-D information,
    since negation probably operates by disrupting the computation of shape from shading. In 3-D,
    the `average' male face differs from the `average' female face by having a more protuberant
    nose/brow and more prominent chin/jaw. The effects of manipulating the shapes of the noses and
    chins of the laser-scanned heads were assessed and significant effects of such manipulations on
    the apparent masculinity or femininity of the heads were revealed. It appears that our ability
    to make this most basic of facial categorisations may be multiply determined by a combination
    of 2-D, 3-D, and textural cues and their interrelationships.
  },
  eprint         = {https://doi.org/10.1068/p220131}
}
@inproceedings{BuoGeb18,
  title          = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author         = {Buolamwini, Joy and Gebru, Timnit},
  year           = 2018,
  booktitle      = {Conference on fairness, accountability and transparency},
  pages          = {77--91},
  organization   = {Pmlr}
}
@article{BurSonOrd08,
  title          = {{Balanced Neighborhoods for Multi-sided Fairness in Recommendation}},
  author         = {Burke, Robin and Sonboli, Nasim and Ordonez-Gauger, Aldo},
  year           = 2018,
  journal        = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  volume         = 81,
  number         = 2008,
  pages          = {202--214},
  url            = {http://proceedings.mlr.press/v81/burke18a.html},
  keywords       = {fair-, multi-sided platform, ness, recommender systems, sparse linear}
}
@article{CalVer10,
  title          = {Three naive Bayes approaches for discrimination-free classification},
  author         = {Calders, Toon and Verwer, Sicco},
  year           = 2010,
  day            = {01},
  journal        = {Data Mining and Knowledge Discovery},
  volume         = 21,
  number         = 2,
  pages          = {277--292},
  doi            = {10.1007/s10618-010-0190-x},
  issn           = {1573-756x},
  url            = {https://doi.org/10.1007/s10618-010-0190-x},
  abstract       = {
    In this paper, we investigate how to modify the naive Bayes classifier in order to perform
    classification that is restricted to be independent with respect to a given sensitive
    attribute. Such independency restrictions occur naturally when the decision process leading to
    the labels in the data-set was biased; e.g., due to gender or racial discrimination. This
    setting is motivated by many cases in which there exist laws that disallow a decision that is
    partly based on discrimination. Naive application of machine learning techniques would result
    in huge fines for companies. We present three approaches for making the naive Bayes classifier
    discrimination-free: (i) modifying the probability of the decision being positive, (ii)
    training one model for every sensitive attribute value and balancing them, and (iii) adding a
    latent variable to the Bayesian model that represents the unbiased label and optimizing the
    model parameters for likelihood using expectation maximization. We present experiments for the
    three approaches on both artificial and real-life data.
  }
}
@inproceedings{CalWeiVinRamVar17,
  title          = {Optimized Pre-Processing for Discrimination Prevention},
  author         = {Calmon, Fl{\'{a}}vio~du~Pin and
                    Wei, Dennis and
                    Vinzamuri, Bhanukiran and
                    Ramamurthy, Karthikeyan~Natesan and
                    Varshney, Kush~R.
  },
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {3992--4001},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/CalmonWVRV17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@misc{census_bureau,
  title          = {U.S. Census Bureau QuickFacts: UNITED STATES},
  journal        = {Census Bureau QuickFacts},
  url            = {https://www.census.gov/quickfacts/fact/table/US/PST045217}
}
@inproceedings{Chi19,
  title          = {Path-Specific Counterfactual Fairness},
  author         = {Chiappa, Silvia},
  year           = 2019,
  booktitle      = {{AAAI} Conference on Artificial Intelligence},
  publisher      = {{AAAI} Press},
  pages          = {7801--7808},
  doi            = {10.1609/aaai.v33i01.33017801},
  url            = {https://doi.org/10.1609/aaai.v33i01.33017801},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/aaai/Chiappa19.bib},
  timestamp      = {Wed, 25 Sep 2019 01:00:00 +0200}
}
@article{Cho17,
  title          = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  author         = {Chouldechova, Alexandra},
  year           = 2017,
  journal        = {Big Data},
  volume         = 5,
  number         = 2,
  pages          = {153--163},
  doi            = {10.1089/big.2016.0047},
  issn           = {2167-6461},
  url            = {https://doi.org/10.1089/big.2016.0047},
  note           = {Pmid: 28632438},
  abstract       = {
    Abstract Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of
    the likelihood that a criminal defendant will reoffend at a future point in time. Although such
    instruments are gaining increasing popularity across the country, their use is attracting
    tremendous controversy. Much of the controversy concerns potential discriminatory bias in the
    risk assessments that are produced. This article discusses several fairness criteria that have
    recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot
    all be simultaneously satisfied when recidivism prevalence differs across groups. We then show
    how disparate impact can arise when an RPI fails to satisfy the criterion of error rate
    balance.
  },
  arxivid        = {1610.07524},
  eprint         = {https://doi.org/10.1089/big.2016.0047}
}
@inproceedings{ChoChoKimHa18,
  title          = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
  author         = {
    Yunjey Choi and Min{-}Je Choi and Munyoung Kim and Jung{-}Woo Ha and Sunghun Kim and Jaegul
    Choo
  },
  year           = 2018,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {8789--8797},
  doi            = {10.1109/cvpr.2018.00916},
  url            = {

    http://openaccess.thecvf.com/content\%5Fcvpr\%5F2018/html/Choi\%5FStarGAN\%5FUnified\%5FGenerative\%5FCVPR\%5F2018\%5Fpaper.html
  },
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/ChoiCKH0C18.bib},
  timestamp      = {Tue, 29 Dec 2020 00:00:00 +0100}
}
@article{CitPas14,
  title          = {The scored society: Due process for automated predictions},
  author         = {Citron, Danielle~Keats and Pasquale, Frank},
  year           = 2014,
  journal        = {Wash. L. Rev.},
  publisher      = {HeinOnline},
  volume         = 89,
  pages          = 1
}
@article{ClaBobKitLam18,
  title          = {Deep learning for classical japanese literature},
  author         = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex an},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1812.01718},
  url            = {https://arxiv.org/abs/1812.01718}
}
@inproceedings{CohAfsTapVan17,
  title          = {EMNIST: Extending MNIST to handwritten letters},
  author         = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  year           = 2017,
  booktitle      = {2017 International Joint Conference on Neural Networks (IJCNN)},
  pages          = {2921--2926},
  organization   = {Ieee}
}
@misc{CorGoe18,
  title          = {Defining and Designing Fair Algorithms},
  author         = {Corbett-Davies, S and Goel, S},
  journal        = {ICML 2018},
  publisher      = {ICML},
  url            = {https://icml.cc/Conferences/2018/Schedule?showEvent=1862},
  note           = {ICML 2018 Tutorial},
  year           = {2018}
}
@inproceedings{CreMadJacWeiSwePitZem19,
  title          = {Flexibly Fair Representation Learning by Disentanglement},
  author         = {
    Elliot Creager and David Madras and J{\"{o}}rn{-}Henrik Jacobsen and Marissa A. Weis and Kevin
    Swersky and Toniann Pitassi and Richard S. Zemel
  },
  year           = 2019,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 97,
  pages          = {1436--1445},
  url            = {http://proceedings.mlr.press/v97/creager19a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/CreagerMJWSPZ19.bib},
  editor         = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  timestamp      = {Thu, 13 Jun 2019 01:00:00 +0200}
}
@article{Ded14,
  title          = {{Wrong side of the tracks : Big Data and Protected Categories}},
  author         = {Dedeo, Simon},
  year           = 2014,
  journal        = {ArXiv preprint},
  volume         = {abs/1412.4643},
  url            = {https://arxiv.org/abs/1412.4643}
}
@inproceedings{Dia14,
  title          = {Algorithmic Accountability Reporting: On the Investigation of Black Boxes},
  author         = {Diakopoulos,  Nicholas},
  year           = 2014,
  publisher      = {Columbia University},
  doi            = {10.7916/d8zk5tw2},
  url            = {https://academiccommons.columbia.edu/doi/10.7916/D8ZK5TW2},
  keywords       = {Journalism,  Journalism--Methodology,  Journalism--Data processing}
}
@article{DiF2019,
  publtype       = {informal},
  author         = {Merler, Michele and
                    Ratha, Nalini~K. and
                    Schmidt~Feris, Rog{\'{e}}rio and
                    Smith, John~R.},
  title={Diversity in Faces},
  year={2019},
  cdate={1546300800000},
  journal={CoRR},
  volume={abs/1901.10436},
  url={http://arxiv.org/abs/1901.10436}
}
@inproceedings{DinKruBen14,
  title          = {NICE: Non-linear Independent Components Estimation},
  author         = {Laurent Dinh and David Krueger and Yoshua Bengio},
  year           = 2014,
  booktitle      = {International Conference on Learning Representations (ICLR)}
}
@misc{donelan_2021,
  title     = {New levelling up plans to improve student outcomes},
  url       = {https://www.gov.uk/government/news/new-levelling-up-plans-to-improve-student-outcomes},
  journal   = {GOV.UK},
  publisher = {Department for Education},
  author    = {Donelan, The Rt Hon Michelle},
  year      = {2021},
  month     = {11}
}
@article{DosKim17,
  title          = {{Towards A Rigorous Science of Interpretable Machine Learning}},
  author         = {{Doshi-Velez}, Finale and {Kim}, Been},
  year           = 2017,
  month          = 2,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1702.08608},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv170208608D},
  archiveprefix  = {arXiv},
  eid            = {arXiv:1702.08608},
  eprint         = {1702.08608},
  keywords       = {
    Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science -
    Machine Learning
  },
  primaryclass   = {stat.ML}
}
@misc{DuaEfi17,
  title          = {{UCI} Machine Learning Repository},
  author         = {Dheeru, Dua and Karra Taniskidou, Efi},
  year           = 2017,
  url            = {http://archive.ics.uci.edu/ml}
}
@inproceedings{DurGemMah17,
  title          = {Generative Multi-Adversarial Networks},
  author         = {Ishan P. Durugkar and Ian Gemp and Sridhar Mahadevan},
  year           = 2017,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=Byk-VI9eg},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/DurugkarGM17.bib},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{DwoHarPitReiZem12,
  title          = {Fairness through Awareness},
  author         = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year           = 2012,
  booktitle      = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  location       = {Cambridge, Massachusetts},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Itcs '12},
  pages          = {214--226},
  doi            = {10.1145/2090236.2090255},
  isbn           = 9781450311151,
  url            = {https://doi.org/10.1145/2090236.2090255},
  abstract       = {
    We study fairness in classification, where individuals are classified, e.g., admitted to a
    university, and the goal is to prevent discrimination against individuals based on their
    membership in some group, while maintaining utility for the classifier (the university). The
    main conceptual contribution of this paper is a framework for fair classification comprising
    (1) a (hypothetical) task-specific metric for determining the degree to which individuals are
    similar with respect to the classification task at hand; (2) an algorithm for maximizing
    utility subject to the fairness constraint, that similar individuals are treated similarly. We
    also present an adaptation of our approach to achieve the complementary goal of "fair
    affirmative action," which guarantees statistical parity (i.e., the demographics of the set of
    individuals receiving any classification are the same as the demographics of the underlying
    population), while treating similar individuals as similarly as possible. Finally, we discuss
    the relationship of fairness to privacy: when fairness implies privacy, and how tools developed
    in the context of differential privacy may be applied to fairness.
  },
  arxivid        = {1104.3913},
  numpages       = 13
}
@inproceedings{DwoIlv18,
  title          = {Fairness Under Composition},
  author         = {Cynthia Dwork and Christina Ilvento},
  year           = 2019,
  booktitle      = {Innovations in Theoretical Computer Science (ITCS)}
}
@inproceedings{EdwSto16,
  title          = {Censoring Representations with an Adversary},
  author         = {Harrison Edwards and Amos J. Storkey},
  year           = 2016,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  url            = {http://arxiv.org/abs/1511.05897},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/EdwardsS15.bib},
  editor         = {Yoshua Bengio and Yann LeCun},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{ElaGol18,
  title          = {Adversarial Removal of Demographic Attributes from Text Data},
  author         = {Elazar, Yanai  and Goldberg, Yoav},
  year           = 2018,
  booktitle      = {Proc. of EMNLP},
  publisher      = {Association for Computational Linguistics},
  address        = {Brussels, Belgium},
  pages          = {11--21},
  doi            = {10.18653/v1/D18-1002},
  url            = {https://aclanthology.org/D18-1002}
}
@inproceedings{ElzJabJunKeaNeeRotSch19,
  title          = {Fair Algorithms for Learning in Allocation Problems},
  author         = {
    Elzayn, Hadi and Jabbari, Shahin and Jung, Christopher and Kearns, Michael and Neel, Seth and
    Roth, Aaron and Schutzman, Zachary
  },
  year           = 2019,
  booktitle      = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  location       = {Atlanta, GA, USA},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Fat\textasteriskcentered '19},
  pages          = {170--179},
  doi            = {10.1145/3287560.3287571},
  isbn           = 9781450361255,
  url            = {https://doi.org/10.1145/3287560.3287571},
  abstract       = {
    Settings such as lending and policing can be modeled by a centralized agent allocating a scarce
    resource (e.g. loans or police officers) amongst several groups, in order to maximize some
    objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such
    problems fairness is also a concern. One natural notion of fairness, based on general
    principles of equality of opportunity, asks that conditional on an individual being a candidate
    for the resource in question, the probability of actually receiving it is approximately
    independent of the individual's group. For example, in lending this would mean that equally
    creditworthy individuals in different racial groups have roughly equal chances of receiving a
    loan. In policing it would mean that two individuals committing the same crime in different
    districts would have roughly equal chances of being arrested.In this paper, we formalize this
    general notion of fairness for allocation problems and investigate its algorithmic
    consequences. Our main technical results include an efficient learning algorithm that converges
    to an optimal fair allocation even when the allocator does not know the frequency of candidates
    (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a
    censored feedback model in which only the number of candidates who received the resource in a
    given allocation can be observed, rather than the true number of candidates in each group. This
    models the fact that we do not learn the creditworthiness of individuals we do not give loans
    to and do not learn about crimes committed if the police presence in a district is low.As an
    application of our framework and algorithm, we consider the predictive policing problem, in
    which the resource being allocated to each group is the number of police officers assigned to
    each district. The learning algorithm is trained on arrest data gathered from its own
    deployments on previous days, resulting in a potential feedback loop that our algorithm
    provably overcomes. In this case, the fairness constraint asks that the probability that an
    individual who has committed a crime is arrested should be independent of the district in which
    they live. We investigate the performance of our learning algorithm on the Philadelphia Crime
    Incidents dataset.
  },
  keywords       = {online learning, censored feedback, algorithmic fairness, resource allocation},
  numpages       = 10
}
@misc{Eng16,
  title          = {{Fast Style Transfer}},
  author         = {Engstrom, Logan},
  year           = 2016,
  month          = 10,
  url            = {https://github.com/lengstrom/fast-style-transfer}
}
@article{FarThoGet18,
  title          = {{A Fairness-aware Hybrid Recommender System}},
  author         = {Farnadi, Golnoosh and Thompson, Spencer K and Getoor, Lise},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1809.09030},
  url            = {https://arxiv.org/abs/1809.09030}
}
@inproceedings{FelFriMoeSchVen15,
  title          = {Certifying and Removing Disparate Impact},
  author         = {
    Michael Feldman and Sorelle A. Friedler and John Moeller and Carlos Scheidegger and Suresh
    Venkatasubramanian
  },
  year           = 2015,
  booktitle      = {
    Proceedings of the 21th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data
    Mining, Sydney, NSW, Australia, August 10-13, 2015
  },
  publisher      = {{Acm}},
  pages          = {259--268},
  doi            = {10.1145/2783258.2783311},
  url            = {https://doi.org/10.1145/2783258.2783311},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/kdd/FeldmanFMSV15.bib},
  editor         = {
    Longbing Cao and Chengqi Zhang and Thorsten Joachims and Geoffrey I. Webb and Dragos D.
    Margineantu and Graham Williams
  },
  timestamp      = {Tue, 06 Nov 2018 00:00:00 +0100}
}
@article{FenYanLyuTanSun19,
  title          = {Learning fair representations via an adversarial framework},
  author         = {Feng, Rui and Yang, Yang and Lyu, Yuehan and Tan, Chenhao and Sun, Yizhou an},
  year           = 2019,
  journal        = {ArXiv preprint},
  volume         = {abs/1904.13341},
  url            = {https://arxiv.org/abs/1904.13341}
}
@article{FouIslKeyPan20,
  title          = {An Intersectional Definition of Fairness},
  author         = {Foulds, James~R. and Islam, Rashidul and Keya, Kamrun~Naher and Pan, Shimei},
  year           = 2020,
  journal        = {Proc. of ICDE},
  publisher      = {Ieee},
  doi            = {10.1109/icde48307.2020.00203},
  isbn           = 9781728129037,
  url            = {http://dx.doi.org/10.1109/ICDE48307.2020.00203}
}
@article{FrieSchVen18,
  title          = {A comparative study of fairness-enhancing interventions in machine learning},
  author         = {
    Sorelle A. Friedler and Carlos Scheidegger and Suresh Venkatasubramanian and Sonam Choudhary
    and Evan P. Hamilton and Derek Roth
  },
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1802.04422},
  url            = {https://arxiv.org/abs/1802.04422}
}
@article{FriSchVen21,
  title          = {
    The (Im)Possibility of Fairness: Different Value Systems Require Different Mechanisms for Fair
    Decision Making
  },
  author         = {Friedler, {Sorelle A.} and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year           = 2021,
  journal        = {Commun. ACM},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  volume         = 64,
  number         = 4,
  pages          = {136--143},
  doi            = {10.1145/3433949},
  issn           = {0001-0782},
  url            = {https://doi.org/10.1145/3433949},
  abstract       = {What does it mean to be fair?},
  issue_date     = {April 2021},
  numpages       = 8
}
@article{FuHeHou2014,
  title          = {Learning Race from Face: A Survey},
  author         = {Fu, S. and He, H. and Hou, Z.},
  year           = 2014,
  journal        = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume         = 36,
  number         = 12,
  pages          = {2483--2509},
  doi            = {10.1109/tpami.2014.2321570}
}
@article{GanUstAjaGerLarLavFraMarLam16,
  title          = {Domain-adversarial training of Neural Networks},
  author         = {
    Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle,
    Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor
  },
  year           = 2016,
  journal        = {Journal of Machine Learning Research},
  volume         = 17,
  number         = 1,
  pages          = {2096--2030}
}
@inproceedings{GarUpcKusLiWeiKilBalHop16,
  title          = {Deep manifold traversal: Changing labels with convolutional features},
  author         = {
    Gardner, Jacob R and Upchurch, Paul and Kusner, Matt J and Li, Yixuan and Weinberger, Kilian Q
    and Bala, Kavita and Hopcroft, John E
  },
  year           = 2016,
  booktitle      = {European Conference on Computer Vision (ECCV)}
}
@inproceedings{GatEckBet15a,
  title          = {A Neural Algorithm of Artistic Style},
  author         = {Gatys, Leon and
                    Ecker, Alexander and
                    Bethge, Matthias},
  year           = 2016,
  url            = {https://doi.org/10.1167/16.12.326},
  doi            = {10.1167/16.12.326},
  journal        = {Journal of Vision},
  volume         = 16,
  issue          = 12
}

@article{GatEckBet17,
  title          = {Texture and art with deep neural networks},
  author         = {Leon A. Gatys and Alexander S Ecker and Matthias Bethge},
  year           = 2017,
  journal        = {Current Opinion in Neurobiology},
  volume         = 46,
  pages          = {178--186}
}
@inproceedings{gatys2016image,
  title          = {Image Style Transfer Using Convolutional Neural Networks},
  author         = {Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
  year           = 2016,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {2414--2423},
  doi            = {10.1109/cvpr.2016.265},
  url            = {https://doi.org/10.1109/CVPR.2016.265},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/GatysEB16.bib},
  timestamp      = {Thu, 25 May 2017 01:00:00 +0200}
}
@inproceedings{Geir18,
  title          = {
    ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and
    robustness
  },
  author         = {
    Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A.
    Wichmann and Wieland Brendel
  },
  year           = 2019,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=Bygh9j09KX},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/GeirhosRMBWB19.bib},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{GoeGuLiRe21,
  title          = {Model Patching: Closing the Subgroup Performance Gap with Data Augmentation},
  author         = {Goel, Karan and Gu, Albert and Li, Yixuan and R{\'{e}}, Christopher},
  year           = 2021,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  address        = {Virtual Event, Austria},
  url            = {https://openreview.net/forum?id=9YlaeLfuhJF},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/GoelGLR21.bib},
  numpages       = 32,
  timestamp      = {Wed, 23 Jun 2021 17:36:39 +0200}
}
@article{GreBorRasSchetal12,
  title          = {A Kernel Two-sample Test},
  author         = {
    Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch\"{o}lkopf, Bernhard and
    Smola, Alexander
  },
  year           = 2012,
  journal        = {Journal of Machine Learning Research (JMLR)},
  volume         = 13,
  pages          = {723--773}
}
@inproceedings{GreBouSmoSch05,
  title          = {Measuring Statistical Dependence with Hilbert-Schmidt Norms},
  author         = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  year           = 2005,
  booktitle      = {Algorithmic Learning Theory},
  publisher      = {Springer Berlin Heidelberg},
  address        = {Berlin, Heidelberg},
  pages          = {63--77},
  isbn           = {978-3-540-31696-1},
  abstract       = {
    We propose an independence criterion based on the eigenspectrum of covariance operators in
    reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the
    Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt
    Independence Criterion, or HSIC). This approach has several advantages, compared with previous
    kernel-based independence criteria. First, the empirical estimate is simpler than any other
    kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly
    defined population quantity which the empirical estimate approaches in the large sample limit,
    with exponential convergence guaranteed between the two: this ensures that independence tests
    based on HSIC do not suffer from slow learning rates. Finally, we show in the context of
    independent component analysis (ICA) that the performance of HSIC is competitive with that of
    previously published kernel-based criteria, and of other recently published ICA methods.
  },
  editor         = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji}
}
@inproceedings{GreHorRasSchSmo07,
  title          = {A Kernel Approach to Comparing Distributions},
  author         = {
    Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte and Sch{\"o}lkopf, Bernhard and
    Smola, Alexander J
  },
  year           = 2007,
  journal        = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
  booktitle      = {Proceedings of the 22. AAAI Conference on Artificial Intelligence},
  publisher      = {AAAI Press},
  address        = {Menlo Park, CA, USA},
  pages          = {1637--1641},
  institution    = {Association for the Advancement of Artificial Intelligence},
  month_numeric  = 7,
  organization   = {Max-Planck-Gesellschaft},
  school         = {Biologische Kybernetik}
}
@inproceedings{GrgRedGumWel18,
  title          = {
    Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk
    Prediction
  },
  author         = {Grgic-Hlaca, Nina and Redmiles, Elissa~M. and Gummadi, Krishna~P. and Weller, Adrian},
  year           = 2018,
  booktitle      = {Proceedings of the 2018 World Wide Web Conference},
  location       = {Lyon, France},
  publisher      = {International World Wide Web Conferences Steering Committee},
  address        = {Republic and Canton of Geneva, CHE},
  series         = {Www '18},
  pages          = {903--912},
  doi            = {10.1145/3178876.3186138},
  isbn           = 9781450356398,
  url            = {https://doi.org/10.1145/3178876.3186138},
  abstract       = {
    As algorithms are increasingly used to make important decisions that affect human lives,
    ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have
    been raised about the fairness of algorithmic decision making. Most prior works on algorithmic
    fairness normatively prescribe how fair decisions ought to be made. In contrast, here, we
    descriptively survey users for how they perceive and reason about fairness in algorithmic
    decision making. A key contribution of this work is the framework we propose to understand why
    people perceive certain features as fair or unfair to be used in algorithms. Our framework
    identifies eight properties of features, such as relevance, volitionality and reliability, as
    latent considerations that inform people's moral judgments about the fairness of feature use in
    decision-making algorithms. We validate our framework through a series of scenario-based
    surveys with 576 people. We find that, based on a person's assessment of the eight latent
    properties of a feature in our exemplar scenario, we can accurately (> 85\%) predict if the
    person will judge the use of the feature as fair. Our findings have important implications. At
    a high-level, we show that people's unfairness concerns are multi-dimensional and argue that
    future studies need to address unfairness concerns beyond discrimination. At a low-level, we
    find considerable disagreements in people's fairness judgments. We identify root causes of the
    disagreements, and note possible pathways to resolve them.
  },
  editor         = {Pierre{-}Antoine Champin and Fabien L. Gandon and Mounia Lalmas and Panagiotis G. Ipeirotis},
  numpages       = 10
}
@inproceedings{grover2018flowgan,
  title          = {Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models},
  author         = {Aditya Grover and Manik Dhar and Stefano Ermon},
  year           = 2018,
  booktitle      = {{AAAI} Conference on Artificial Intelligence},
  publisher      = {{AAAI} Press},
  pages          = {3069--3076},
  url            = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17409},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/aaai/GroverDE18.bib},
  editor         = {Sheila A. McIlraith and Kilian Q. Weinberger},
  timestamp      = {Mon, 22 Oct 2018 01:00:00 +0200}
}
@article{Gupta2018ProxyFairness,
  title          = {{Proxy Fairness}},
  author         = {Gupta, Maya and Cotter, Andrew and Fard, Mahdi Milani and Wang, Serena},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1806.11212},
  url            = {https://arxiv.org/abs/1806.11212}
}
@inproceedings{HarPriSre16,
  title          = {Equality of Opportunity in Supervised Learning},
  author         = {Moritz Hardt and Eric Price and Nati Srebro},
  year           = 2016,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {3315--3323},
  url            = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/HardtPNS16.bib},
  editor         = {Daniel D. Lee and Masashi Sugiyama and Ulrike von Luxburg and Isabelle Guyon and Roman Garnett},
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@article{HarWal19,
  title          = {Stretching Human Laws to Apply to Machines: The Dangers of a ``Colorblind'' Computer},
  author         = {Harned, Zach and Wallach, Hanna},
  year           = 2019,
  journal        = {Florida State Univeristy Law Review},
  publisher      = {HeinOnline},
  volume         = 47,
  pages          = 617
}
@article{Heidari2018,
  title          = {
    {A Moral Framework for Understanding of Fair ML through Economic Models of Equality of
    Opportunity}
  },
  author         = {Heidari, Hoda and Loi, Michele and Gummadi, Krishna P. and Krause, Andreas},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1809.03400},
  url            = {https://arxiv.org/abs/1809.03400}
}
@inproceedings{HeZhaRenSun16,
  title          = {Deep Residual Learning for Image Recognition},
  author         = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year           = 2016,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {770--778},
  doi            = {10.1109/cvpr.2016.90},
  url            = {https://doi.org/10.1109/CVPR.2016.90},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  timestamp      = {Wed, 17 Apr 2019 01:00:00 +0200}
}
@inproceedings{higgins2017beta,
  title          = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author         = {
    Irina Higgins and Lo{\"{\i}}c Matthey and Arka Pal and Christopher Burgess and Xavier Glorot
    and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner
  },
  year           = 2017,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=Sy2fzU9gl},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@article{Hinnefeld2018EvaluatingBias,
  title          = {{Evaluating Fairness Metrics in the Presence of Dataset Bias}},
  author         = {Hinnefeld, J. Henry and Cooman, Peter and Mammo, Nat and Deese, Rupert},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1809.09245},
  url            = {https://arxiv.org/abs/1809.09245}
}
@article{Holmstrom2016MachineForecasting,
  title          = {Machine learning applied to weather forecasting},
  author         = {Holmstrom, Mark and
                    Liu, Dylan and
                    Vo, Christopher},
  journal        = {Meteorol. Appl},
  pages          = {1--5},
  year           = {2016}
}
@inproceedings{holstein2019improving,
  title          = {Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?},
  author         = {
    Kenneth Holstein and Jennifer Wortman Vaughan and Hal Daum{\'{e}} III and Miroslav Dud{\'{\i}}k
    and Hanna M. Wallach
  },
  year           = 2019,
  booktitle      = {
    Proceedings of the 2019 {CHI} Conference on Human Factors in Computing Systems, {CHI} 2019,
    Glasgow, Scotland, UK, May 04-09, 2019
  },
  publisher      = {{Acm}},
  pages          = 600,
  doi            = {10.1145/3290605.3300830},
  url            = {https://doi.org/10.1145/3290605.3300830},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/chi/HolsteinVDDW19.bib},
  editor         = {Stephen A. Brewster and Geraldine Fitzpatrick and Anna L. Cox and Vassilis Kostakos},
  timestamp      = {Wed, 01 May 2019 01:00:00 +0200}
}
@book{Hume00,
  title          = {An enquiry concerning human understanding: A critical edition},
  author         = {Hume, David},
  year           = 2000,
  publisher      = {Oxford University Press},
  volume         = 3
}
@article{HwangCOMPUTATIONALINTELLIGENCE,
  title          = {{Computational Power And The Social Impact Of Artificial Intelligence}},
  author         = {Hwang, Tim},
  pages          = {1--44},
  journal        = {Available at SSRN 3147971},
  year           = {2018}
}
@inproceedings{ioffe2015batch,
  title          = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author         = {Sergey Ioffe and Christian Szegedy},
  year           = 2015,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {JMLR.org},
  series         = {{JMLR} Workshop and Conference Proceedings},
  volume         = 37,
  pages          = {448--456},
  url            = {http://proceedings.mlr.press/v37/ioffe15.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/IoffeS15.bib},
  editor         = {Francis R. Bach and David M. Blei},
  timestamp      = {Wed, 29 May 2019 01:00:00 +0200}
}
@inproceedings{IsoZhuZhoEfr17,
  title          = {Image-to-Image Translation with Conditional Adversarial Networks},
  author         = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei~A.},
  year           = 2017,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {5967--5976},
  doi            = {10.1109/cvpr.2017.632},
  url            = {https://doi.org/10.1109/CVPR.2017.632},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/IsolaZZE17.bib},
  timestamp      = {Thu, 16 Nov 2017 00:00:00 +0100}
}
@inproceedings{JacBehZemBet19,
  title          = {Excessive Invariance Causes Adversarial Vulnerability},
  author         = {J{\"{o}}rn{-}Henrik Jacobsen and Jens Behrmann and Richard S. Zemel and Matthias Bethge},
  year           = 2019,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=BkfbpsAcF7},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/JacobsenBZB19.bib},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{JacSmeOya18,
  title          = {i-RevNet: Deep Invertible Networks},
  author         = {J{\"{o}}rn{-}Henrik Jacobsen and Arnold W. M. Smeulders and Edouard Oyallon},
  year           = 2018,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=HJsjkMb0Z},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/JacobsenSO18.bib},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{jaiswal2018unsupervised,
  title          = {Unsupervised Adversarial Invariance},
  author         = {Ayush Jaiswal and Rex Yue Wu and Wael Abd{-}Almageed and Prem Natarajan},
  year           = 2018,
  booktitle      = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages          = {5097--5107},
  url            = {https://proceedings.neurips.cc/paper/2018/hash/03e7ef47cee6fa4ae7567394b99912b7-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/JaiswalWAN18.bib},
  editor         = {
    Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}}
    Cesa{-}Bianchi and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{JiaNac20,
  title          = {Identifying and Correcting Label Bias in Machine Learning},
  author         = {Heinrich Jiang and Ofir Nachum},
  year           = 2020,
  booktitle      = {
    The 23rd International Conference on Artificial Intelligence and Statistics, {AISTATS} 2020,
    26-28 August 2020, Online [Palermo, Sicily, Italy]
  },
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 108,
  pages          = {702--712},
  url            = {http://proceedings.mlr.press/v108/jiang20a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/aistats/JiangN20.bib},
  editor         = {Silvia Chiappa and Roberto Calandra},
  timestamp      = {Mon, 29 Jun 2020 01:00:00 +0200}
}
@article{JoBen17,
  title          = {Measuring the tendency of {CNN}s to Learn Surface Statistical Regularities},
  author         = {Jason Jo and Yoshua Bengio},
  year           = 2017,
  journal        = {ArXiv preprint},
  volume         = {abs/1711.11561},
  url            = {https://arxiv.org/abs/1711.11561}
}
@inproceedings{JohAlaFei2016,
  title          = {Perceptual losses for real-time style transfer and super-resolution},
  author         = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  year           = 2016,
  booktitle      = {European Conference on Computer Vision (ECCV)}
}
@article{JosKoyVijKimGho19,
  title          = {Towards Realistic Individual Recourse and Actionable Explanation},
  author         = {Joshi, Shalmali an},
  year           = 2019,
  journal        = {ArXiv preprint},
  volume         = {abs/1907.09615},
  url            = {https://arxiv.org/abs/1907.09615}
}
@inproceedings{KalZho18,
  title          = {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
  author         = {Nathan Kallus and Angela Zhou},
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {2444--2453},
  url            = {http://proceedings.mlr.press/v80/kallus18a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/KallusZ18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@inproceedings{KamAkaAso2012,
  title          = {Fairness-aware classifier with prejudice remover regularizer},
  author         = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  year           = 2012,
  booktitle      = {European conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  pages          = {35--50}
}
@inproceedings{KamCal09,
  title          = {Classifying without discriminating},
  author         = {Kamiran, Faisal and Calders, Toon},
  year           = 2009,
  booktitle      = {2009 2nd International Conference on Computer, Control and Communication, IC4 2009},
  doi            = {10.1109/ic4.2009.4909197},
  isbn           = 9781424433148
}
@article{KamCal12,
  title          = {Data preprocessing techniques for classification without discrimination},
  author         = {Kamiran, Faisal and Calders, Toon},
  year           = 2012,
  journal        = {Knowledge and Information Systems},
  publisher      = {Springer},
  volume         = 33,
  number         = 1,
  pages          = {1--33},
  doi            = {10.1007/s10115-011-0463-8},
  isbn           = 1011501104,
  issn           = {02191377},
  keywords       = {Classification, Discrimination-aware data mining, Preprocessing}
}
@techreport{Kamiran2011,
  title          = {Discrimination-aware Classification},
  author         = {Kamiran, F.},
  year           = 2011,
  address        = {Eindhoven},
  pages          = 157,
  doi            = {10.6100/ir717576},
  isbn           = 9789038627892,
  url            = {https://research.tue.nl/en/publications/discrimination-aware-classification},
  institution    = {Technische Universiteit Eindhoven},
  keywords       = {Classifiers, Data Mining, Education, Industry, Labeling, Learning Systems, Processing}
}
@article{kamiran2012data,
  title          = {Data preprocessing techniques for classification without discrimination},
  author         = {Kamiran, Faisal and Calders, Toon},
  year           = 2012,
  journal        = {Knowledge and Information Systems},
  volume         = 33,
  number         = 1,
  pages          = {1--33}
}
@inproceedings{KarSchVal21,
  title          = {Algorithmic Recourse: From Counterfactual Explanations to Interventions},
  author         = {Karimi, Amir-Hossein and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
  year           = 2021,
  booktitle      = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  location       = {Virtual Event, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {FAccT '21},
  pages          = {353--362},
  doi            = {10.1145/3442188.3445899},
  isbn           = 9781450383097,
  url            = {https://doi.org/10.1145/3442188.3445899},
  abstract       = {
    As machine learning is increasingly used to inform consequential decision-making (e.g.,
    pre-trial bail and loan approval), it becomes important to explain how the system arrived at
    its decision, and also suggest actions to achieve a favorable decision. Counterfactual
    explanations -"how the world would have (had) to be different for a desirable outcome to
    occur"- aim to satisfy these criteria. Existing works have primarily focused on designing
    algorithms to obtain counterfactual explanations for a wide range of settings. However, it has
    largely been overlooked that ultimately, one of the main objectives is to allow people to act
    rather than just understand. In layman's terms, counterfactual explanations inform an
    individual where they need to get to, but not how to get there. In this work, we rely on causal
    reasoning to caution against the use of counterfactual explanations as a recommendable set of
    actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest
    counterfactual explanations to recourse through minimal interventions, shifting the focus from
    explanations to interventions.
  },
  keywords       = {
    minimal interventions, algorithmic recourse, counterfactual explanations, causal inference,
    consequential recommendations, explainable artificial intelligence, contrastive explanations
  },
  numpages       = 10
}
@inproceedings{KeaNeeRotWe18,
  title          = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author         = {Michael J. Kearns and Seth Neel and Aaron Roth and Zhiwei Steven Wu},
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {2569--2577},
  url            = {http://proceedings.mlr.press/v80/kearns18a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/KearnsNRW18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Tue, 08 Oct 2019 01:00:00 +0200}
}
@phdthesis{Keh21,
  month          = {September},
  title          = {Learning with biased data: invariant representations and target labels},
  school         = {University of Sussex},
  author         = {Thomas Maximilian Kehrenberg},
  year           = {2021},
  url            = {http://sro.sussex.ac.uk/id/eprint/101574/},
  abstract       = {
    Biased data represents a significant challenge for the proper functioning of machine learning models, which affects the trustworthiness of deployed models. These biases are usually introduced by the data generation process, i.e., data is collected from non-representative samples or is the result of biased processes. However, these data deficiencies can be very expensive or even impossible to fix, which makes it desirable to solve the problem on the algorithmic end. In this work, I consider two different forms of data bias: labelling bias and sampling bias; investigated under the framework of algorithmic fairness and evaluated using common fairness metrics. Labelling bias here refers to a systematic bias, correlated with a sensitive attribute, which causes the labels in the dataset to differ from the ?true? labels; whereas sampling bias indicates that samples are missing from the training set in a systematic way, but are still present in the setting where the model is intended to be deployed. Both biases will make a naively trained model fail to generalize. I present three approaches to tackling this problem, each relying on some form of additional knowledge about the data. The first approach, dealing with labelling bias, is based on implicit, probabilistic target labels which satisfy certain given statistics. These target labels can be used to train any likelihood-based model. The second approach deals with strong spurious correlations in the training data, which can be seen as a specific form of sampling bias. A bias-free partially-labelled context set is used to learn an interpretable representation of the data which is invariant to the spurious correlation and can be assessed qualitatively. The third approach deals with less extreme cases of sampling bias, but relaxes the assumption of having labels in the context set, by learning an invariant representation via distribution matching.
  }
}
@article{KehCheQua20,
  title          = {Tuning Fairness by Balancing Target Labels},
  author         = {Kehrenberg, Thomas and Chen, Zexun and Quadrianto, Novi},
  year           = 2020,
  journal        = {Frontiers in Artificial Intelligence},
  volume         = 3,
  pages          = 33,
  doi            = {10.3389/frai.2020.00033},
  issn           = {2624-8212},
  url            = {https://www.frontiersin.org/article/10.3389/frai.2020.00033},
  abstract       = {
    The issue of fairness in machine learning models has recently attracted a lot of attention as
    ensuring it will ensure continued confidence of the general public in the deployment of machine
    learning systems. We focus on mitigating the harm incurred by a biased machine learning system
    that offers better outputs (e.g., loans, job interviews) for certain groups than for others. We
    show that bias in the output can naturally be controlled in probabilistic models by introducing
    a latent target output. This formulation has several advantages: first, it is a unified
    framework for several notions of group fairness such as Demographic Parity and Equality of
    Opportunity; second, it is expressed as a marginalization instead of a constrained problem; and
    third, it allows the encoding of our knowledge of what unbiased outputs should be. Practically,
    the second allows us to avoid unstable constrained optimization procedures and to reuse
    off-the-shelf toolboxes. The latter translates to the ability to control the level of fairness
    by directly varying fairness target rates. In contrast, existing approaches rely on
    intermediate, arguably unintuitive, control parameters such as covariance thresholds.
  },
  sortname       = {Kehrenberg},
  sorttitle      = {a}
}
@inproceedings{kim2018disentangling,
  title          = {Disentangling by Factorising},
  author         = {Hyunjik Kim and Andriy Mnih},
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {2654--2663},
  url            = {http://proceedings.mlr.press/v80/kim18b.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/KimM18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@inproceedings{kim2019learning,
  title          = {Learning Not to Learn: Training Deep Neural Networks With Biased Data},
  author         = {Byungju Kim and Hyunwoo Kim and Kyungsu Kim and Sungjin Kim and Junmo Kim},
  year           = 2019,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {Computer Vision Foundation / {IEEE}},
  pages          = {9012--9020},
  doi            = {10.1109/cvpr.2019.00922},
  url            = {

    http://openaccess.thecvf.com/content\%5FCVPR\%5F2019/html/Kim\%5FLearning\%5FNot\%5Fto\%5FLearn\%5FTraining\%5FDeep\%5FNeural\%5FNetworks\%5FWith\%5FBiased\%5FCVPR\%5F2019\%5Fpaper.html
  },
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/KimKKKK19.bib},
  timestamp      = {Mon, 20 Jan 2020 00:00:00 +0100}
}
@inproceedings{KinDha18,
  title          = {Glow: Generative Flow with Invertible 1x1 Convolutions},
  author         = {Diederik P. Kingma and Prafulla Dhariwal},
  year           = 2018,
  booktitle      = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages          = {10236--10245},
  url            = {https://proceedings.neurips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/KingmaD18.bib},
  editor         = {
    Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}}
    Cesa{-}Bianchi and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{kingma2013auto,
  title          = {Auto-Encoding Variational Bayes},
  author         = {Diederik P. Kingma and Max Welling},
  year           = 2014,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  url            = {http://arxiv.org/abs/1312.6114},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  editor         = {Yoshua Bengio and Yann LeCun},
  timestamp      = {Fri, 29 Mar 2019 00:00:00 +0100}
}
@inproceedings{kingma2014adam,
  title          = {Adam: {A} Method for Stochastic Optimization},
  author         = {Diederik P. Kingma and Jimmy Ba},
  year           = 2015,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  url            = {http://arxiv.org/abs/1412.6980},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  editor         = {Yoshua Bengio and Yann LeCun},
  timestamp      = {Thu, 25 Jul 2019 01:00:00 +0200}
}
@inproceedings{klambauer2017self,
  title          = {Self-Normalizing Neural Networks},
  author         = {G{\"{u}}nter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {971--980},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/KlambauerUMH17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@InProceedings{KleMulRag16,
  author         = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  title          = {{Inherent Trade-Offs in the Fair Determination of Risk Scores}},
  booktitle      = {8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  pages          = {43:1--43:23},
  series         = {Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN           = {978-3-95977-029-3},
  ISSN           = {1868-8969},
  year           = {2017},
  volume         = {67},
  editor         = {Christos H. Papadimitriou},
  publisher      = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address        = {Dagstuhl, Germany},
  URL            = {http://drops.dagstuhl.de/opus/volltexte/2017/8156},
  URN            = {urn:nbn:de:0030-drops-81560},
  doi            = {10.4230/LIPIcs.ITCS.2017.43},
  annote         = {Keywords: algorithmic fairness, risk tools, calibration}
}
@inproceedings{KusLofRusSil17,
  title          = {Counterfactual Fairness},
  author         = {Matt J. Kusner and Joshua R. Loftus and Chris Russell and Ricardo Silva},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {4066--4076},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/KusnerLRS17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{Kusner2017CounterfactualFairness,
  title          = {Counterfactual Fairness},
  author         = {Matt J. Kusner and Joshua R. Loftus and Chris Russell and Ricardo Silva},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {4066--4076},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/KusnerLRS17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{LevHas15,
  title          = {Age and gender classification using convolutional neural networks},
  author         = {Levi, G. and Hassncer, T.},
  year           = 2015,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages          = {34--42},
  doi            = {10.1109/cvprw.2015.7301352}
}
@inproceedings{liu2015faceattributes,
  title          = {Deep Learning Face Attributes in the Wild},
  author         = {Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},
  year           = 2015,
  booktitle      = {
    2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015, Santiago, Chile, December
    7-13, 2015
  },
  publisher      = {{IEEE} Computer Society},
  pages          = {3730--3738},
  doi            = {10.1109/iccv.2015.425},
  url            = {https://doi.org/10.1109/ICCV.2015.425},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iccv/LiuLWT15.bib},
  timestamp      = {Tue, 10 Dec 2019 00:00:00 +0100}
}
@inproceedings{Liu2018,
  title          = {Delayed Impact of Fair Machine Learning},
  author         = {Lydia T. Liu and Sarah Dean and Esther Rolf and Max Simchowitz and Moritz Hardt},
  year           = 2019,
  booktitle      = {
    Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    {IJCAI} 2019, Macao, China, August 10-16, 2019
  },
  publisher      = {ijcai.org},
  pages          = {6196--6200},
  doi            = {10.24963/ijcai.2019/862},
  url            = {https://doi.org/10.24963/ijcai.2019/862},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/ijcai/LiuDRSH19.bib},
  editor         = {Sarit Kraus},
  timestamp      = {Tue, 20 Aug 2019 01:00:00 +0200}
}
@inproceedings{liu2019variance,
  title          = {On the Variance of the Adaptive Learning Rate and Beyond},
  author         = {
    Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao
    and Jiawei Han
  },
  year           = 2020,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  publisher      = {OpenReview.net},
  url            = {https://openreview.net/forum?id=rkgz2aEKDr},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iclr/LiuJHCLG020.bib},
  timestamp      = {Thu, 07 May 2020 01:00:00 +0200}
}
@inproceedings{locatello2019challenging,
  title          = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author         = {
    Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar R{\"{a}}tsch and Sylvain Gelly
    and Bernhard Sch{\"{o}}lkopf and Olivier Bachem
  },
  year           = 2019,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 97,
  pages          = {4114--4124},
  url            = {http://proceedings.mlr.press/v97/locatello19a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/LocatelloBLRGSB19.bib},
  editor         = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  timestamp      = {Tue, 11 Jun 2019 01:00:00 +0200}
}
@inproceedings{locatello2019fairness,
  title          = {On the Fairness of Disentangled Representations},
  author         = {
    Francesco Locatello and Gabriele Abbati and Thomas Rainforth and Stefan Bauer and Bernhard
    Sch{\"{o}}lkopf and Olivier Bachem
  },
  year           = 2019,
  booktitle      = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages          = {14584--14597},
  url            = {https://proceedings.neurips.cc/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/LocatelloARBSB19.bib},
  editor         = {
    Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and
    Emily B. Fox and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{LopNisChi17,
  title          = {Discovering Causal Signals in Images},
  author         = {
    David Lopez{-}Paz and Robert Nishihara and Soumith Chintala and Bernhard Sch{\"{o}}lkopf and
    L{\'{e}}on Bottou
  },
  year           = 2017,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {58--66},
  doi            = {10.1109/cvpr.2017.14},
  url            = {https://doi.org/10.1109/CVPR.2017.14},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/Lopez-PazNCSB17.bib},
  timestamp      = {Wed, 14 Nov 2018 00:00:00 +0100}
}
@inproceedings{LouSweLiWeletal16,
  title          = {The Variational Fair Autoencoder},
  author         = {Christos Louizos and Kevin Swersky and Yujia Li and Max Welling and Richard S. Zemel},
  year           = 2016,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  url            = {http://arxiv.org/abs/1511.00830},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/LouizosSLWZ15.bib},
  editor         = {Yoshua Bengio and Yann LeCun},
  timestamp      = {Fri, 29 Mar 2019 00:00:00 +0100}
}
@inproceedings{luong2011k,
  title          = {k-NN as an implementation of situation testing for discrimination discovery and prevention},
  author         = {Binh Luong Thanh and Salvatore Ruggieri and Franco Turini},
  year           = 2011,
  booktitle      = {
    Proceedings of the 17th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data
    Mining, San Diego, CA, USA, August 21-24, 2011
  },
  publisher      = {{Acm}},
  pages          = {502--510},
  doi            = {10.1145/2020408.2020488},
  url            = {https://doi.org/10.1145/2020408.2020488},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/kdd/ThanhRT11.bib},
  editor         = {Chid Apt{\'{e}} and Joydeep Ghosh and Padhraic Smyth},
  timestamp      = {Tue, 06 Nov 2018 00:00:00 +0100}
}
@inproceedings{MadCrePitZem18,
  title          = {Learning Adversarially Fair and Transferable Representations},
  author         = {David Madras and Elliot Creager and Toniann Pitassi and Richard S. Zemel},
  year           = 2018,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 80,
  pages          = {3381--3390},
  url            = {http://proceedings.mlr.press/v80/madras18a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/MadrasCPZ18.bib},
  editor         = {Jennifer G. Dy and Andreas Krause},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@inproceedings{MadCrePitZem19,
  title          = {Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data},
  author         = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  year           = 2019,
  booktitle      = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  location       = {Atlanta, GA, USA},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Fat\textasteriskcentered '19},
  pages          = {349--358},
  doi            = {10.1145/3287560.3287564},
  isbn           = 9781450361255,
  url            = {https://doi.org/10.1145/3287560.3287564},
  abstract       = {
    How do we learn from biased data? Historical datasets often reflect historical prejudices;
    sensitive or protected attributes may affect the observed treatments and outcomes.
    Classification algorithms tasked with predicting outcomes accurately from these datasets tend
    to replicate these biases. We advocate a causal modeling approach to learning from biased data,
    exploring the relationship between fair classification and intervention. We propose a causal
    model in which the sensitive attribute confounds both the treatment and the outcome. Building
    on prior work in deep learning and generative modeling, we describe how to learn the parameters
    of this causal model from observational data alone, even in the presence of unobserved
    confounders. We show experimentally that fairness-aware causal modeling provides better
    estimates of the causal effects between the sensitive attribute, the treatment, and the
    outcome. We further present evidence that estimating these causal effects can help learn
    policies that are both more accurate and fair, when presented with a historically biased
    dataset.
  },
  keywords       = {causal inference, fairness in machine learning, variational inference},
  numpages       = 10
}
@inproceedings{MadPitZem18,
  title          = {Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
  author         = {David Madras and Toniann Pitassi and Richard S. Zemel},
  year           = 2018,
  booktitle      = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages          = {6150--6160},
  url            = {https://proceedings.neurips.cc/paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/MadrasPZ18.bib},
  editor         = {
    Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}}
    Cesa{-}Bianchi and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{MahVed15,
  title          = {Understanding deep image representations by inverting them},
  author         = {Aravindh Mahendran and Andrea Vedaldi},
  year           = 2015,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {5188--5196},
  doi            = {10.1109/cvpr.2015.7299155},
  url            = {https://doi.org/10.1109/CVPR.2015.7299155},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/MahendranV15.bib},
  timestamp      = {Sun, 02 Jun 2019 01:00:00 +0200}
}
@inproceedings{mary2019fairness,
  title          = {Fairness-Aware Learning for Continuous Attributes and Treatments},
  author         = {J{\'{e}}r{\'{e}}mie Mary and Cl{\'{e}}ment Calauz{\`{e}}nes and Noureddine El Karoui},
  year           = 2019,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 97,
  pages          = {4382--4391},
  url            = {http://proceedings.mlr.press/v97/mary19a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/MaryCK19.bib},
  editor         = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  timestamp      = {Tue, 11 Jun 2019 01:00:00 +0200}
}
@article{Miller19,
  title          = {Explanation in artificial intelligence: Insights from the social sciences},
  author         = {Tim Miller},
  year           = 2019,
  journal        = {Artificial Intelligence},
  volume         = 267,
  pages          = {1--38},
  doi            = {https://doi.org/10.1016/j.artint.2018.07.007},
  issn           = {0004-3702},
  url            = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
  abstract       = {
    There has been a recent resurgence in the area of explainable artificial intelligence as
    researchers and practitioners seek to provide more transparency to their algorithms. Much of
    this research is focused on explicitly explaining decisions or actions to a human observer, and
    it should not be controversial to say that looking at how humans explain to each other can
    serve as a useful starting point for explanation in artificial intelligence. However, it is
    fair to say that most work in explainable artificial intelligence uses only the researchers'
    intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of
    research in philosophy, psychology, and cognitive science of how people define, generate,
    select, evaluate, and present explanations, which argues that people employ certain cognitive
    biases and social expectations to the explanation process. This paper argues that the field of
    explainable artificial intelligence can build on this existing research, and reviews relevant
    papers from philosophy, cognitive psychology/science, and social psychology, which study these
    topics. It draws out some important findings, and discusses ways that these can be infused with
    work on explainable artificial intelligence.
  },
  keywords       = {Explanation, Explainability, Interpretability, Explainable AI, Transparency}
}
@article{MirOsi14,
  title          = {Conditional Generative Adversarial Nets},
  author         = {Mirza, Mehdi and Osindero, Simon},
  year           = 2014,
  journal        = {ArXiv preprint},
  volume         = {abs/1411.1784},
  url            = {https://arxiv.org/abs/1411.1784}
}
@book{molnar,
  title          = {Interpretable Machine Learning},
  author         = {Christoph Molnar},
  year           = 2019,
  url            = {https://christophm.github.io/interpretable-ml-book/},
  subtitle       = {A Guide for Making Black Box Models Explainable}
}

@inproceedings{MooJanPetSch09,
  title          = {
    Regression by dependence minimization and its application to causal inference in additive noise
    models
  },
  author         = {Joris M. Mooij and Dominik Janzing and Jonas Peters and Bernhard Sch{\"{o}}lkopf},
  year           = 2009,
  booktitle      = {
    Proceedings of the 26th Annual International Conference on Machine Learning, {ICML} 2009,
    Montreal, Quebec, Canada, June 14-18, 2009
  },
  publisher      = {{Acm}},
  series         = {{ACM} International Conference Proceeding Series},
  volume         = 382,
  pages          = {745--752},
  doi            = {10.1145/1553374.1553470},
  url            = {https://doi.org/10.1145/1553374.1553470},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/MooijJPS09.bib},
  editor         = {Andrea Pohoreckyj Danyluk and L{\'{e}}on Bottou and Michael L. Littman},
  timestamp      = {Tue, 06 Nov 2018 00:00:00 +0100}
}
@inproceedings{MozSon20,
  title          = {Consistent Estimators for Learning to Defer to an Expert},
  author         = {Hussein Mozannar and David A. Sontag},
  year           = 2020,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 119,
  pages          = {7076--7087},
  url            = {http://proceedings.mlr.press/v119/mozannar20b.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/MozannarS20.bib},
  timestamp      = {Tue, 15 Dec 2020 00:00:00 +0100}
}
@article{Munoz2016BigRights,
  title          = {
    {Big Data : A Report on Algorithmic Systems , Opportunity , and Civil Rights Big Data : A
    Report on Algorithmic Systems , Opportunity , and Civil Rights}
  },
  author         = {Munoz, Cecilia and Smith, Megan and Patil, DJ},
  year           = 2010,
  journal        = {ArXiv preprint},
  volume         = {abs/1011.1669},
  url            = {https://arxiv.org/abs/1011.1669}
}
@misc{national_education_statistics,
  title          = {Mobile Digest of Education Statistics, 2017},
  journal        = {
    National Center for Education Statistics (NCES) Home Page, a part of the U.S. Department of
    Education
  },
  publisher      = {National Center for Education Statistics},
  url            = {

    https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx
  }
}
@inproceedings{normflows2015,
  title          = {Variational Inference with Normalizing Flows},
  author         = {Danilo Jimenez Rezende and Shakir Mohamed},
  year           = 2015,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {JMLR.org},
  series         = {{JMLR} Workshop and Conference Proceedings},
  volume         = 37,
  pages          = {1530--1538},
  url            = {http://proceedings.mlr.press/v37/rezende15.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/RezendeM15.bib},
  editor         = {Francis R. Bach and David M. Blei},
  timestamp      = {Wed, 29 May 2019 01:00:00 +0200}
}
@misc{ParChuChaLeeShi21,
  title          = {Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts},
  author         = {Park, Song and Chun, Sanghyuk and Cha, Junbum and Lee, Bado and Shim, Hyunjung},
  year           = 2021,
  archiveprefix  = {arXiv},
  eprint         = {2104.00887},
  primaryclass   = {cs.CV}
}
@book{Pearl09,
  title          = {Causality},
  author         = {Pearl, Judea},
  year           = 2009,
  publisher      = {Cambridge University Press},
  address        = {Cambridge},
  doi            = {10.1017/cbo9780511803161},
  edition        = 2,
  place          = {Cambridge}
}
@inproceedings{PedRugTur08,
  title          = {{Discrimination-aware data mining}},
  author         = {Pedreshi, Dino and Ruggieri, Salvatore and Turini, Franco},
  year           = 2008,
  booktitle      = {
    Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data
    mining - KDD 08
  },
  publisher      = {Acm},
  pages          = {560--568},
  doi            = {10.1145/1401890.1401959},
  isbn           = 9781605581934,
  editor         = {Li, Ying and Liu, Bing and Sarawagi, Sunita}
}
@inproceedings{Quadrianto2017RecyclingFairness,
  title          = {Recycling Privileged Learning and Distribution Matching for Fairness},
  author         = {Novi Quadrianto and Viktoriia Sharmanska},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {677--688},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/QuadriantoS17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{QuaSha17,
  title          = {Recycling Privileged Learning and Distribution Matching for Fairness},
  author         = {Novi Quadrianto and Viktoriia Sharmanska},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {677--688},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/QuadriantoS17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{RahRec08,
  title          = {Random Features for Large-Scale Kernel Machines},
  author         = {Ali Rahimi and Benjamin Recht},
  year           = 2007,
  booktitle      = {Advances in Neural Information Processing Systems   (NIPS)},
  publisher      = {Curran Associates, Inc.},
  pages          = {1177--1184},
  url            = {https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/RahimiR07.bib},
  editor         = {John C. Platt and Daphne Koller and Yoram Singer and Sam T. Roweis},
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{Ribeiro2016WhyClassifier,
  title          = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  author         = {Marco T{\'{u}}lio Ribeiro and Sameer Singh and Carlos Guestrin},
  year           = 2016,
  booktitle      = {
    Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data
    Mining, San Francisco, CA, USA, August 13-17, 2016
  },
  publisher      = {{Acm}},
  pages          = {1135--1144},
  doi            = {10.1145/2939672.2939778},
  url            = {https://doi.org/10.1145/2939672.2939778},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/kdd/Ribeiro0G16.bib},
  editor         = {
    Balaji Krishnapuram and Mohak Shah and Alexander J. Smola and Charu C. Aggarwal and Dou Shen
    and Rajeev Rastogi
  },
  timestamp      = {Fri, 25 Dec 2020 00:00:00 +0100}
}
@article{RosRub83,
  title          = {The central role of the propensity score in observational studies for causal effects},
  author         = {Paul R Rosenbaum and Donald B Rubin},
  year           = 1983,
  journal        = {Biometrika},
  volume         = 70,
  number         = 1,
  pages          = {41--55}
}
@article{Win02,
  title          = {The shapley value},
  author         = {Winter, Eyal},
  journal        = {Handbook of game theory with economic applications},
  volume         = {3},
  pages          = {2025--2054},
  year           = {2002},
  publisher      = {Elsevier}
}
@techreport{RS2017,
  title          = {Machine learning: the power and promise of computers that learn by example},
  author         = {{Working Group}},
  year           = 2017,
  series         = {Technical Report},
  institution    = {The Royal Society}
}
@article{Rubin90,
  title          = {Formal mode of statistical inference for causal effects},
  author         = {Rubin, Donald B},
  year           = 1990,
  journal        = {Journal of statistical planning and inference},
  publisher      = {Elsevier},
  volume         = 25,
  number         = 3,
  pages          = {279--292}
}
@article{SalKueSteAniHinLonGha18,
  title          = {Aequitas: A Bias and Fairness Audit Toolkit},
  author         = {Saleiro, Pedro an},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1811.05577},
  url            = {https://arxiv.org/abs/1811.05577}
}
@article{SatHofChe18,
  title          = {Fairness {GAN}},
  author         = {Prasanna Sattigeri and Samuel C. Hoffman and Vijil Chenthamarakshan and Kush R. Varshney},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1805.09910},
  url            = {https://arxiv.org/abs/1805.09910}
}
@article{SattigeriFairnessGAN,
  title          = {{Fairness GAN}},
  author         = {
    Sattigeri, Prasanna and Hoffman, Samuel C and Chenthamarakshan, Vijil and Varshney, Kush R and
    Heights, Yorktown
  },
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1805.09910},
  url            = {https://arxiv.org/abs/1805.09910}
}
@article{scikit-learn,
  title          = {Scikit-learn: Machine Learning in {P}ython},
  author         = {
    Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O.
    and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and
    Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.
  },
  year           = 2011,
  journal        = {Journal of Machine Learning Research},
  volume         = 12,
  pages          = {2825--2830}
}
@misc{Hol18,
  title          = {{AI in the UK : ready , willing and able?}},
  author         = {Select Committee on Artificial Intelligence},
  publisher      = {UK House of Lords},
  year           = 2018,
  journal        = {ArXiv preprint},
  url            = {https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10002.htm}
}
@inbook{VeaVanBin18,
  author         = {Veale, Michael and
                    Van~Kleek, Max and
                    Binns, Reuben},
  title          = {Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making},
  year           = {2018},
  isbn           = {9781450356206},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  url            = {https://doi.org/10.1145/3173574.3174014},
  abstract       = {Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications.},
  booktitle      = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages          = {1–14},
  numpages       = {14}
}
@article{ShaHenDarQua20,
  title          = {Contrastive Examples for Addressing the Tyranny of the Majority},
  author         = {Viktoriia Sharmanska and Lisa Anne Hendricks and Trevor Darrell and Novi Quadrianto},
  year           = 2020,
  journal        = {ArXiv preprint},
  volume         = {abs/2004.06524},
  url            = {https://arxiv.org/abs/2004.06524}
}
@inproceedings{ShaJohSon17,
  title          = {Estimating individual treatment effect: generalization bounds and algorithms},
  author         = {Uri Shalit and Fredrik D. Johansson and David A. Sontag},
  year           = 2017,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 70,
  pages          = {3076--3085},
  url            = {http://proceedings.mlr.press/v70/shalit17a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/ShalitJS17.bib},
  editor         = {Doina Precup and Yee Whye Teh},
  timestamp      = {Fri, 15 Nov 2019 00:00:00 +0100}
}
@article{silver2017mastering,
  title          = {Mastering the game of go without human knowledge},
  author         = {
    Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang,
    Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian
    and others
  },
  year           = 2017,
  journal        = {nature},
  publisher      = {Nature Publishing Group},
  volume         = 550,
  number         = 7676,
  pages          = {354--359}
}
@article{SimBhaWel21,
  title          = {Machine Learning and the Meaning of Equal Treatment},
  author         = {Simons, Joshua and Bhatti, Sophia~Adams and Weller, Adrian},
  year           = 2021,
  journal        = {Aies}
}
@inproceedings{SimZis15,
  title          = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author         = {Karen Simonyan and Andrew Zisserman},
  year           = 2015,
  booktitle      = {International Conference on Learning Representations (ICLR)},
  url            = {http://arxiv.org/abs/1409.1556},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  editor         = {Yoshua Bengio and Yann LeCun},
  timestamp      = {Fri, 29 Mar 2019 00:00:00 +0100}
}
@article{SinGanSinSar16,
  title          = {Machine Learning for High-Throughput Stress Phenotyping in Plants},
  author         = {Singh, Arti and Ganapathysubramanian, Baskar and Singh, Asheesh~Kumar and Sarkar, Soumik},
  year           = 2016,
  journal        = {Trends in Plant Science},
  volume         = 21,
  number         = 2,
  pages          = {110--124},
  doi            = {https://doi.org/10.1016/j.tplants.2015.10.015},
  issn           = {1360-1385},
  url            = {http://www.sciencedirect.com/science/article/pii/S1360138515002630},
  abstract       = {
    Advances in automated and high-throughput imaging technologies have resulted in a deluge of
    high-resolution images and sensor data of plants. However, extracting patterns and features
    from this large corpus of data requires the use of machine learning (ML) tools to enable data
    assimilation and feature identification for stress phenotyping. Four stages of the decision
    cycle in plant stress phenotyping and plant breeding activities where different ML approaches
    can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv)
    prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML
    tools to enable the plant community to correctly and easily apply the appropriate ML tools and
    best-practice guidelines for various biotic and abiotic stress traits.
  },
  keywords       = {
    high-throughput phenotyping, machine learning, Imaging, plant breeding, biotic stress, abiotic
    stress
  }
}
@article{SonSmoGreBedetal12,
  title          = {Feature Selection via Dependence Maximization},
  author         = {Song, Le and Smola, Alex and Gretton, Arthur and Bedo, Justin and Borgwardt, Karsten},
  year           = 2012,
  journal        = {Journal of Machine Learning Research (JMLR)},
  volume         = 13,
  pages          = {1393--1434}
}
@article{Sweeney2013DiscriminationDelivery,
  title          = {Discrimination in Online Ad Delivery},
  author         = {Sweeney, Latanya},
  year           = 2013,
  month          = 5,
  journal        = {Commun. ACM},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  volume         = 56,
  number         = 5,
  pages          = {44--54},
  doi            = {10.1145/2447976.2447990},
  issn           = {0001-0782},
  url            = {https://doi.org/10.1145/2447976.2447990},
  abstract       = {Google ads, black names and white names, racial discrimination, and click advertising.},
  issue_date     = {May 2013},
  numpages       = 11
}
@online{target_oxbridge,
  title          = {Target Oxbridge},
  author         = {Rare Recruitment},
  year           = 2021,
  url            = {https://targetoxbridge.co.uk/the\%5Fprogramme.html},
  urldate        = {2021-08-01},
  organization   = {Target Oxbridge}
}
@article{Tol19,
  title          = {Fair and Unbiased Algorithmic Decision Making: Current State and Future Challenges},
  author         = {Song{\"{u}}l Tolan},
  year           = 2019,
  journal        = {ArXiv preprint},
  volume         = {abs/1901.04730},
  url            = {https://arxiv.org/abs/1901.04730}
}
@techreport{TransparentRobotics,
  title          = {{Transparent, Explainable, and Accountable AI for Robotics}},
  number         = 6,
  pages          = {1--6},
  keywords       = {Brent Mittelstadt, Luciano Floridi, Sandra Wachter}
}
@inproceedings{ulyanov2017,
  title          = {
    Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and
    Texture Synthesis
  },
  author         = {Dmitry Ulyanov and Andrea Vedaldi and Victor S. Lempitsky},
  year           = 2017,
  booktitle      = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher      = {{IEEE} Computer Society},
  pages          = {4105--4113},
  doi            = {10.1109/cvpr.2017.437},
  url            = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.437},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/UlyanovVL17.bib},
  timestamp      = {Tue, 14 Nov 2017 00:00:00 +0100}
}
@inproceedings{UlyLebVedLem16,
  title          = {Texture Networks: Feed-forward Synthesis of Textures and Stylized Images},
  author         = {Dmitry Ulyanov and Vadim Lebedev and Andrea Vedaldi and Victor S. Lempitsky},
  year           = 2016,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {JMLR.org},
  series         = {{JMLR} Workshop and Conference Proceedings},
  volume         = 48,
  pages          = {1349--1357},
  url            = {http://proceedings.mlr.press/v48/ulyanov16.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/UlyanovLVL16.bib},
  editor         = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  timestamp      = {Wed, 29 May 2019 01:00:00 +0200}
}
@inproceedings{UstSpaLiu19,
  title          = {Actionable Recourse in Linear Classification},
  author         = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
  year           = 2019,
  booktitle      = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  location       = {Atlanta, GA, USA},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Fat\textasteriskcentered '19},
  pages          = {10--19},
  doi            = {10.1145/3287560.3287566},
  isbn           = 9781450361255,
  url            = {https://doi.org/10.1145/3287560.3287566},
  abstract       = {
    Classification models are often used to make decisions that affect humans: whether to approve a
    loan application, extend a job offer, or provide insurance. In such applications, individuals
    should have the ability to change the decision of the model. When a person is denied a loan by
    a credit scoring model, for example, they should be able to change the input variables of the
    model in a way that will guarantee approval. Otherwise, this person will be denied the loan so
    long as the model is deployed, and -- more importantly --will lack agency over a decision that
    affects their livelihood.In this paper, we propose to evaluate a linear classification model in
    terms of recourse, which we define as the ability of a person to change the decision of the
    model through actionable input variables (e.g., income vs. age or marital status). We present
    an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a
    target population; and (ii) generate a list of actionable changes for a person to obtain a
    desired outcome. We discuss how our tools can inform different stakeholders by using them to
    audit recourse for credit scoring models built with real-world datasets. Our results illustrate
    how recourse can be significantly affected by common modeling practices, and motivate the need
    to evaluate recourse in algorithmic decision-making.
  },
  keywords       = {integer programming, audit, credit scoring, accountability, classification, recourse},
  numpages       = 10
}
@inproceedings{van2016conditional,
  title          = {Conditional Image Generation with PixelCNN Decoders},
  author         = {
    A{\"{a}}ron van den Oord and Nal Kalchbrenner and Lasse Espeholt and Koray Kavukcuoglu and
    Oriol Vinyals and Alex Graves
  },
  year           = 2016,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {4790--4798},
  url            = {https://proceedings.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/OordKEKVG16.bib},
  editor         = {Daniel D. Lee and Masashi Sugiyama and Ulrike von Luxburg and Isabelle Guyon and Roman Garnett},
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@misc{Vin17,
  title          = {DeepMind's AI became a superhuman chess player in a few hours, just for fun},
  author         = {Vincent, James},
  year           = 2017,
  journal        = {The Verge},
  publisher      = {The Verge},
  url            = {https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go}
}
@article{Wachter20,
  title          = {Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI},
  author         = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year           = 2020,
  journal        = {SSRN Electronic Journal},
  publisher      = {Elsevier BV},
  doi            = {10.2139/ssrn.3547922},
  issn           = {1556-5068},
  url            = {http://dx.doi.org/10.2139/ssrn.3547922}
}
@article{WacMitRus18,
  title          = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  author         = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
  year           = 2018,
  journal        = {Harvard Journal of Law \& Technology},
  volume         = 31,
  number         = 2
}
@article{Wadsworth2018AchievingPrediction,
  title          = {{Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction}},
  author         = {Wadsworth, Christina and Vera, Francesca and Piech, Chris},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1807.00199},
  url            = {https://arxiv.org/abs/1807.00199}
}
@techreport{WEF18,
  title          = {How to prevent discriminatory outcomes in machine learning},
  author         = {{Global Future Council on Human Rights {2016-18}}},
  year           = 2018,
  series         = {White Paper},
  institution    = {World Economic Forum}
}
@inproceedings{WicPanTri19,
  title          = {Unlocking Fairness: a Trade-off Revisited},
  author         = {Michael L. Wick and Swetasudha Panda and Jean{-}Baptiste Tristan},
  year           = 2019,
  booktitle      = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages          = {8780--8789},
  url            = {https://proceedings.neurips.cc/paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/WickpT19.bib},
  editor         = {
    Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and
    Emily B. Fox and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@article{Xiang21,
  title          = {Reconciling Legal and Technical Approaches to Algorithmic Bias},
  author         = {Alice Xiang},
  year           = 2021,
  journal        = {Tennessee Law Review},
  volume         = 88,
  url            = {https://ssrn.com/abstract=3650635}
}
@inproceedings{xiao2017dna,
  title          = {{DNA}-{GAN}: {L}earning disentangled representations from multi-attribute images},
  author         = {Xiao, Taihong and Hong, Jiapeng and Ma, Jinwen},
  year           = 2018,
  booktitle      = {ICLR workshop}
}
@article{xiao2017fashion,
  title          = {Fashion-mnist: a novel image dataset for benchmarking machine learnin},
  author         = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year           = 2017,
  journal        = {ArXiv preprint},
  volume         = {abs/1708.07747},
  url            = {https://arxiv.org/abs/1708.07747}
}
@inproceedings{XuFairGAN:Networks,
  title          = {FairGAN: Fairness-aware Generative Adversarial Networks},
  author         = {Depeng Xu and Shuhan Yuan and Lu Zhang and Xintao Wu},
  year           = 2018,
  booktitle      = {
    {IEEE} International Conference on Big Data, Big Data 2018, Seattle, WA, USA, December 10-13,
    2018
  },
  publisher      = {{Ieee}},
  pages          = {570--575},
  doi            = {10.1109/BigData.2018.8622525},
  url            = {https://doi.org/10.1109/BigData.2018.8622525},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/bigdataconf/XuYZW18.bib},
  editor         = {
    Naoki Abe and Huan Liu and Calton Pu and Xiaohua Hu and Nesreen K. Ahmed and Mu Qiao and Yang
    Song and Donald Kossmann and Bing Liu and Kisung Lee and Jiliang Tang and Jingrui He and
    Jeffrey S. Saltz
  },
  timestamp      = {Wed, 27 Feb 2019 00:00:00 +0100}
}
@inproceedings{Yao2017BeyondFiltering,
  title          = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
  author         = {Sirui Yao and Bert Huang},
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {2921--2930},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/YaoH17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@article{Yeom2018,
  title          = {
    {Discriminative but Not Discriminatory: A Comparison of Fairness Definitions under Different
    Worldviews}
  },
  author         = {Yeom, Samuel and Tschantz, Michael Carl},
  year           = 2018,
  journal        = {ArXiv preprint},
  volume         = {abs/1808.08619},
  url            = {https://arxiv.org/abs/1808.08619}
}
@inproceedings{Zafar2015FairnessClassification,
  title          = {Fairness Constraints: Mechanisms for Fair Classification},
  author         = {Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez{-}Rodriguez and Krishna P. Gummadi},
  year           = 2017,
  booktitle      = {
    Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,
    {AISTATS} 2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}
  },
  publisher      = {{Pmlr}},
  series         = {Proceedings of Machine Learning Research},
  volume         = 54,
  pages          = {962--970},
  url            = {http://proceedings.mlr.press/v54/zafar17a.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/aistats/ZafarVGG17.bib},
  editor         = {Aarti Singh and Xiaojin (Jerry) Zhu},
  timestamp      = {Wed, 03 Apr 2019 01:00:00 +0200}
}
@inproceedings{ZafValRodGum17b,
  title          = {
    Fairness Beyond Disparate Treatment {\&} Disparate Impact: Learning Classification without
    Disparate Mistreatment
  },
  author         = {Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez{-}Rodriguez and Krishna P. Gummadi},
  year           = 2017,
  booktitle      = {Proceedings of the 26th International Conference on World Wide Web},
  location       = {Perth, Australia},
  publisher      = {{Acm}},
  address        = {Republic and Canton of Geneva, CHE},
  series         = {Www '17},
  pages          = {1171--1180},
  doi            = {10.1145/3038912.3052660},
  isbn           = 9781450349130,
  url            = {https://doi.org/10.1145/3038912.3052660},
  abstract       = {
    Automated data-driven decision making systems are increasingly being used to assist, or even
    replace humans in many settings. These systems function by learning from historical decisions,
    often taken by humans. In order to maximize the utility of these systems (or, classifiers),
    their training involves minimizing the errors (or, misclassifications) over the given
    historical data. However, it is quite possible that the optimally trained classifier makes
    decisions for people belonging to different social groups with different misclassification
    rates (e.g., misclassification rates for females are higher than for males), thereby placing
    these groups at an unfair disadvantage. To account for and avoid such unfairness, in this
    paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in
    terms of misclassification rates. We then propose intuitive measures of disparate mistreatment
    for decision boundary-based classifiers, which can be easily incorporated into their
    formulation as convex-concave constraints. Experiments on synthetic as well as real world
    datasets show that our methodology is effective at avoiding disparate mistreatment, often at a
    small cost in terms of accuracy.
  },
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/www/ZafarVGG17.bib},
  editor         = {Rick Barrett and Rick Cummings and Eugene Agichtein and Evgeniy Gabrilovich},
  keywords       = {
    fair decision making, fair classification, machine learning and law, discrimination in decision
    making, algorithmic decision making
  },
  timestamp      = {Tue, 06 Nov 2018 00:00:00 +0100}
}
@inproceedings{zaheer2017deep,
  title          = {Deep Sets},
  author         = {
    Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnab{\'{a}}s P{\'{o}}czos and
    Ruslan Salakhutdinov and Alexander J. Smola
  },
  year           = 2017,
  booktitle      = {Advances in Neural Information Processing Systems  (NIPS)},
  pages          = {3391--3401},
  url            = {https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/nips/ZaheerKRPSS17.bib},
  editor         = {
    Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and
    S. V. N. Vishwanathan and Roman Garnett
  },
  timestamp      = {Thu, 21 Jan 2021 00:00:00 +0100}
}
@inproceedings{ZehBonCasHajMegBae17,
  title          = {FA*IR: {A} Fair Top-k Ranking Algorithm},
  author         = {
    Meike Zehlike and Francesco Bonchi and Carlos Castillo and Sara Hajian and Mohamed Megahed and
    Ricardo Baeza{-}Yates
  },
  year           = 2017,
  booktitle      = {
    Proceedings of the 2017 {ACM} on Conference on Information and Knowledge Management, {CIKM}
    2017, Singapore, November 06 - 10, 2017
  },
  publisher      = {{Acm}},
  pages          = {1569--1578},
  doi            = {10.1145/3132847.3132938},
  url            = {https://doi.org/10.1145/3132847.3132938},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cikm/ZehlikeB0HMB17.bib},
  editor         = {
    Ee{-}Peng Lim and Marianne Winslett and Mark Sanderson and Ada Wai{-}Chee Fu and Jimeng Sun and
    J. Shane Culpepper and Eric Lo and Joyce C. Ho and Debora Donato and Rakesh Agrawal and Yu
    Zheng and Carlos Castillo and Aixin Sun and Vincent S. Tseng and Chenliang Li
  },
  timestamp      = {Sun, 25 Oct 2020 01:00:00 +0200}
}
@inproceedings{ZemWuSwePitetal13,
  title          = {Learning Fair Representations},
  author         = {Richard S. Zemel and Yu Wu and Kevin Swersky and Toniann Pitassi and Cynthia Dwork},
  year           = 2013,
  booktitle      = {International Conference on Machine Learning  (ICML)},
  publisher      = {JMLR.org},
  series         = {{JMLR} Workshop and Conference Proceedings},
  volume         = 28,
  pages          = {325--333},
  url            = {http://proceedings.mlr.press/v28/zemel13.html},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/ZemelWSPD13.bib},
  timestamp      = {Wed, 29 May 2019 01:00:00 +0200}
}
@inproceedings{ZhaLemMit18,
  title          = {Mitigating Unwanted Biases with Adversarial Learning},
  author         = {Zhang, Brian~Hu and Lemoine, Blake and Mitchell, Margaret},
  year           = 2018,
  booktitle      = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  location       = {New Orleans, LA, USA},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {Aies '18},
  pages          = {335--340},
  doi            = {10.1145/3278721.3278779},
  isbn           = 9781450360128,
  url            = {https://doi.org/10.1145/3278721.3278779},
  abstract       = {
    Machine learning is a tool for building models that accurately represent input training data.
    When undesired biases concerning demographic groups are in the training data, well-trained
    models will reflect those biases. We present a framework for mitigating such biases by
    including a variable for the group of interest and simultaneously learning a predictor and an
    adversary. The input to the network X, here text or census data, produces a prediction Y, such
    as an analogy completion or income bracket, while the adversary tries to model a protected
    variable Z, here gender or zip code. The objective is to maximize the predictor's ability to
    predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion,
    this method results in accurate predictions that exhibit less evidence of stereotyping Z. When
    applied to a classification task using the UCI Adult (Census) Dataset, it results in a
    predictive model that does not lose much accuracy while achieving very close to equality of
    odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of
    fairness as well as a wide range of gradient-based learning models, including both regression
    and classification tasks.
  },
  keywords       = {unbiasing, multi-task learning, adversarial learning, debiasing},
  numpages       = 6
}
@inproceedings{zhang2018examining,
  title          = {Examining {CNN} Representations With Respect to Dataset Bias},
  author         = {Quanshi Zhang and Wenguan Wang and Song{-}Chun Zhu},
  year           = 2018,
  booktitle      = {{AAAI} Conference on Artificial Intelligence},
  publisher      = {{AAAI} Press},
  pages          = {4464--4473},
  url            = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17429},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/aaai/ZhangWZ18.bib},
  editor         = {Sheila A. McIlraith and Kilian Q. Weinberger},
  timestamp      = {Mon, 22 Oct 2018 01:00:00 +0200}
}
@article{zhang2018visual,
  title          = {Visual interpretability for Deep Learning: a survey},
  author         = {Zhang, Quanshi and Zhu, Song-Chun},
  year           = 2018,
  journal        = {Frontiers of Information Technology \& Electronic Engineering},
  volume         = 19,
  number         = 1,
  pages          = {27--39}
}
@inproceedings{ZhuParIsoEfr17,
  title          = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
  author         = {Jun{-}Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
  year           = 2017,
  booktitle      = {
    {IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice, Italy, October 22-29,
    2017
  },
  publisher      = {{IEEE} Computer Society},
  pages          = {2242--2251},
  doi            = {10.1109/iccv.2017.244},
  url            = {https://doi.org/10.1109/ICCV.2017.244},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/iccv/ZhuPIE17.bib},
  timestamp      = {Thu, 11 Jan 2018 00:00:00 +0100}
}
@misc{BrazilianAdmissions,
  author         = {Castro da Silva, Bruno},
  publisher      = {Harvard Dataverse},
  title          = {{UFRGS Entrance Exam and GPA Data}},
  UNF            = {UNF:6:MQqEQGXiIfQTbS7q9QJ5uw==},
  year           = {2019},
  version        = {V1},
  doi            = {10.7910/DVN/O35FW8},
  url            = {https://doi.org/10.7910/DVN/O35FW8}
}
@article{Alb19,
  title          = {AI in talent acquisition: a review of AI-applications used in recruitment and selection},
  author         = {Albert, Edward~Tristram},
  journal        = {Strategic HR Review},
  year           = {2019},
  publisher      = {Emerald Publishing Limited}
}
@article{Jum21,
  author         = {Jumper, John
                    and Evans, Richard
                    and Pritzel, Alexander
                    and Green, Tim
                    and Figurnov, Michael
                    and Ronneberger, Olaf
                    and Tunyasuvunakool, Kathryn
                    and Bates, Russ
                    and {\v{Z}}{\'i}dek, Augustin
                    and Potapenko, Anna
                    and Bridgland, Alex
                    and Meyer, Clemens
                    and Kohl, Simon A. A.
                    and Ballard, Andrew J.
                    and Cowie, Andrew
                    and Romera-Paredes, Bernardino
                    and Nikolov, Stanislav
                    and Jain, Rishub
                    and Adler, Jonas
                    and Back, Trevor
                    and Petersen, Stig
                    and Reiman, David
                    and Clancy, Ellen
                    and Zielinski, Michal
                    and Steinegger, Martin
                    and Pacholska, Michalina
                    and Berghammer, Tamas
                    and Bodenstein, Sebastian
                    and Silver, David
                    and Vinyals, Oriol
                    and Senior, Andrew W.
                    and Kavukcuoglu, Koray
                    and Kohli, Pushmeet
                    and Hassabis, Demis},
  title          = {Highly accurate protein structure prediction with AlphaFold},
  journal        = {Nature},
  year           = {2021},
  month          = {Aug},
  day            = {01},
  volume         = {596},
  number         = {7873},
  pages          = {583-589},
  issn           = {1476-4687},
  doi            = {10.1038/s41586-021-03819-2},
  url            = {https://doi.org/10.1038/s41586-021-03819-2},
  abstract       = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50 years9. Despite recent progress10--14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.}
}
@article{Rav21,
  author         = {Ravuri, Suman
                    and Lenc, Karel
                    and Willson, Matthew
                    and Kangin, Dmitry
                    and Lam, Remi
                    and Mirowski, Piotr
                    and Fitzsimons, Megan
                    and Athanassiadou, Maria
                    and Kashem, Sheleem
                    and Madge, Sam
                    and Prudden, Rachel
                    and Mandhane, Amol
                    and Clark, Aidan
                    and Brock, Andrew
                    and Simonyan, Karen
                    and Hadsell, Raia
                    and Robinson, Niall
                    and Clancy, Ellen
                    and Arribas, Alberto
                    and Mohamed, Shakir},
  title          = {Skilful precipitation nowcasting using deep generative models of radar},
  journal        = {Nature},
  year           = {2021},
  month          = {Sep},
  day            = {01},
  volume         = {597},
  number         = {7878},
  pages          = {672-677},
  abstract       = {Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making1,2. State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations3,4. Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints5,6. While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536{\thinspace}km{\thinspace}{\texttimes}{\thinspace}1,280{\thinspace}km and with lead times from 5--90{\thinspace}min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89{\%} of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
  issn           = {1476-4687},
  doi            = {10.1038/s41586-021-03854-z},
  url            = {https://doi.org/10.1038/s41586-021-03854-z}
}

