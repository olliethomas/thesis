---
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
@string{AMSTrans = "American Mathematical Society Translations"}
@string{AMSTrans = "Amer. Math. Soc. Transl."}
@string{BullAMS = "Bulletin of the American Mathematical Society"}
@string{BullAMS = "Bull. Amer. Math. Soc."}
@string{ProcAMS = "Proceedings of the American Mathematical Society"}
@string{ProcAMS = "Proc. Amer. Math. Soc."}
@string{TransAMS = "Transactions of the American Mathematical Society"}
@string{TransAMS = "Trans. Amer. Math. Soc."}
%ACM
@string{CACM = "Communications of the {ACM}"}
@string{CACM = "Commun. {ACM}"}
@string{CompServ = "Comput. Surveys"}
@string{JACM = "J. ACM"}
@string{ACMMathSoft = "{ACM} Transactions on Mathematical Software"}
@string{ACMMathSoft = "{ACM} Trans. Math. Software"}
@string{SIGNUM = "{ACM} {SIGNUM} Newsletter"}
@string{SIGNUM = "{ACM} {SIGNUM} Newslett."}
@string{AmerSocio = "American Journal of Sociology"}
@string{AmerStatAssoc = "Journal of the American Statistical Association"}
@string{AmerStatAssoc = "J. Amer. Statist. Assoc."}
@string{ApplMathComp = "Applied Mathematics and Computation"}
@string{ApplMathComp = "Appl. Math. Comput."}
@string{AmerMathMonthly = "American Mathematical Monthly"}
@string{AmerMathMonthly = "Amer. Math. Monthly"}
@string{BIT = "{BIT}"}
@string{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology"}
@string{BritStatPsych = "Brit. J. Math. Statist. Psych."}
@string{CanMathBull = "Canadian Mathematical Bulletin"}
@string{CanMathBull = "Canad. Math. Bull."}
@string{CompApplMath = "Journal of Computational and Applied Mathematics"}
@string{CompApplMath = "J. Comput. Appl. Math."}
@string{CompPhys = "Journal of Computational Physics"}
@string{CompPhys = "J. Comput. Phys."}
@string{CompStruct = "Computers and Structures"}
@string{CompStruct = "Comput. \& Structures"}
@string{CompJour = "The Computer Journal"}
@string{CompJour = "Comput. J."}
@string{CompSysSci = "Journal of Computer and System Sciences"}
@string{CompSysSci = "J. Comput. System Sci."}
@string{Computing = "Computing"}
@string{ContempMath = "Contemporary Mathematics"}
@string{ContempMath = "Contemp. Math."}
@string{Crelle = "Crelle's Journal"}
@string{GiornaleMath = "Giornale di Mathematiche"}
@string{GiornaleMath = "Giorn. Mat."}
% didn't find in AMS MR., ibid.

 %IEEE
@string{Computer = "{IEEE} Computer"}
@string{IEEETransComp = "{IEEE} Transactions on Computers"}
@string{IEEETransComp = "{IEEE} Trans. Comput."}
@string{IEEETransAC = "{IEEE} Transactions on Automatic Control"}
@string{IEEETransAC = "{IEEE} Trans. Automat. Control"}
@string{IEEESpec = "{IEEE} Spectrum"}
% didn't find in AMS MR
@string{ProcIEEE = "Proceedings of the {IEEE}"}
@string{ProcIEEE = "Proc. {IEEE}"}
% didn't find in AMS MR
@string{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems"}
@string{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems"}
@string{IMANumerAna = "{IMA} Journal of Numerical Analysis"}
@string{IMANumerAna = "{IMA} J. Numer. Anal."}
@string{InfProcLet = "Information Processing Letters"}
@string{InfProcLet = "Inform. Process. Lett."}
@string{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications"}
@string{InstMathApp = "J. Inst. Math. Appl."}
@string{IntControl = "International Journal of Control"}
@string{IntControl = "Internat. J. Control"}
@string{IntNumerEng = "International Journal for Numerical Methods in
     Engineering"}
@string{IntNumerEng = "Internat. J. Numer. Methods Engrg."}
@string{IntSuper = "International Journal of Supercomputing Applications"}
@string{IntSuper = "Internat. J. Supercomputing Applic."}
% didn't find
%% in AMS MR
@string{Kibernetika = "Kibernetika"}
@string{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards"}
@string{JResNatBurStand = "J. Res. Nat. Bur. Standards"}
@string{LinAlgApp = "Linear Algebra and its Applications"}
@string{LinAlgApp = "Linear Algebra Appl."}
@string{MathAnaAppl = "Journal of Mathematical Analysis and Applications"}
@string{MathAnaAppl = "J. Math. Anal. Appl."}
@string{MathAnnalen = "Mathematische Annalen"}
@string{MathAnnalen = "Math. Ann."}
@string{MathPhys = "Journal of Mathematical Physics"}
@string{MathPhys = "J. Math. Phys."}
@string{MathComp = "Mathematics of Computation"}
@string{MathComp = "Math. Comp."}
@string{MathScand = "Mathematica Scandinavica"}
@string{MathScand = "Math. Scand."}
@string{TablesAidsComp = "Mathematical Tables and Other Aids to Computation"}
@string{TablesAidsComp = "Math. Tables Aids Comput."}
@string{NumerMath = "Numerische Mathematik"}
@string{NumerMath = "Numer. Math."}
@string{PacificMath = "Pacific Journal of Mathematics"}
@string{PacificMath = "Pacific J. Math."}
@string{ParDistComp = "Journal of Parallel and Distributed Computing"}
@string{ParDistComp = "J. Parallel and Distrib. Comput."}
% didn't find
%% in AMS MR
@string{ParComputing = "Parallel Computing"}
@string{ParComputing = "Parallel Comput."}
@string{PhilMag = "Philosophical Magazine"}
@string{PhilMag = "Philos. Mag."}
@string{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA"}
@string{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A."}
@string{Psychometrika = "Psychometrika"}
@string{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)"}
@string{QuartMath = "Quart. J. Math. Oxford Ser. (2)"}
@string{QuartApplMath = "Quarterly of Applied Mathematics"}
@string{QuartApplMath = "Quart. Appl. Math."}
@string{RevueInstStat = "Review of the International Statisical Institute"}
@string{RevueInstStat = "Rev. Inst. Internat. Statist."}
%SIAM
@string{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics"}
@string{JSIAM = "J. Soc. Indust. Appl. Math."}
@string{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis"}
@string{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal."}
@string{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods"}
@string{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods"}
@string{SIAMAppMath = "{SIAM} Journal on Applied Mathematics"}
@string{SIAMAppMath = "{SIAM} J. Appl. Math."}
@string{SIAMComp = "{SIAM} Journal on Computing"}
@string{SIAMComp = "{SIAM} J. Comput."}
@string{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications"}
@string{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl."}
@string{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis"}
@string{SIAMNumAnal = "{SIAM} J. Numer. Anal."}
@string{SIAMReview = "{SIAM} Review"}
@string{SIAMReview = "{SIAM} Rev."}
@string{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing"}
@string{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput."}
@string{SoftPracExp = "Software Practice and Experience"}
@string{SoftPracExp = "Software Prac. Experience"}
% didn't find in AMS MR
@string{StatScience = "Statistical Science"}
@string{StatScience = "Statist. Sci."}
@string{Techno = "Technometrics"}
@string{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics"}
@string{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys."}
@string{VLSICompSys = "Journal of {VLSI} and Computer Systems"}
@string{VLSICompSys = "J. {VLSI} Comput. Syst."}
@string{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik"}
@string{ZAngewMathMech = "Z. Angew. Math. Mech."}
@string{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik"}
@string{ZAngewMathPhys = "Z. Angew. Math. Phys."}
% Publishers % ================================================= |
@string{Academic = "Academic Press"}
@string{ACMPress = "{ACM} Press"}
@string{AdamHilger = "Adam Hilger"}
@string{AddisonWesley = "Addison-Wesley"}
@string{AllynBacon = "Allyn and Bacon"}
@string{AMS = "American Mathematical Society"}
@string{Birkhauser = "Birkha{\"u}ser"}
@string{CambridgePress = "Cambridge University Press"}
@string{Chelsea = "Chelsea"}
@string{ClaredonPress = "Claredon Press"}
@string{DoverPub = "Dover Publications"}
@string{Eyolles = "Eyolles"}
@string{HoltRinehartWinston = "Holt, Rinehart and Winston"}
@string{Interscience = "Interscience"}
@string{JohnsHopkinsPress = "The Johns Hopkins University Press"}
@string{JohnWileySons = "John Wiley and Sons"}
@string{Macmillan = "Macmillan"}
@string{MathWorks = "The Math Works Inc."}
@string{McGrawHill = "McGraw-Hill"}
@string{NatBurStd = "National Bureau of Standards"}
@string{NorthHolland = "North-Holland"}
@string{OxfordPress = "Oxford University Press"}
%address Oxford or London?
@string{PergamonPress = "Pergamon Press"}
@string{PlenumPress = "Plenum Press"}
@string{PrenticeHall = "Prentice-Hall"}
@string{SIAMPub = "{SIAM} Publications"}
@string{Springer = "Springer-Verlag"}
@string{TexasPress = "University of Texas Press"}
@string{VanNostrand = "Van Nostrand"}
@string{WHFreeman = "W. H. Freeman and Co."}
---
%Entries
@article{GarSchRosVioRezEslTeh18,
  title        = {Neural processes},
  author       = {Garnelo, Marta and
                  Schwarz, Jonathan and
                  Rosenbaum, Dan and
                  Viola, Fabio and
                  Rezende, Danilo~J and
                  Eslami, SM and
                  Teh, Yee Whye},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1807.01622}
}
@article{KehCheQua18,
  title        = {Interpretable Fairness via Target Labels in Gaussian Process Models},
  author       = {Kehrenberg, Thomas and
                  Chen, Zexun and
                  Quadrianto, Novi},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1810.05598}
}
@inproceedings{HeiLoiGumKra19,
  author       = {Heidari, Hoda and
                  Loi, Michele and
                  Gummadi, Krishna~P. and
                  Krause, Andreas},
  title        = {A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity},
  year         = {2019},
  isbn         = {9781450361255},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3287560.3287584},
  doi          = {10.1145/3287560.3287584},
  abstract     = {We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages        = {181–190},
  numpages     = {10},
  keywords     = {Statistical Parity, Equality of Opportunity (EOP), Rawlsian and Luck Egalitarian EOP, Equality of Odds, Predictive Value Parity, Fairness for Machine Learning},
  location     = {Atlanta, GA, USA},
  series       = {FAT* '19}
}
@inproceedings{BolChaZouSalKal16,
  title        = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author       = {Bolukbasi, Tolga and
                  Chang, Kai-Wei and
                  Zou, James~Y and
                  Saligrama, Venkatesh and
                  Kalai, Adam~T},
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems 29},
  pages        = {4349--4357},
  editor       = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{KleMulRag17,
  title        = {Inherent Trade-Offs in the Fair Determination of Risk Scores},
  author       = {Kleinberg, Jon~M. and
                  Mullainathan, Sendhil and
                  Raghavan, Manish},
  year         = 2017,
  booktitle    = {Innovations in Theoretical Computer Science Conference},
  pages        = {43:1--43:23}
}
@article{Cho17,
  author       = {Chouldechova, Alexandra},
  title        = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  journal      = {Big Data},
  volume       = {5},
  number       = {2},
  pages        = {153-163},
  year         = {2017},
  doi          = {10.1089/big.2016.0047},
  note         = {PMID: 28632438},
  URL          = {https://doi.org/10.1089/big.2016.0047},
  eprint       = {https://doi.org/10.1089/big.2016.0047},
  abstract     = {Abstract Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance.}
}
@inproceedings{RusKusLofSil17,
  title        = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},
  author       = {Russell, Chris and
                  Kusner, Matt~J and
                  Loftus, Joshua and
                  Silva, Ricardo},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems 30},
  pages        = {6414--6423},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{ChoRot18,
  author       = {Chouldechova, Alexandra and
                  Roth, Aaron},
  title        = {The Frontiers of Fairness in Machine Learning},
  journal      = {CoRR},
  volume       = {abs/1810.08810},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.08810},
  archivePrefix = {arXiv},
  eprint       = {1810.08810},
  timestamp    = {Tue, 08 Oct 2019 14:34:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-08810.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{KamCal12,
  title        = {Data preprocessing techniques for classification without discrimination},
  author       = {Kamiran, Faisal and
                  Calders, Toon},
  year         = 2012,
  journal      = {Knowledge and Information Systems},
  publisher    = {Springer},
  volume       = 33,
  number       = 1,
  pages        = {1--33},
  doi          = {10.1007/s10115-011-0463-8},
  isbn         = 1011501104,
  issn         = {02191377},
  keywords     = {Classification, Discrimination-aware data mining, Preprocessing}
}
@inproceedings{AdeValGhaWel29,
  title        = {One-network adversarial fairness},
  author       = {Adel, Tameem and
                  Valera, Isabel and
                  Ghahramani, Zoubin and
                  Weller, Adrian},
  year         = 2019,
  booktitle    = {Thirty-Third AAAI Conference on Artificial Intelligence}
}
@article{SatHofCheVar18,
  title        = {Fairness gan},
  author       = {Sattigeri, Prasanna and
                  Hoffman, Samuel~C and
                  Chenthamarakshan, Vijil and
                  Varshney, Kush~R},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1805.09910}
}
@inproceedings{XuYuaZhaWu18,
  title        = {Fairgan: Fairness-aware generative adversarial networks},
  author       = {Xu, Depeng and
                  Yuan, Shuhan and
                  Zhang, Lu and
                  Wu, Xintao},
  year         = 2018,
  booktitle    = {2018 IEEE International Conference on Big Data (Big Data)},
  pages        = {570--575},
  organization = {IEEE}
}
@article{LouShiSchWel19,
  title        = {The Functional Neural Process},
  author       = {Louizos, Christos and
                  Shi, Xiahan and
                  Schutte, Klamer and
                  Welling, Max},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1906.08324}
}
@inproceedings{QuaShaTho19,
  title        = {Discovering fair representations in the data domain},
  author       = {Quadrianto, Novi and
                  Sharmanska, Viktoriia and
                  Thomas, Oliver},
  year         = 2019,
  booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages        = {8227--8236}
}
@inproceedings{KilBalKusWeletal19,
  title        = {The Sensitivity of Counterfactual Fairness to Unmeasured Confounding},
  author       = {Kilbertus, Niki and
                  Ball, Philip~J. and
                  Kusner, Matt~J. and
                  Weller, Adrian and
                  Silva, Ricardo},
  year         = 2019,
  booktitle    = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, {UAI}}
}
@misc{DuaGra17,
  author       = {Dua, Dheeru and
                  Graff, Casey},
  year         = 2017,
  title        = {{UCI} Machine Learning Repository},
  url          = {http://archive.ics.uci.edu/ml},
  institution  = {University of California, Irvine, School of Information and Computer Sciences}
}
@article{FouIslKeyPan20,
  title        = {An Intersectional Definition of Fairness},
  author       = {Foulds, James~R. and
                  Islam, Rashidul and
                  Keya, Kamrun~Naher and
                  Pan, Shimei},
  ISBN         = {9781728129037},
  url          = {http://dx.doi.org/10.1109/ICDE48307.2020.00203},
  DOI          = {10.1109/icde48307.2020.00203},
  journal      = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
  publisher    = {IEEE},
  year         = {2020},
  month        = apr
}
@misc{BarHar17,
  title        = {Fairness in Machine Learning},
  author       = {Barocas, S and
                  Hardt, M},
  journal      = {NIPS 2017},
  publisher    = {NIPS},
  url          = {https://nips.cc/Conferences/2017/Schedule?showEvent=8734}
}
@unpublished{CorGoe18,
  title        = {Defining and Designing Fair Algorithms},
  author       = {Corbett-Davies, S and
                  Goel, S},
  note         = {ICML 2018 Tutorial},
  url          = {https://icml.cc/Conferences/2018/Schedule?showEvent=1862}
}
@article{AngLarMatKir16,
  title        = {Machine bias},
  author       = {Angwin, Julia and
                  Larson, Jeff and
                  Mattu, Surya and
                  Kirchner, Lauren},
  journal      = {ProPublica, May},
  volume       = {23},
  number       = {2016},
  pages        = {139--159},
  year         = {2016}
}
@book{BarHarNar19,
  title        = {Fairness and Machine Learning},
  author       = {Barocas, Solon and
                  Hardt, Moritz and
                  Narayanan, Arvind},
  publisher    = {fairmlbook.org},
  note         = {\url{http://www.fairmlbook.org}},
  year         = {2019}
}
@misc{Vin17,
  title        = {DeepMind's AI became a superhuman chess player in a few hours, just for fun},
  author       = {Vincent, James},
  year         = 2017,
  month        = dec,
  journal      = {The Verge},
  publisher    = {The Verge},
  url          = {https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go}
}
@article{BroSan18,
  title        = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author       = {Brown, N. and
                  Sandholm, T.},
  year         = 2018,
  month        = jan,
  journal      = {Science},
  volume       = 359,
  pages        = {418--424},
  doi          = {10.1126/science.aao1733},
  adsurl       = {http://adsabs.harvard.edu/abs/2018Sci...359..418B},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{SilSchSim17,
  title        = {Mastering the game of Go without human knowledge},
  author       = {Silver, David and
                  Schrittwieser, Julian and
                  Simonyan, Karen and
                  Antonoglou, Ioannis and
                  Huang, Aja and
                  Guez, Arthur and
                  Hubert, Thomas and
                  Baker, Lucas and
                  Lai, Matthew and
                  Bolton, Adrian and
                  Chen, Yutian and
                  Lillicrap, Timothy and
                  Hui, Fan and
                  Sifre, Laurent and
                  van~den~Driessche, George and
                  Graepel, Thore and
                  Hassabis, Demis},
  year         = 2017,
  month        = 10,
  day          = 18,
  journal      = {Nature},
  publisher    = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
  volume       = 550,
  pages        = {354 EP  -},
  url          = {http://dx.doi.org/10.1038/nature24270},
  date         = {2017/10/18/online},
  date-added   = {2018-09-25 12:15:14 +0000},
  date-modified = {2018-09-25 12:15:14 +0000},
  l3           = {10.1038/nature24270; https://www.nature.com/articles/nature24270#supplementary-information},
  m3           = {Article},
  ty           = {JOUR},
  bdsk-url-1   = {http://dx.doi.org/10.1038/nature24270}
}
@article{SinGanSinSar16,
  title        = {Machine Learning for High-Throughput Stress Phenotyping in Plants},
  author       = {Singh, Arti and
                  Ganapathysubramanian, Baskar and
                  Singh, Asheesh~Kumar and
                  Sarkar, Soumik},
  year         = 2016,
  journal      = {Trends in Plant Science},
  volume       = 21,
  number       = 2,
  pages        = {110--124},
  doi          = {https://doi.org/10.1016/j.tplants.2015.10.015},
  issn         = {1360-1385},
  url          = {http://www.sciencedirect.com/science/article/pii/S1360138515002630},
  keywords     = {high-throughput phenotyping, machine learning, Imaging, plant breeding, biotic stress, abiotic stress},
  abstract     = {Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits.}
}
@misc{Ideal,
  title        = {AI For Recruiting Software | High-Volume Hiring | Maximize Quality of Hire},
  journal      = {Ideal},
  url          = {https://ideal.com/}
}
@book{Mol18,
  title        = {Interpretable Machine Learning},
  author       = {Christoph Molnar},
  year         = 2018,
  publisher    = {https://christophm.github.io/interpretable-ml-book/}
}
@misc{USCBQ,
  title        = {U.S. Census Bureau QuickFacts: UNITED STATES},
  journal      = {Census Bureau QuickFacts},
  url          = {https://www.census.gov/quickfacts/fact/table/US/PST045217}
}
@misc{USNCES,
  title        = {Mobile Digest of Education Statistics, 2017},
  journal      = {National Center for Education Statistics (NCES) Home Page, a part of the U.S. Department of Education},
  publisher    = {National Center for Education Statistics},
  url          = {https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx}
}
@book{Pearl09,
  title        = {Causality: Models, Reasoning and Inference},
  author       = {Pearl, Judea},
  year         = 2009,
  publisher    = {Cambridge University Press},
  address      = {New York, NY, USA},
  isbn         = {052189560X, 9780521895606},
  edition      = {2nd}
}
@inproceedings{KamCal09,
  title        = {Classifying without discriminating},
  author       = {Kamiran, Faisal and
                  Calders, Toon},
  year         = 2009,
  booktitle    = {2009 2nd International Conference on Computer, Control and Communication, IC4 2009},
  doi          = {10.1109/IC4.2009.4909197},
  isbn         = 9781424433148
}
@techreport{kamiran2011,
  title        = {Discrimination-aware Classification},
  author       = {Kamiran, F.},
  year         = 2011,
  address      = {Eindhoven},
  pages        = 157,
  doi          = {10.6100/IR717576},
  isbn         = 9789038627892,
  url          = {https://research.tue.nl/en/publications/discrimination-aware-classification},
  institution  = {Technische Universiteit Eindhoven},
  keywords     = {Classifiers, Data Mining, Education, Industry, Labeling, Learning Systems, Processing}
}
@inproceedings{RibSinGue16,
  title        = {"Why should I trust you?" Explaining the predictions of any classifier},
  author       = {Ribeiro, Marco~Tulio and
                  Singh, Sameer and
                  Guestrin, Carlos},
  year         = 2016,
  booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages        = {1135--1144}
}
@article{FarKouThoSriGet18,
  author       = {Farnadi, Golnoosh and
                  Kouki, Pigi and
                  Thompson, Spencer~K. and
                  Srinivasan, Sriram and
                  Getoor, Lise},
  title        = {A Fairness-aware Hybrid Recommender System},
  journal      = {CoRR},
  volume       = {abs/1809.09030},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09030},
  archivePrefix= {arXiv},
  eprint       = {1809.09030},
  timestamp    = {Fri, 04 Sep 2020 14:32:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{WadVerPie18,
  author       = {Wadsworth, Christina and
                  Vera, Francesca and
                  Piech, Chris},
  title        = {Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction},
  journal      = {CoRR},
  volume       = {abs/1807.00199},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.00199},
  archivePrefix= {arXiv},
  eprint       = {1807.00199},
  timestamp    = {Mon, 13 Aug 2018 16:47:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-00199.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ElaGol18,
  title        = {Adversarial Removal of Demographic Attributes from Text Data},
  author       = {Elazar, Yanai and
                  Goldberg, Yoav},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month        = oct,
  year         = 2018,
  address      = {Brussels, Belgium},
  publisher    = {Association for Computational Linguistics},
  url          = {https://www.aclweb.org/anthology/D18-1002},
  doi          = {10.18653/v1/D18-1002},
  pages        = {11--21},
  abstract     = {Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in{---}and can be recovered from{---}the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to{---}and likely condition on{---}demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.},
}
@article{Dia14,
  title        = {Algorithmic accountability reporting: On the investigation of black boxes},
  author       = {Diakopoulos, Nicholas},
  year         = 2014,
  journal      = {NA}
}
@article{BurSonOrd08,
  title        = {Balanced Neighborhoods for Multi-sided Fairness in Recommendation},
  author       = {Burke, Robin and
                  Sonboli, Nasim and
                  Ordonez-Gauger, Aldo},
  year         = 2018,
  journal      = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  volume       = 81,
  number       = 2008,
  pages        = {202--214},
  url          = {http://proceedings.mlr.press/v81/burke18a.html},
  keywords     = {fair-, multi-sided platform, ness, recommender systems, sparse linear}
}
@inproceedings{YaoHua17,
  author       = {Yao, Sirui and
                  Huang, Bert},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages        = {},
  publisher    = {Curran Associates, Inc.},
  title        = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
  url          = {https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
  volume       = {30},
  year         = {2017}
}
@article{MunSmiPat16,
  title        = {Big Data : A Report on Algorithmic Systems , Opportunity , and Civil Rights Big Data : A Report on Algorithmic Systems , Opportunity , and Civil Rights},
  author       = {Munoz, Cecilia and
                  Smith, Megan and
                  Patil, DJ},
  year         = 2016,
  journal      = {Executive Office of the President of USA},
  number       = {May},
  doi          = {10.1177/0162243915598056},
  isbn         = 9788578110796,
  issn         = {0162-2439},
  url          = {https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf},
  pmid         = 25246403,
  arxivid      = {arXiv:1011.1669v3}
}
@inproceedings{FelFriMoeSchVen15,
  author       = {Feldman, Michael and
                  Friedler, Sorelle A. and
                  Moeller, John and
                  Scheidegger, Carlos and
                  Venkatasubramanian, Suresh},
  title        = {Certifying and Removing Disparate Impact},
  year         = {2015},
  isbn         = {9781450336642},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/2783258.2783311},
  doi          = {10.1145/2783258.2783311},
  abstract     = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
  booktitle    = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages        = {259–268},
  numpages     = {10},
  keywords     = {fairness, disparate impact, machine learning},
  location     = {Sydney, NSW, Australia},
  series       = {KDD '15}
}
@article{Hwa18,
  author       = {Tim Hwang},
  title        = {Computational Power and the Social Impact of Artificial Intelligence},
  journal      = {CoRR},
  volume       = {abs/1803.08971},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.08971},
  archivePrefix= {arXiv},
  eprint       = {1803.08971},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-08971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  note         = {Available at SSRN 3147971}
}
@article{Swe13,
  author       = {Sweeney, Latanya},
  title        = {Discrimination in Online Ad Delivery},
  year         = {2013},
  issue_date   = {May 2013},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = {56},
  number       = {5},
  issn         = {0001-0782},
  url          = {https://doi.org/10.1145/2447976.2447990},
  doi          = {10.1145/2447976.2447990},
  abstract     = {Google ads, black names and white names, racial discrimination, and click advertising.},
  journal      = {Commun. ACM},
  month        = may,
  pages        = {44–54},
  numpages     = {11}
}
@article{HinCooMamDee18,
  author       = {Hinnefeld, J.~Henry and
                  Cooman, Peter and
                  Mammo, Nat and
                  Deese, Rupert},
  title        = {Evaluating Fairness Metrics in the Presence of Dataset Bias},
  journal      = {CoRR},
  volume       = {abs/1809.09245},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09245},
  archivePrefix= {arXiv},
  eprint       = {1809.09245},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09245.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ZehBonCasHajMegBae17,
  author       = {Zehlike, Meike and
                  Bonchi, Francesco and
                  Castillo, Carlos and
                  Hajian, Sara and
                  Megahed, Mohamed and
                  Baeza-Yates, Ricardo},
  title        = {FA*IR: A Fair Top-k Ranking Algorithm},
  year         = {2017},
  isbn         = {9781450349185},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3132847.3132938},
  doi          = {10.1145/3132847.3132938},
  abstract     = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
  booktitle    = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages        = {1569–1578},
  numpages     = {10},
  keywords     = {ranking, top-k selection, bias in computer systems, algorithmic fairness},
  location     = {Singapore, Singapore},
  series       = {CIKM '17}
}
@inproceedings{ElzJabJunKeaNeeRotSch19,
  author       = {Elzayn, Hadi and
                  Jabbari, Shahin and
                  Jung, Christopher and
                  Kearns, Michael and
                  Neel, Seth and
                  Roth, Aaron and
                  Schutzman, Zachary},
  title        = {Fair Algorithms for Learning in Allocation Problems},
  year         = {2019},
  isbn         = {9781450361255},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3287560.3287571},
  doi          = {10.1145/3287560.3287571},
  abstract     = {Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages        = {170–179},
  numpages     = {10},
  keywords     = {online learning, censored feedback, algorithmic fairness, resource allocation},
  location     = {Atlanta, GA, USA},
  series       = {FAT* '19}
}
@article{ZafValGomKri19,
  author       = {Zafar, Muhammad~Bilal  and
                  Valera, Isabel and
                  Gomez-Rodriguez, Manuel and
                  Krishna~P. Gummadi},
  title        = {Fairness Constraints: A Flexible Approach for Fair Classification},
  journal      = {Journal of Machine Learning Research},
  year         = {2019},
  volume       = {20},
  number       = {75},
  pages        = {1-42},
  url          = {http://jmlr.org/papers/v20/18-262.html}
}
@inproceedings{DwoHarPitReiZem12,
  author       = {Dwork, Cynthia and
                  Hardt, Moritz and
                  Pitassi, Toniann and
                  Reingold, Omer and
                  Zemel, Richard},
  title        = {Fairness through Awareness},
  year         = {2012},
  isbn         = {9781450311151},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/2090236.2090255},
  doi          = {10.1145/2090236.2090255},
  abstract     = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  booktitle    = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  pages        = {214–226},
  numpages     = {13},
  location     = {Cambridge, Massachusetts},
  series       = {ITCS '12}
}
@InProceedings{AdaAdaGreJedKacMic18,
  author       = {Adamski, Igor and
                  Adamski, Robert and
                  Grel, Tomasz and
                  J{\k{e}}drych, Adam and
                  Kaczmarek, Kamil and
                  Michalewski, Henryk},
  editor       = {Yokota, Rio and
                  Weiland, Mich{\`e}le and
                  Keyes, David and
                  Trinitis, Carsten},
  title        = {Distributed Deep Reinforcement Learning: Learn How to Play Atari Games in 21Â minutes},
  booktitle    = {High Performance Computing},
  year         = 2018,
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {370--388},
  abstract     = {We present a study in Distributed Deep Reinforcement Learning (DDRL) focused on scalability of a state-of-the-art Deep Reinforcement Learning algorithm known as Batch Asynchronous Advantage Actor-Critic (BA3C). We show that using the Adam optimization algorithm with a batch size of upÂ to 2048 is a viable choice for carrying out large scale machine learning computations. This, combined with careful reexamination of the optimizer's hyperparameters, using synchronous training on the node level (while keeping the local, single node part of the algorithm asynchronous) and minimizing the model's memory footprint, allowed us to achieve linear scaling for upÂ to 64 CPU nodes. This corresponds to a training time of 21Â min on 768 CPU cores, as opposed to the 10Â h required when using a single node with 24 cores achieved by a baseline single-node implementation.},
  isbn         = {978-3-319-92040-5}
}
@article{LuoRugTur11,
  title        = {k-NN as an implementation of situation testing for discrimination discovery and prevention},
  author       = {Luong, Binh Thanh and
                  Ruggieri, Salvatore and
                  Turini, Franco},
  year         = 2011,
  journal      = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
  pages        = 502,
  doi          = {10.1145/2020408.2020488},
  isbn         = 9781450308137,
  url          = {http://dl.acm.org/citation.cfm?doid=2020408.2020488},
  keywords     = {discrimination discovery and prevention, k-nn classification}
}
@InProceedings{MadCrePitZem18,
  title        = {Learning Adversarially Fair and Transferable Representations},
  author       = {Madras, David and
                  Creager, Elliot and
                  Pitassi, Toniann and
                  Zemel, Richard},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  pages        = {3384--3393},
  year         = {2018},
  editor       = {Dy, Jennifer and Krause, Andreas},
  volume       = {80},
  series       = {Proceedings of Machine Learning Research},
  month        = {10--15 Jul},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v80/madras18a/madras18a.pdf},
  url          = {http://proceedings.mlr.press/v80/madras18a.html},
  abstract     = {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.}
}
@article{HolLiuVo16,
  title        = {Machine learning applied to weather forecasting},
  author       = {Holmstrom, Mark and
                  Liu, Dylan and
                  Vo, Christopher},
  year         = 2016,
  journal      = {Stanford University},
  pages        = {2--4}
}
@inproceedings{ZhaLemMit18,
  author       = {Zhang, Brian~Hu and
                  Lemoine, Blake and
                  Mitchell, Margaret},
  title        = {Mitigating Unwanted Biases with Adversarial Learning},
  year         = {2018},
  isbn         = {9781450360128},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3278721.3278779},
  doi          = {10.1145/3278721.3278779},
  abstract     = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
  booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages        = {335–340},
  numpages     = {6},
  keywords     = {unbiasing, multi-task learning, adversarial learning, debiasing},
  location     = {New Orleans, LA, USA},
  series       = {AIES '18}
}
@article{GupCotFarWan18,
  author       = {Gupta, Maya~R. and
                  Cotter, Andrew and
                  Fard, Mahdi~Milani and
                  Wang, Serena},
  title        = {Proxy Fairness},
  journal      = {CoRR},
  volume       = {abs/1806.11212},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.11212},
  archivePrefix= {arXiv},
  eprint       = {1806.11212},
  timestamp    = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1806-11212.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{QuaSha17,
  author       = {Quadrianto, Novi and
                  Sharmanska, Viktoriia},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher    = {Curran Associates, Inc.},
  title        = {Recycling Privileged Learning and Distribution Matching for Fairness},
  url          = {https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
  volume       = {30},
  year         = {2017}
}
@article{AnaCra18,
  title        = {Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability},
  author       = {Ananny, Mike and
                  Crawford, Kate},
  year         = 2018,
  journal      = {new media \& society},
  publisher    = {SAGE Publications Sage UK: London, England},
  volume       = 20,
  number       = 3,
  pages        = {973--989}
}
@article{HOL18,
  title        = {SELECT COMMITTEE ON ARTIFICIAL INTELLIGENCE},
  author       = {HOUSE, OF LORDS},
  year         = 2018,
  journal      = {AI in the UK: ready, willing and able}
}
@article{CitPas14,
  title        = {The scored society: Due process for automated predictions},
  author       = {Citron, Danielle Keats and
                  Pasquale, Frank},
  year         = 2014,
  journal      = {Wash. L. Rev.},
  publisher    = {HeinOnline},
  volume       = 89,
  pages        = 1
}
@book{Roth88,
  title        = {The Shapley value: essays in honor of Lloyd S. Shapley},
  author       = {Roth, Alvin E},
  year         = 1988,
  publisher    = {Cambridge University Press}
}
@article{WacMitFlo17,
  title        = {Transparent, explainable, and accountable AI for robotics},
  author       = {Wachter, Sandra and
                  Mittelstadt, Brent and
                  Floridi, Luciano},
  year         = 2017,
  journal      = {NA}
}
@article{DeDeo14,
  author       = {DeDeo, Simon},
  title        = {"Wrong side of the tracks": Big Data and Protected Categories},
  journal      = {CoRR},
  volume       = {abs/1412.4643},
  year         = {2014},
  url          = {http://arxiv.org/abs/1412.4643},
  archivePrefix= {arXiv},
  eprint       = {1412.4643},
  timestamp    = {Mon, 13 Aug 2018 16:47:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/DeDeo14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{Hill20,
  title        = {Wrongfully Accused by an Algorithm},
  author       = {Hill, Kashmir},
  year         = 2020,
  month        = jun,
  journal      = {The New York Times},
  publisher    = {The New York Times},
  url          = {https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html}
}
@misc{Reu18,
  title        = {Amazon ditched AI recruiting tool that favored men for technical jobs},
  url          = {https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine},
  journal      = {The Guardian},
  publisher    = {Guardian News and Media},
  author       = {Reuters},
  year         = {2018},
  month        = {Oct}
}
@misc{BBC19,
  title        = {Police officers raise concerns about 'biased' AI data},
  url          = {https://www.bbc.co.uk/news/technology-49717378},
  journal      = {BBC News},
  publisher    = {BBC},
  year         = {2019},
  month        = {Sep}
}
@misc{CisKoy19,
  title        = {Representation Learning and Fairness},
  url          = {https://neurips.cc/Conferences/2019/Schedule?showEvent=13212},
  author       = {Cisse, Moustapha and
                  Koyejo, Sanmi},
  note         = {NeurIPS 2019 Tutorial},
}
@article{FriSchVen16,
  title        = {On the (im)possibility of fairness},
  author       = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year         = 2016,
  journal      = {CoRR},
  volume       = {abs/1609.07236},
  url          = {http://arxiv.org/abs/1609.07236},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/FriedlerSV16},
  timestamp    = {Mon, 03 Oct 2016 17:51:10 +0200}
}
@inproceedings{HarPriSre16,
  title        = {Equality of Opportunity in Supervised Learning},
  author       = {Hardt, Moritz and Price, Eric and Srebro, Nati},
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems 29},
  publisher    = {Curran Associates, Inc.},
  pages        = {3315--3323},
  url          = {http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf},
  editor       = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@incollection{KusLofRusSil17,
  title        = {Counterfactual Fairness},
  author       = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems 30},
  publisher    = {Curran Associates, Inc.},
  pages        = {4066–4076},
  url          = {http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf},
  editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.}
}
@incollection{WicPanTri19,
  title        = {Unlocking Fairness: a Trade-off Revisited},
  author       = {Wick, Michael and Panda, Swetasudha and Tristan, Jean-Baptiste},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32},
  publisher    = {Curran Associates, Inc.},
  pages        = {8783--8792},
  url          = {http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{KehBarThoQua20,
  author       = {Kehrenberg, Thomas and
                  Bartlett, Myles and
                  Thomas, Oliver and
                  Quadrianto, Novi},
  editor       = {Vedaldi, Andrea and
                  Bischof, Horst and
                  Brox, Thomas and
                  Frahm, Jan-Michael},
  title        = {Null-Sampling for Interpretable and Fair Representations},
  booktitle    = {Computer Vision -- ECCV 2020},
  year         = {2020},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {565--580},
  abstract     = {We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. Invariance implies a selectivity for high level, relevant correlations w.r.t. class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished. To address this problem, we introduce an adversarially trained model with a null-sampling procedure to produce invariant representations in the data domain. To enable disentanglement, a partially-labelled representative set is used. By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors. We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset. (The code can be found at https://github.com/predictive-analytics-lab/nifr).},
  isbn         = {978-3-030-58574-7}
}
@inproceedings{NabShp18,
  title        = {Fair Inference on Outcomes},
  author       = {Nabi, Razieh and Shpitser, Ilya},
  year         = 2018,
  booktitle    = {Proceedings of the Thirty Second Conference on Association for the Advancement of Artificial Intelligence (AAAI-32nd)},
  publisher    = {AAAI Press}
}
@inproceedings{Chi19,
  title        = {Path-specific counterfactual fairness},
  author       = {Chiappa, Silvia},
  year         = 2019,
  booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = 33,
  pages        = {7801--7808}
}
@article{BeuCheZhaChi17,
  title        = {Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations},
  author       = {Beutel, Alex and Chen, Jilin and Zhao, Zhe and Chi, Ed H.},
  year         = 2017,
  journal      = {CoRR},
  volume       = {abs/1707.00075},
  url          = {http://arxiv.org/abs/1707.00075},
  archiveprefix= {arXiv},
  eprint       = {1707.00075},
  timestamp    = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BeutelCZC17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{MadCrePitZem18b,
  title        = {Fairness Through Causal Awareness: Learning Latent-Variable Models for Biased Data},
  author       = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard S.},
  year         = 2018,
  journal      = {CoRR},
  volume       = {abs/1809.02519},
  url          = {http://arxiv.org/abs/1809.02519},
  archiveprefix= {arXiv},
  eprint       = {1809.02519},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-02519.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{LiuDeaRolSimetal18,
  title        = {Delayed Impact of Fair Machine Learning},
  author       = {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {StockholmsmÃ€ssan, Stockholm Sweden},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {3150--3158},
  url          = {http://proceedings.mlr.press/v80/liu18c.html},
  editor       = {Jennifer Dy and Andreas Krause},
  pdf          = {http://proceedings.mlr.press/v80/liu18c/liu18c.pdf},
  abstract     = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.}
}
@inproceedings{ZemWuSwePitDwo13,
  title        = {Learning Fair Representations},
  author       = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  pages        = {325--333},
  url          = {http://proceedings.mlr.press/v28/zemel13.html},
  editor       = {Sanjoy Dasgupta and David McAllester},
  pdf          = {http://proceedings.mlr.press/v28/zemel13.pdf},
  abstract     = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}
@inproceedings{EdwSto16,
  author       = {Edwards, Harrison and
                  Storkey, Amos~J. },
  editor       = {Bengio, Yoshua and
                  LeCun, Yann},
  title        = {Censoring Representations with an Adversary},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.05897},
  timestamp    = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/EdwardsS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{GanUstAjaGerLarLavMarLem16,
  title        = {Domain-Adversarial Training of Neural Networks},
  author       = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and March, Mario and Lempitsky, Victor},
  year         = 2016,
  journal      = {Journal of Machine Learning Research},
  volume       = 17,
  number       = 59,
  pages        = {1--35},
  url          = {http://jmlr.org/papers/v17/15-239.html}
}
@inproceedings{LouSweLiWelZem15,
  title        = {The Variational Fair Autoencoder},
  author       = {Louizos, Christos and
                  Swersky, Kevin and
                  Li, Yujia and
                  Welling, Max and
                  Zemel, Richard~S.},
  year         = {2016},
  cdate        = {1451606400000},
  url          = {http://arxiv.org/abs/1511.00830},
  booktitle    = {ICLR},
  crossref     = {conf/iclr/2016}
}
@inproceedings{GreHorRasSchSmo07,
  title        = {A Kernel Approach to Comparing Distributions},
  author       = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  year         = 2007,
  month        = jul,
  journal      = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
  booktitle    = {Proceedings of the 22. AAAI Conference on Artificial Intelligence},
  publisher    = {AAAI Press},
  address      = {Menlo Park, CA, USA},
  pages        = {1637--1641},
  organization = {Max-Planck-Gesellschaft},
  institution  = {Association for the Advancement of Artificial Intelligence},
  school       = {Biologische Kybernetik},
  month_numeric = 7
}
@article{ChoRot20,
  title        = {A snapshot of the frontiers of fairness in machine learning},
  author       = {Alexandra Chouldechova and Aaron Roth},
  year         = 2020,
  journal      = {Commun. {ACM}},
  volume       = 63,
  number       = 5,
  pages        = {82--89}
}
@article{BarSel16,
  title        = {Big data's disparate impact},
  author       = {Barocas, S. and Selbst, A.},
  year         = 2016,
  journal      = {California Law Review},
  volume       = 104,
  number       = 3,
  pages        = {671--732}
}
@inproceedings{BarCraShaWal17,
  title        = {The problem with bias: from allocative to representational harms in machine learning},
  author       = {Barocas, Solon and Crawford, Kate and Shapiro, Aaron and Wallach, Hanna},
  year         = 2017,
  booktitle    = {Special Interest Group for Computing, Information and Society (SIGCIS)}
}
@inproceedings{KamAkaAso2012,
  title        = {Fairness-aware classifier with prejudice remover regularizer},
  author       = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  year         = 2012,
  booktitle    = {European conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  pages        = {35--50}
}
@article{CalVer10,
  title        = {Three naive Bayes approaches for discrimination-free classification},
  author       = {Calders, Toon and Verwer, Sicco},
  year         = 2010,
  month        = sep,
  day          = {01},
  journal      = {Data Mining and Knowledge Discovery},
  volume       = 21,
  number       = 2,
  pages        = {277--292},
  doi          = {10.1007/s10618-010-0190-x},
  issn         = {1573-756X},
  url          = {https://doi.org/10.1007/s10618-010-0190-x},
  abstract     = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.}
}
@inproceedings{JiaNac20,
  title        = {Identifying and Correcting Label Bias in Machine Learning},
  author       = {Jiang, Heinrich and Nachum, Ofir},
  year         = 2020,
  booktitle    = {The 23rd International Conference on Artificial Intelligence and Statistics, {AISTATS} 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 108,
  pages        = {702--712},
  editor       = {Chiappa, Silvia and Calandra, Roberto}
}
@inproceedings{KalZho18,
  title        = {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
  author       = {Kallus, Nathan and Zhou, Angela},
  year         = 2018,
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {2444--2453},
  editor       = {Dy, Jennifer G. and Krause, Andreas}
}
@inproceedings{PedRugTur08,
  title        = {Discrimination-aware data mining},
  author       = {Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  year         = 2008,
  booktitle    = {Proceedings of the 14th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008},
  publisher    = {ACM},
  pages        = {560--568},
  editor       = {Li, Ying and Liu, Bing and Sarawagi, Sunita},
  doi          = {10.1145/1401890.1401959},
  isbn         = 9781605581934
}
@inproceedings{AgaBeyDudLanetal18,
  title        = {A Reductions Approach to Fair Classification},
  author       = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Stockholmsmässan, Stockholm Sweden},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {60--69},
  url          = {http://proceedings.mlr.press/v80/agarwal18a.html},
  editor       = {Jennifer Dy and Andreas Krause},
  pdf          = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  abstract     = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.}
}
@article{Rubin90,
  title        = {Formal mode of statistical inference for causal effects},
  author       = {Rubin, Donald B},
  year         = 1990,
  journal      = {Journal of statistical planning and inference},
  publisher    = {Elsevier},
  volume       = 25,
  number       = 3,
  pages        = {279--292}
}
@article{Wachter20,
  title        = {Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI},
  author       = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year         = 2020,
  journal      = {SSRN Electronic Journal},
  publisher    = {Elsevier BV},
  doi          = {10.2139/ssrn.3547922},
  issn         = {1556-5068},
  url          = {http://dx.doi.org/10.2139/ssrn.3547922}
}
@inproceedings{MadPitZem18,
  title        = {Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
  author       = {Madras, David and Pitassi, Toni and Zemel, Richard},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 31,
  pages        = {6147--6157},
  url          = {https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf},
  editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@article{Miller19,
  title        = {Explanation in artificial intelligence: Insights from the social sciences},
  author       = {Tim Miller},
  year         = 2019,
  journal      = {Artificial Intelligence},
  volume       = 267,
  pages        = {1--38},
  doi          = {https://doi.org/10.1016/j.artint.2018.07.007},
  issn         = {0004-3702},
  url          = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
  keywords     = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  abstract     = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}
@book{Hume00,
  title        = {An enquiry concerning human understanding: A critical edition},
  author       = {Hume, David},
  year         = 2000,
  publisher    = {Oxford University Press},
  volume       = 3
}
@inproceedings{BluSta20,
  title        = {Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?},
  author       = {Avrim Blum and Kevin Stangl},
  year         = 2020,
  booktitle    = {1st Symposium on Foundations of Responsible Computing, {FORC} 2020, June 1-3, 2020, Harvard University, Cambridge, MA, {USA} (virtual conference)},
  publisher    = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  series       = {LIPIcs},
  volume       = 156,
  pages        = {3:1--3:20},
  editor       = {Aaron Roth}
}
@inproceedings{YeoTsc21,
  title        = {Avoiding Disparity Amplification under Different Worldviews},
  author       = {Samuel Yeom and Michael Carl Tschantz},
  year         = 2021,
  booktitle    = {ACM Conference on Fairness, Accountability, and Transparency}
}
@article{KamZliCal13,
  title        = {Quantifying explainable discrimination and removing illegal discrimination in automated decision making},
  author       = {Faisal Kamiran and Indrė Žliobaitė and Toon Calders},
  year         = 2013,
  journal      = {Knowl Inf Syst},
  volume       = 35,
  url          = {https://doi.org/10.1007/s10115-012-0584-8}
}
@article{ShaHenDarQua20,
  title        = {Contrastive Examples for Addressing the Tyranny of the Majority},
  author       = {Viktoriia Sharmanska and Lisa Anne Hendricks and Trevor Darrell and Novi Quadrianto},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2004.06524}
}
@article{GoeGuLiRe20,
  title        = {Model Patching: Closing the Subgroup Performance Gap with Data Augmentation},
  author       = {Karan Goel and Albert Gu and Yixuan Li and Christopher R{\'{e}}},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2008.06775}
}
@article{RosRub83,
  title        = {The central role of the propensity score in observational studies for causal effects},
  author       = {Paul R Rosenbaum and Donald B Rubin},
  year         = 1983,
  journal      = {Biometrika},
  volume       = 70,
  number       = 1,
  pages        = {41--55}
}
@misc{FeiGogUhl21,
  title        = {Hiding Behind Machines: When Blame Is Shifted to Artificial Agents},
  author       = {Till Feier and Jan Gogoll and Matthias Uhl},
  year         = 2021,
  eprint       = {2101.11465},
  archiveprefix = {arXiv},
  primaryclass = {cs.CY}
}
@article{Tol19,
  title        = {Fair and Unbiased Algorithmic Decision Making: Current State and Future Challenges},
  author       = {Song{\"{u}}l Tolan},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1901.04730},
  url          = {http://arxiv.org/abs/1901.04730},
  archiveprefix = {arXiv},
  eprint       = {1901.04730},
  timestamp    = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-04730.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
