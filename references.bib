---
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
@string{AMSTrans = "American Mathematical Society Translations"}
@string{AMSTrans = "Amer. Math. Soc. Transl."}
@string{BullAMS = "Bulletin of the American Mathematical Society"}
@string{BullAMS = "Bull. Amer. Math. Soc."}
@string{ProcAMS = "Proceedings of the American Mathematical Society"}
@string{ProcAMS = "Proc. Amer. Math. Soc."}
@string{TransAMS = "Transactions of the American Mathematical Society"}
@string{TransAMS = "Trans. Amer. Math. Soc."}
%ACM
@string{CACM = "Communications of the {ACM}"}
@string{CACM = "Commun. {ACM}"}
@string{CompServ = "Comput. Surveys"}
@string{JACM = "J. ACM"}
@string{ACMMathSoft = "{ACM} Transactions on Mathematical Software"}
@string{ACMMathSoft = "{ACM} Trans. Math. Software"}
@string{SIGNUM = "{ACM} {SIGNUM} Newsletter"}
@string{SIGNUM = "{ACM} {SIGNUM} Newslett."}
@string{AmerSocio = "American Journal of Sociology"}
@string{AmerStatAssoc = "Journal of the American Statistical Association"}
@string{AmerStatAssoc = "J. Amer. Statist. Assoc."}
@string{ApplMathComp = "Applied Mathematics and Computation"}
@string{ApplMathComp = "Appl. Math. Comput."}
@string{AmerMathMonthly = "American Mathematical Monthly"}
@string{AmerMathMonthly = "Amer. Math. Monthly"}
@string{BIT = "{BIT}"}
@string{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology"}
@string{BritStatPsych = "Brit. J. Math. Statist. Psych."}
@string{CanMathBull = "Canadian Mathematical Bulletin"}
@string{CanMathBull = "Canad. Math. Bull."}
@string{CompApplMath = "Journal of Computational and Applied Mathematics"}
@string{CompApplMath = "J. Comput. Appl. Math."}
@string{CompPhys = "Journal of Computational Physics"}
@string{CompPhys = "J. Comput. Phys."}
@string{CompStruct = "Computers and Structures"}
@string{CompStruct = "Comput. \& Structures"}
@string{CompJour = "The Computer Journal"}
@string{CompJour = "Comput. J."}
@string{CompSysSci = "Journal of Computer and System Sciences"}
@string{CompSysSci = "J. Comput. System Sci."}
@string{Computing = "Computing"}
@string{ContempMath = "Contemporary Mathematics"}
@string{ContempMath = "Contemp. Math."}
@string{Crelle = "Crelle's Journal"}
@string{GiornaleMath = "Giornale di Mathematiche"}
@string{GiornaleMath = "Giorn. Mat."}
% didn't find in AMS MR., ibid.

 %IEEE
@string{Computer = "{IEEE} Computer"}
@string{IEEETransComp = "{IEEE} Transactions on Computers"}
@string{IEEETransComp = "{IEEE} Trans. Comput."}
@string{IEEETransAC = "{IEEE} Transactions on Automatic Control"}
@string{IEEETransAC = "{IEEE} Trans. Automat. Control"}
@string{IEEESpec = "{IEEE} Spectrum"}
% didn't find in AMS MR
@string{ProcIEEE = "Proceedings of the {IEEE}"}
@string{ProcIEEE = "Proc. {IEEE}"}
% didn't find in AMS MR
@string{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems"}
@string{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems"}
@string{IMANumerAna = "{IMA} Journal of Numerical Analysis"}
@string{IMANumerAna = "{IMA} J. Numer. Anal."}
@string{InfProcLet = "Information Processing Letters"}
@string{InfProcLet = "Inform. Process. Lett."}
@string{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications"}
@string{InstMathApp = "J. Inst. Math. Appl."}
@string{IntControl = "International Journal of Control"}
@string{IntControl = "Internat. J. Control"}
@string{IntNumerEng = "International Journal for Numerical Methods in
     Engineering"}
@string{IntNumerEng = "Internat. J. Numer. Methods Engrg."}
@string{IntSuper = "International Journal of Supercomputing Applications"}
@string{IntSuper = "Internat. J. Supercomputing Applic."}
% didn't find
%% in AMS MR
@string{Kibernetika = "Kibernetika"}
@string{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards"}
@string{JResNatBurStand = "J. Res. Nat. Bur. Standards"}
@string{LinAlgApp = "Linear Algebra and its Applications"}
@string{LinAlgApp = "Linear Algebra Appl."}
@string{MathAnaAppl = "Journal of Mathematical Analysis and Applications"}
@string{MathAnaAppl = "J. Math. Anal. Appl."}
@string{MathAnnalen = "Mathematische Annalen"}
@string{MathAnnalen = "Math. Ann."}
@string{MathPhys = "Journal of Mathematical Physics"}
@string{MathPhys = "J. Math. Phys."}
@string{MathComp = "Mathematics of Computation"}
@string{MathComp = "Math. Comp."}
@string{MathScand = "Mathematica Scandinavica"}
@string{MathScand = "Math. Scand."}
@string{TablesAidsComp = "Mathematical Tables and Other Aids to Computation"}
@string{TablesAidsComp = "Math. Tables Aids Comput."}
@string{NumerMath = "Numerische Mathematik"}
@string{NumerMath = "Numer. Math."}
@string{PacificMath = "Pacific Journal of Mathematics"}
@string{PacificMath = "Pacific J. Math."}
@string{ParDistComp = "Journal of Parallel and Distributed Computing"}
@string{ParDistComp = "J. Parallel and Distrib. Comput."}
% didn't find
%% in AMS MR
@string{ParComputing = "Parallel Computing"}
@string{ParComputing = "Parallel Comput."}
@string{PhilMag = "Philosophical Magazine"}
@string{PhilMag = "Philos. Mag."}
@string{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA"}
@string{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A."}
@string{Psychometrika = "Psychometrika"}
@string{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)"}
@string{QuartMath = "Quart. J. Math. Oxford Ser. (2)"}
@string{QuartApplMath = "Quarterly of Applied Mathematics"}
@string{QuartApplMath = "Quart. Appl. Math."}
@string{RevueInstStat = "Review of the International Statisical Institute"}
@string{RevueInstStat = "Rev. Inst. Internat. Statist."}
%SIAM
@string{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics"}
@string{JSIAM = "J. Soc. Indust. Appl. Math."}
@string{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis"}
@string{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal."}
@string{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods"}
@string{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods"}
@string{SIAMAppMath = "{SIAM} Journal on Applied Mathematics"}
@string{SIAMAppMath = "{SIAM} J. Appl. Math."}
@string{SIAMComp = "{SIAM} Journal on Computing"}
@string{SIAMComp = "{SIAM} J. Comput."}
@string{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications"}
@string{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl."}
@string{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis"}
@string{SIAMNumAnal = "{SIAM} J. Numer. Anal."}
@string{SIAMReview = "{SIAM} Review"}
@string{SIAMReview = "{SIAM} Rev."}
@string{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing"}
@string{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput."}
@string{SoftPracExp = "Software Practice and Experience"}
@string{SoftPracExp = "Software Prac. Experience"}
% didn't find in AMS MR
@string{StatScience = "Statistical Science"}
@string{StatScience = "Statist. Sci."}
@string{Techno = "Technometrics"}
@string{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics"}
@string{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys."}
@string{VLSICompSys = "Journal of {VLSI} and Computer Systems"}
@string{VLSICompSys = "J. {VLSI} Comput. Syst."}
@string{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik"}
@string{ZAngewMathMech = "Z. Angew. Math. Mech."}
@string{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik"}
@string{ZAngewMathPhys = "Z. Angew. Math. Phys."}
% Publishers % ================================================= |
@string{Academic = "Academic Press"}
@string{ACMPress = "{ACM} Press"}
@string{AdamHilger = "Adam Hilger"}
@string{AddisonWesley = "Addison-Wesley"}
@string{AllynBacon = "Allyn and Bacon"}
@string{AMS = "American Mathematical Society"}
@string{Birkhauser = "Birkha{\"u}ser"}
@string{CambridgePress = "Cambridge University Press"}
@string{Chelsea = "Chelsea"}
@string{ClaredonPress = "Claredon Press"}
@string{DoverPub = "Dover Publications"}
@string{Eyolles = "Eyolles"}
@string{HoltRinehartWinston = "Holt, Rinehart and Winston"}
@string{Interscience = "Interscience"}
@string{JohnsHopkinsPress = "The Johns Hopkins University Press"}
@string{JohnWileySons = "John Wiley and Sons"}
@string{Macmillan = "Macmillan"}
@string{MathWorks = "The Math Works Inc."}
@string{McGrawHill = "McGraw-Hill"}
@string{NatBurStd = "National Bureau of Standards"}
@string{NorthHolland = "North-Holland"}
@string{OxfordPress = "Oxford University Press"}
%address Oxford or London?
@string{PergamonPress = "Pergamon Press"}
@string{PlenumPress = "Plenum Press"}
@string{PrenticeHall = "Prentice-Hall"}
@string{SIAMPub = "{SIAM} Publications"}
@string{Springer = "Springer-Verlag"}
@string{TexasPress = "University of Texas Press"}
@string{VanNostrand = "Van Nostrand"}
@string{WHFreeman = "W. H. Freeman and Co."}
---
%Entries
@article{GarSchRosVioRezEslTeh18,
  title        = {Neural processes},
  author       = {Garnelo, Marta and
                  Schwarz, Jonathan and
                  Rosenbaum, Dan and
                  Viola, Fabio and
                  Rezende, Danilo~J and
                  Eslami, SM and
                  Teh, Yee Whye},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1807.01622}
}
@article{KehCheQua18,
  title        = {Interpretable Fairness via Target Labels in Gaussian Process Models},
  author       = {Kehrenberg, Thomas and
                  Chen, Zexun and
                  Quadrianto, Novi},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1810.05598}
}
@InProceedings{HeiLoiGumKra19,
  author       = {Heidari, Hoda and
                  Loi, Michele and
                  Gummadi, Krishna~P. and
                  Krause, Andreas},
  title        = {A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity},
  year         = {2019},
  isbn         = {9781450361255},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3287560.3287584},
  doi          = {10.1145/3287560.3287584},
  abstract     = {We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages        = {181–190},
  numpages     = {10},
  keywords     = {Statistical Parity, Equality of Opportunity (EOP), Rawlsian and Luck Egalitarian EOP, Equality of Odds, Predictive Value Parity, Fairness for Machine Learning},
  location     = {Atlanta, GA, USA},
  series       = {FAT* '19}
}
@InProceedings{BolChaZouSalKal16,
  title        = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author       = {Bolukbasi, Tolga and
                  Chang, Kai-Wei and
                  Zou, James~Y and
                  Saligrama, Venkatesh and
                  Kalai, Adam~T},
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems 29},
  pages        = {4349--4357},
  editor       = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@InProceedings{KleMulRag17,
  author       = {Jon Kleinberg and Sendhil Mullainathan and Manish Raghavan},
  title        = {Inherent Trade-Offs in the Fair Determination of Risk Scores},
  booktitle    = {8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  pages        = {43:1--43:23},
  series       = {Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN         = {978-3-95977-029-3},
  ISSN         = {1868-8969},
  year         = {2017},
  volume       = {67},
  editor       = {Christos H. Papadimitriou},
  publisher    = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address      = {Dagstuhl, Germany},
  URL          = {http://drops.dagstuhl.de/opus/volltexte/2017/8156},
  URN          = {urn:nbn:de:0030-drops-81560},
  doi          = {10.4230/LIPIcs.ITCS.2017.43},
  annote       = {Keywords: algorithmic fairness, risk tools, calibration}
}
@article{Cho17,
  author       = {Chouldechova, Alexandra},
  title        = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  journal      = {Big Data},
  volume       = {5},
  number       = {2},
  pages        = {153-163},
  year         = {2017},
  doi          = {10.1089/big.2016.0047},
  note         = {PMID: 28632438},
  URL          = {https://doi.org/10.1089/big.2016.0047},
  eprint       = {https://doi.org/10.1089/big.2016.0047},
  abstract     = {Abstract Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance.}
}
@InProceedings{RusKusLofSil17,
  title        = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},
  author       = {Russell, Chris and
                  Kusner, Matt~J and
                  Loftus, Joshua and
                  Silva, Ricardo},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems 30},
  pages        = {6414--6423},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{ChoRot18,
  author       = {Chouldechova, Alexandra and
                  Roth, Aaron},
  title        = {The Frontiers of Fairness in Machine Learning},
  journal      = {CoRR},
  volume       = {abs/1810.08810},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.08810},
  archivePrefix = {arXiv},
  eprint       = {1810.08810},
  timestamp    = {Tue, 08 Oct 2019 14:34:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-08810.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{KamCal12,
  title        = {Data preprocessing techniques for classification without discrimination},
  author       = {Kamiran, Faisal and
                  Calders, Toon},
  year         = 2012,
  journal      = {Knowledge and Information Systems},
  publisher    = {Springer},
  volume       = 33,
  number       = 1,
  pages        = {1--33},
  doi          = {10.1007/s10115-011-0463-8},
  isbn         = 1011501104,
  issn         = {02191377},
  keywords     = {Classification, Discrimination-aware data mining, Preprocessing}
}
@InProceedings{AdeValGhaWel29,
  title        = {One-network adversarial fairness},
  author       = {Adel, Tameem and
                  Valera, Isabel and
                  Ghahramani, Zoubin and
                  Weller, Adrian},
  year         = 2019,
  booktitle    = {Thirty-Third AAAI Conference on Artificial Intelligence}
}
@article{SatHofCheVar18,
  author       = {Sattigeri, P. and
                  Hoffman, S.~C. and
                  Chenthamarakshan, V. and
                  Varshney, K.~R.},
  journal      = {IBM Journal of Research and Development},
  title        = {Fairness GAN: Generating datasets with fairness properties using a generative adversarial network},
  year         = {2019},
  volume       = {63},
  number       = {4/5},
  pages        = {3:1-3:9},
  doi          = {10.1147/JRD.2019.2945519}
}
@InProceedings{XuYuaZhaWu18,
  title        = {Fairgan: Fairness-aware generative adversarial networks},
  author       = {Xu, Depeng and
                  Yuan, Shuhan and
                  Zhang, Lu and
                  Wu, Xintao},
  year         = 2018,
  booktitle    = {2018 IEEE International Conference on Big Data (Big Data)},
  pages        = {570--575},
  organization = {IEEE}
}
@article{LouShiSchWel19,
  title        = {The Functional Neural Process},
  author       = {Louizos, Christos and
                  Shi, Xiahan and
                  Schutte, Klamer and
                  Welling, Max},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1906.08324}
}
@InProceedings{QuaShaTho19,
  title        = {Discovering fair representations in the data domain},
  author       = {Quadrianto, Novi and
                  Sharmanska, Viktoriia and
                  Thomas, Oliver},
  year         = 2019,
  booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages        = {8227--8236}
}
@InProceedings{KilBalKusWeletal19,
  title        = {The Sensitivity of Counterfactual Fairness to Unmeasured Confounding},
  author       = {Kilbertus, Niki and
                  Ball, Philip~J. and
                  Kusner, Matt~J. and
                  Weller, Adrian and
                  Silva, Ricardo},
  year         = 2019,
  booktitle    = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, {UAI}}
}
@misc{DuaGra17,
  author       = {Dua, Dheeru and
                  Graff, Casey},
  year         = 2017,
  title        = {{UCI} Machine Learning Repository},
  url          = {http://archive.ics.uci.edu/ml},
  institution  = {University of California, Irvine, School of Information and Computer Sciences}
}
@article{FouIslKeyPan20,
  title        = {An Intersectional Definition of Fairness},
  author       = {Foulds, James~R. and
                  Islam, Rashidul and
                  Keya, Kamrun~Naher and
                  Pan, Shimei},
  ISBN         = {9781728129037},
  url          = {http://dx.doi.org/10.1109/ICDE48307.2020.00203},
  DOI          = {10.1109/icde48307.2020.00203},
  journal      = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
  publisher    = {IEEE},
  year         = {2020},
  month        = apr
}
@misc{BarHar17,
  title        = {Fairness in Machine Learning},
  author       = {Barocas, S and
                  Hardt, M},
  journal      = {NIPS 2017},
  publisher    = {NIPS},
  url          = {https://nips.cc/Conferences/2017/Schedule?showEvent=8734}
}
@unpublished{CorGoe18,
  title        = {Defining and Designing Fair Algorithms},
  author       = {Corbett-Davies, S and
                  Goel, S},
  note         = {ICML 2018 Tutorial},
  url          = {https://icml.cc/Conferences/2018/Schedule?showEvent=1862}
}
@article{AngLarMatKir16,
  title        = {Machine bias},
  author       = {Angwin, Julia and
                  Larson, Jeff and
                  Mattu, Surya and
                  Kirchner, Lauren},
  journal      = {ProPublica, May},
  volume       = {23},
  number       = {2016},
  pages        = {139--159},
  year         = {2016}
}
@book{BarHarNar19,
  title        = {Fairness and Machine Learning},
  author       = {Barocas, Solon and
                  Hardt, Moritz and
                  Narayanan, Arvind},
  publisher    = {fairmlbook.org},
  note         = {\url{http://www.fairmlbook.org}},
  year         = {2019}
}
@misc{Vin17,
  title        = {DeepMind's AI became a superhuman chess player in a few hours, just for fun},
  author       = {Vincent, James},
  year         = 2017,
  month        = dec,
  journal      = {The Verge},
  publisher    = {The Verge},
  url          = {https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go}
}
@article{BroSan18,
  title        = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author       = {Brown, N. and
                  Sandholm, T.},
  year         = 2018,
  month        = jan,
  journal      = {Science},
  volume       = 359,
  pages        = {418--424},
  doi          = {10.1126/science.aao1733},
  adsurl       = {http://adsabs.harvard.edu/abs/2018Sci...359..418B},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{SilSchSim17,
  title        = {Mastering the game of Go without human knowledge},
  author       = {Silver, David and
                  Schrittwieser, Julian and
                  Simonyan, Karen and
                  Antonoglou, Ioannis and
                  Huang, Aja and
                  Guez, Arthur and
                  Hubert, Thomas and
                  Baker, Lucas and
                  Lai, Matthew and
                  Bolton, Adrian and
                  Chen, Yutian and
                  Lillicrap, Timothy and
                  Hui, Fan and
                  Sifre, Laurent and
                  van~den~Driessche, George and
                  Graepel, Thore and
                  Hassabis, Demis},
  year         = 2017,
  month        = 10,
  day          = 18,
  journal      = {Nature},
  publisher    = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
  volume       = 550,
  pages        = {354 EP  -},
  url          = {http://dx.doi.org/10.1038/nature24270},
  date         = {2017/10/18/online},
  date-added   = {2018-09-25 12:15:14 +0000},
  date-modified = {2018-09-25 12:15:14 +0000},
  l3           = {10.1038/nature24270; https://www.nature.com/articles/nature24270#supplementary-information},
  m3           = {Article},
  ty           = {JOUR},
  bdsk-url-1   = {http://dx.doi.org/10.1038/nature24270}
}
@article{SinGanSinSar16,
  title        = {Machine Learning for High-Throughput Stress Phenotyping in Plants},
  author       = {Singh, Arti and
                  Ganapathysubramanian, Baskar and
                  Singh, Asheesh~Kumar and
                  Sarkar, Soumik},
  year         = 2016,
  journal      = {Trends in Plant Science},
  volume       = 21,
  number       = 2,
  pages        = {110--124},
  doi          = {https://doi.org/10.1016/j.tplants.2015.10.015},
  issn         = {1360-1385},
  url          = {http://www.sciencedirect.com/science/article/pii/S1360138515002630},
  keywords     = {high-throughput phenotyping, machine learning, Imaging, plant breeding, biotic stress, abiotic stress},
  abstract     = {Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits.}
}
@misc{Ideal,
  title        = {AI For Recruiting Software | High-Volume Hiring | Maximize Quality of Hire},
  journal      = {Ideal},
  url          = {https://ideal.com/}
}
@book{Mol18,
  title        = {Interpretable Machine Learning},
  author       = {Christoph Molnar},
  year         = 2018,
  publisher    = {https://christophm.github.io/interpretable-ml-book/}
}
@misc{USCBQ,
  title        = {U.S. Census Bureau QuickFacts: UNITED STATES},
  journal      = {Census Bureau QuickFacts},
  url          = {https://www.census.gov/quickfacts/fact/table/US/PST045217}
}
@misc{USNCES,
  title        = {Mobile Digest of Education Statistics, 2017},
  journal      = {National Center for Education Statistics (NCES) Home Page, a part of the U.S. Department of Education},
  publisher    = {National Center for Education Statistics},
  url          = {https://nces.ed.gov/programs/digest/mobile/Enrollment\%5FES\%5FEnrollment\%5Fby\%5FRace\%5Fand\%5FEthnicity.aspx}
}
@book{Pearl09,
  title        = {Causality: Models, Reasoning and Inference},
  author       = {Pearl, Judea},
  year         = 2009,
  publisher    = {Cambridge University Press},
  address      = {New York, NY, USA},
  isbn         = {052189560X, 9780521895606},
  edition      = {2nd}
}
@InProceedings{KamCal09,
  title        = {Classifying without discriminating},
  author       = {Kamiran, Faisal and
                  Calders, Toon},
  year         = 2009,
  booktitle    = {2009 2nd International Conference on Computer, Control and Communication, IC4 2009},
  doi          = {10.1109/IC4.2009.4909197},
  isbn         = 9781424433148
}
@techreport{kamiran2011,
  title        = {Discrimination-aware Classification},
  author       = {Kamiran, F.},
  year         = 2011,
  address      = {Eindhoven},
  pages        = 157,
  doi          = {10.6100/IR717576},
  isbn         = 9789038627892,
  url          = {https://research.tue.nl/en/publications/discrimination-aware-classification},
  institution  = {Technische Universiteit Eindhoven},
  keywords     = {Classifiers, Data Mining, Education, Industry, Labeling, Learning Systems, Processing}
}
@InProceedings{RibSinGue16,
  title        = {"Why should I trust you?" Explaining the predictions of any classifier},
  author       = {Ribeiro, Marco~Tulio and
                  Singh, Sameer and
                  Guestrin, Carlos},
  year         = 2016,
  booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages        = {1135--1144}
}
@article{FarKouThoSriGet18,
  author       = {Farnadi, Golnoosh and
                  Kouki, Pigi and
                  Thompson, Spencer~K. and
                  Srinivasan, Sriram and
                  Getoor, Lise},
  title        = {A Fairness-aware Hybrid Recommender System},
  journal      = {CoRR},
  volume       = {abs/1809.09030},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09030},
  archivePrefix= {arXiv},
  eprint       = {1809.09030},
  timestamp    = {Fri, 04 Sep 2020 14:32:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{WadVerPie18,
  author       = {Wadsworth, Christina and
                  Vera, Francesca and
                  Piech, Chris},
  title        = {Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction},
  journal      = {CoRR},
  volume       = {abs/1807.00199},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.00199},
  archivePrefix= {arXiv},
  eprint       = {1807.00199},
  timestamp    = {Mon, 13 Aug 2018 16:47:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-00199.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{ElaGol18,
  title        = {Adversarial Removal of Demographic Attributes from Text Data},
  author       = {Elazar, Yanai and
                  Goldberg, Yoav},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  month        = oct,
  year         = 2018,
  address      = {Brussels, Belgium},
  publisher    = {Association for Computational Linguistics},
  url          = {https://www.aclweb.org/anthology/D18-1002},
  doi          = {10.18653/v1/D18-1002},
  pages        = {11--21},
  abstract     = {Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in{---}and can be recovered from{---}the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to{---}and likely condition on{---}demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.},
}
@article{Dia14,
  title        = {Algorithmic accountability reporting: On the investigation of black boxes},
  author       = {Diakopoulos, Nicholas},
  year         = 2014,
  journal      = {NA}
}
@article{BurSonOrd08,
  title        = {Balanced Neighborhoods for Multi-sided Fairness in Recommendation},
  author       = {Burke, Robin and
                  Sonboli, Nasim and
                  Ordonez-Gauger, Aldo},
  year         = 2018,
  journal      = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  volume       = 81,
  number       = 2008,
  pages        = {202--214},
  url          = {http://proceedings.mlr.press/v81/burke18a.html},
  keywords     = {fair-, multi-sided platform, ness, recommender systems, sparse linear}
}
@InProceedings{YaoHua17,
  author       = {Yao, Sirui and
                  Huang, Bert},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages        = {},
  publisher    = {Curran Associates, Inc.},
  title        = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
  url          = {https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
  volume       = {30},
  year         = {2017}
}
@article{MunSmiPat16,
  title        = {Big Data : A Report on Algorithmic Systems , Opportunity , and Civil Rights Big Data : A Report on Algorithmic Systems , Opportunity , and Civil Rights},
  author       = {Munoz, Cecilia and
                  Smith, Megan and
                  Patil, DJ},
  year         = 2016,
  journal      = {Executive Office of the President of USA},
  number       = {May},
  doi          = {10.1177/0162243915598056},
  isbn         = 9788578110796,
  issn         = {0162-2439},
  url          = {https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016\%5F0504\%5Fdata\%5Fdiscrimination.pdf},
  pmid         = 25246403,
  arxivid      = {arXiv:1011.1669v3}
}
@InProceedings{FelFriMoeSchVen15,
  author       = {Feldman, Michael and
                  Friedler, Sorelle A. and
                  Moeller, John and
                  Scheidegger, Carlos and
                  Venkatasubramanian, Suresh},
  title        = {Certifying and Removing Disparate Impact},
  year         = {2015},
  isbn         = {9781450336642},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/2783258.2783311},
  doi          = {10.1145/2783258.2783311},
  abstract     = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
  booktitle    = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages        = {259–268},
  numpages     = {10},
  keywords     = {fairness, disparate impact, machine learning},
  location     = {Sydney, NSW, Australia},
  series       = {KDD '15}
}
@article{Hwa18,
  author       = {Tim Hwang},
  title        = {Computational Power and the Social Impact of Artificial Intelligence},
  journal      = {CoRR},
  volume       = {abs/1803.08971},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.08971},
  archivePrefix= {arXiv},
  eprint       = {1803.08971},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-08971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  note         = {Available at SSRN 3147971}
}
@article{Swe13,
  author       = {Sweeney, Latanya},
  title        = {Discrimination in Online Ad Delivery},
  year         = {2013},
  issue_date   = {May 2013},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = {56},
  number       = {5},
  issn         = {0001-0782},
  url          = {https://doi.org/10.1145/2447976.2447990},
  doi          = {10.1145/2447976.2447990},
  abstract     = {Google ads, black names and white names, racial discrimination, and click advertising.},
  journal      = {Commun. ACM},
  month        = may,
  pages        = {44–54},
  numpages     = {11}
}
@article{HinCooMamDee18,
  author       = {Hinnefeld, J.~Henry and
                  Cooman, Peter and
                  Mammo, Nat and
                  Deese, Rupert},
  title        = {Evaluating Fairness Metrics in the Presence of Dataset Bias},
  journal      = {CoRR},
  volume       = {abs/1809.09245},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09245},
  archivePrefix= {arXiv},
  eprint       = {1809.09245},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09245.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{ZehBonCasHajMegBae17,
  author       = {Zehlike, Meike and
                  Bonchi, Francesco and
                  Castillo, Carlos and
                  Hajian, Sara and
                  Megahed, Mohamed and
                  Baeza-Yates, Ricardo},
  title        = {FA*IR: A Fair Top-k Ranking Algorithm},
  year         = {2017},
  isbn         = {9781450349185},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3132847.3132938},
  doi          = {10.1145/3132847.3132938},
  abstract     = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
  booktitle    = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages        = {1569–1578},
  numpages     = {10},
  keywords     = {ranking, top-k selection, bias in computer systems, algorithmic fairness},
  location     = {Singapore, Singapore},
  series       = {CIKM '17}
}
@InProceedings{ElzJabJunKeaNeeRotSch19,
  author       = {Elzayn, Hadi and
                  Jabbari, Shahin and
                  Jung, Christopher and
                  Kearns, Michael and
                  Neel, Seth and
                  Roth, Aaron and
                  Schutzman, Zachary},
  title        = {Fair Algorithms for Learning in Allocation Problems},
  year         = {2019},
  isbn         = {9781450361255},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3287560.3287571},
  doi          = {10.1145/3287560.3287571},
  abstract     = {Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages        = {170–179},
  numpages     = {10},
  keywords     = {online learning, censored feedback, algorithmic fairness, resource allocation},
  location     = {Atlanta, GA, USA},
  series       = {FAT* '19}
}
@article{ZafValGomKri19,
  author       = {Zafar, Muhammad~Bilal  and
                  Valera, Isabel and
                  Gomez-Rodriguez, Manuel and
                  Krishna~P. Gummadi},
  title        = {Fairness Constraints: A Flexible Approach for Fair Classification},
  journal      = {Journal of Machine Learning Research},
  year         = {2019},
  volume       = {20},
  number       = {75},
  pages        = {1-42},
  url          = {http://jmlr.org/papers/v20/18-262.html}
}
@InProceedings{DwoHarPitReiZem12,
  author       = {Dwork, Cynthia and
                  Hardt, Moritz and
                  Pitassi, Toniann and
                  Reingold, Omer and
                  Zemel, Richard},
  title        = {Fairness through Awareness},
  year         = {2012},
  isbn         = {9781450311151},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/2090236.2090255},
  doi          = {10.1145/2090236.2090255},
  abstract     = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  booktitle    = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  pages        = {214–226},
  numpages     = {13},
  location     = {Cambridge, Massachusetts},
  series       = {ITCS '12}
}
@InProceedings{AdaAdaGreJedKacMic18,
  author       = {Adamski, Igor and
                  Adamski, Robert and
                  Grel, Tomasz and
                  J{\k{e}}drych, Adam and
                  Kaczmarek, Kamil and
                  Michalewski, Henryk},
  editor       = {Yokota, Rio and
                  Weiland, Mich{\`e}le and
                  Keyes, David and
                  Trinitis, Carsten},
  title        = {Distributed Deep Reinforcement Learning: Learn How to Play Atari Games in 21Â minutes},
  booktitle    = {High Performance Computing},
  year         = 2018,
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {370--388},
  abstract     = {We present a study in Distributed Deep Reinforcement Learning (DDRL) focused on scalability of a state-of-the-art Deep Reinforcement Learning algorithm known as Batch Asynchronous Advantage Actor-Critic (BA3C). We show that using the Adam optimization algorithm with a batch size of upÂ to 2048 is a viable choice for carrying out large scale machine learning computations. This, combined with careful reexamination of the optimizer's hyperparameters, using synchronous training on the node level (while keeping the local, single node part of the algorithm asynchronous) and minimizing the model's memory footprint, allowed us to achieve linear scaling for upÂ to 64 CPU nodes. This corresponds to a training time of 21Â min on 768 CPU cores, as opposed to the 10Â h required when using a single node with 24 cores achieved by a baseline single-node implementation.},
  isbn         = {978-3-319-92040-5}
}
@article{LuoRugTur11,
  title        = {k-NN as an implementation of situation testing for discrimination discovery and prevention},
  author       = {Luong, Binh Thanh and
                  Ruggieri, Salvatore and
                  Turini, Franco},
  year         = 2011,
  journal      = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
  pages        = 502,
  doi          = {10.1145/2020408.2020488},
  isbn         = 9781450308137,
  url          = {http://dl.acm.org/citation.cfm?doid=2020408.2020488},
  keywords     = {discrimination discovery and prevention, k-nn classification}
}
@InProceedings{MadCrePitZem18,
  title        = {Learning Adversarially Fair and Transferable Representations},
  author       = {Madras, David and
                  Creager, Elliot and
                  Pitassi, Toniann and
                  Zemel, Richard},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  pages        = {3384--3393},
  year         = {2018},
  editor       = {Dy, Jennifer and Krause, Andreas},
  volume       = {80},
  series       = {Proceedings of Machine Learning Research},
  month        = {10--15 Jul},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v80/madras18a/madras18a.pdf},
  url          = {http://proceedings.mlr.press/v80/madras18a.html},
  abstract     = {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.}
}
@article{HolLiuVo16,
  title        = {Machine learning applied to weather forecasting},
  author       = {Holmstrom, Mark and
                  Liu, Dylan and
                  Vo, Christopher},
  year         = 2016,
  journal      = {Stanford University},
  pages        = {2--4}
}
@InProceedings{ZhaLemMit18,
  author       = {Zhang, Brian~Hu and
                  Lemoine, Blake and
                  Mitchell, Margaret},
  title        = {Mitigating Unwanted Biases with Adversarial Learning},
  year         = {2018},
  isbn         = {9781450360128},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3278721.3278779},
  doi          = {10.1145/3278721.3278779},
  abstract     = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
  booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages        = {335–340},
  numpages     = {6},
  keywords     = {unbiasing, multi-task learning, adversarial learning, debiasing},
  location     = {New Orleans, LA, USA},
  series       = {AIES '18}
}
@article{GupCotFarWan18,
  author       = {Gupta, Maya~R. and
                  Cotter, Andrew and
                  Fard, Mahdi~Milani and
                  Wang, Serena},
  title        = {Proxy Fairness},
  journal      = {CoRR},
  volume       = {abs/1806.11212},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.11212},
  archivePrefix= {arXiv},
  eprint       = {1806.11212},
  timestamp    = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1806-11212.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{QuaSha17,
  author       = {Quadrianto, Novi and
                  Sharmanska, Viktoriia},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher    = {Curran Associates, Inc.},
  title        = {Recycling Privileged Learning and Distribution Matching for Fairness},
  url          = {https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
  volume       = {30},
  year         = {2017}
}
@article{AnaCra18,
  title        = {Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability},
  author       = {Ananny, Mike and
                  Crawford, Kate},
  year         = 2018,
  journal      = {new media \& society},
  publisher    = {SAGE Publications Sage UK: London, England},
  volume       = 20,
  number       = 3,
  pages        = {973--989}
}
@article{HOL18,
  title        = {SELECT COMMITTEE ON ARTIFICIAL INTELLIGENCE},
  author       = {HOUSE, OF LORDS},
  year         = 2018,
  journal      = {AI in the UK: ready, willing and able}
}
@article{CitPas14,
  title        = {The scored society: Due process for automated predictions},
  author       = {Citron, Danielle Keats and
                  Pasquale, Frank},
  year         = 2014,
  journal      = {Wash. L. Rev.},
  publisher    = {HeinOnline},
  volume       = 89,
  pages        = 1
}
@book{Roth88,
  title        = {The Shapley value: essays in honor of Lloyd S. Shapley},
  author       = {Roth, Alvin E},
  year         = 1988,
  publisher    = {Cambridge University Press}
}
@article{WacMitFlo17,
  title        = {Transparent, explainable, and accountable AI for robotics},
  author       = {Wachter, Sandra and
                  Mittelstadt, Brent and
                  Floridi, Luciano},
  year         = 2017,
  journal      = {NA}
}
@article{DeDeo14,
  author       = {DeDeo, Simon},
  title        = {"Wrong side of the tracks": Big Data and Protected Categories},
  journal      = {CoRR},
  volume       = {abs/1412.4643},
  year         = {2014},
  url          = {http://arxiv.org/abs/1412.4643},
  archivePrefix= {arXiv},
  eprint       = {1412.4643},
  timestamp    = {Mon, 13 Aug 2018 16:47:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/DeDeo14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{Hill20,
  title        = {Wrongfully Accused by an Algorithm},
  author       = {Hill, Kashmir},
  year         = 2020,
  month        = jun,
  journal      = {The New York Times},
  publisher    = {The New York Times},
  url          = {https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html}
}
@misc{Reu18,
  title        = {Amazon ditched AI recruiting tool that favored men for technical jobs},
  url          = {https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine},
  journal      = {The Guardian},
  publisher    = {Guardian News and Media},
  author       = {Reuters},
  year         = {2018},
  month        = {Oct}
}
@misc{BBC19,
  title        = {Police officers raise concerns about 'biased' AI data},
  url          = {https://www.bbc.co.uk/news/technology-49717378},
  journal      = {BBC News},
  publisher    = {BBC},
  year         = {2019},
  month        = {Sep}
}
@misc{CisKoy19,
  title        = {Representation Learning and Fairness},
  url          = {https://neurips.cc/Conferences/2019/Schedule?showEvent=13212},
  author       = {Cisse, Moustapha and
                  Koyejo, Sanmi},
  note         = {NeurIPS 2019 Tutorial},
}
@article{FriSchVen16,
  title        = {On the (im)possibility of fairness},
  author       = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year         = 2016,
  journal      = {CoRR},
  volume       = {abs/1609.07236},
  url          = {http://arxiv.org/abs/1609.07236},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/FriedlerSV16},
  timestamp    = {Mon, 03 Oct 2016 17:51:10 +0200}
}
@InProceedings{HarPriSre16,
  title        = {Equality of Opportunity in Supervised Learning},
  author       = {Hardt, Moritz and Price, Eric and Srebro, Nati},
  year         = 2016,
  booktitle    = {Advances in Neural Information Processing Systems 29},
  publisher    = {Curran Associates, Inc.},
  pages        = {3315--3323},
  url          = {http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf},
  editor       = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@incollection{KusLofRusSil17,
  title        = {Counterfactual Fairness},
  author       = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems 30},
  publisher    = {Curran Associates, Inc.},
  pages        = {4066–4076},
  url          = {http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf},
  editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.}
}
@incollection{WicPanTri19,
  title        = {Unlocking Fairness: a Trade-off Revisited},
  author       = {Wick, Michael and Panda, Swetasudha and Tristan, Jean-Baptiste},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32},
  publisher    = {Curran Associates, Inc.},
  pages        = {8783--8792},
  url          = {http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@InProceedings{KehBarThoQua20,
  author       = {Kehrenberg, Thomas and
                  Bartlett, Myles and
                  Thomas, Oliver and
                  Quadrianto, Novi},
  editor       = {Vedaldi, Andrea and
                  Bischof, Horst and
                  Brox, Thomas and
                  Frahm, Jan-Michael},
  title        = {Null-Sampling for Interpretable and Fair Representations},
  booktitle    = {Computer Vision -- ECCV 2020},
  year         = {2020},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {565--580},
  abstract     = {We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. Invariance implies a selectivity for high level, relevant correlations w.r.t. class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished. To address this problem, we introduce an adversarially trained model with a null-sampling procedure to produce invariant representations in the data domain. To enable disentanglement, a partially-labelled representative set is used. By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors. We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset. (The code can be found at https://github.com/predictive-analytics-lab/nifr).},
  isbn         = {978-3-030-58574-7}
}
@InProceedings{NabShp18,
  title        = {Fair Inference on Outcomes},
  author       = {Nabi, Razieh and Shpitser, Ilya},
  year         = 2018,
  booktitle    = {Proceedings of the Thirty Second Conference on Association for the Advancement of Artificial Intelligence (AAAI-32nd)},
  publisher    = {AAAI Press}
}
@InProceedings{Chi19,
  title        = {Path-specific counterfactual fairness},
  author       = {Chiappa, Silvia},
  year         = 2019,
  booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = 33,
  pages        = {7801--7808}
}
@article{BeuCheZhaChi17,
  title        = {Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations},
  author       = {Beutel, Alex and Chen, Jilin and Zhao, Zhe and Chi, Ed H.},
  year         = 2017,
  journal      = {CoRR},
  volume       = {abs/1707.00075},
  url          = {http://arxiv.org/abs/1707.00075},
  archiveprefix= {arXiv},
  eprint       = {1707.00075},
  timestamp    = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BeutelCZC17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{MadCrePitZem18b,
  title        = {Fairness Through Causal Awareness: Learning Latent-Variable Models for Biased Data},
  author       = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard S.},
  year         = 2018,
  journal      = {CoRR},
  volume       = {abs/1809.02519},
  url          = {http://arxiv.org/abs/1809.02519},
  archiveprefix= {arXiv},
  eprint       = {1809.02519},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-02519.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{LiuDeaRolSimetal18,
  title        = {Delayed Impact of Fair Machine Learning},
  author       = {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {StockholmsmÃ€ssan, Stockholm Sweden},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {3150--3158},
  url          = {http://proceedings.mlr.press/v80/liu18c.html},
  editor       = {Jennifer Dy and Andreas Krause},
  pdf          = {http://proceedings.mlr.press/v80/liu18c/liu18c.pdf},
  abstract     = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.}
}
@InProceedings{ZemWuSwePitDwo13,
  title        = {Learning Fair Representations},
  author       = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  pages        = {325--333},
  url          = {http://proceedings.mlr.press/v28/zemel13.html},
  editor       = {Sanjoy Dasgupta and David McAllester},
  pdf          = {http://proceedings.mlr.press/v28/zemel13.pdf},
  abstract     = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}
@InProceedings{EdwSto16,
  author       = {Edwards, Harrison and
                  Storkey, Amos~J. },
  editor       = {Bengio, Yoshua and
                  LeCun, Yann},
  title        = {Censoring Representations with an Adversary},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.05897},
  timestamp    = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/EdwardsS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{GanUstAjaGerLarLavMarLem16,
  title        = {Domain-Adversarial Training of Neural Networks},
  author       = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and March, Mario and Lempitsky, Victor},
  year         = 2016,
  journal      = {Journal of Machine Learning Research},
  volume       = 17,
  number       = 59,
  pages        = {1--35},
  url          = {http://jmlr.org/papers/v17/15-239.html}
}
@InProceedings{LouSweLiWelZem15,
  title        = {The Variational Fair Autoencoder},
  author       = {Louizos, Christos and
                  Swersky, Kevin and
                  Li, Yujia and
                  Welling, Max and
                  Zemel, Richard~S.},
  year         = {2016},
  cdate        = {1451606400000},
  url          = {http://arxiv.org/abs/1511.00830},
  booktitle    = {ICLR},
  crossref     = {conf/iclr/2016}
}
@InProceedings{GreHorRasSchSmo07,
  title        = {A Kernel Approach to Comparing Distributions},
  author       = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  year         = 2007,
  month        = jul,
  journal      = {Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-07)},
  booktitle    = {Proceedings of the 22. AAAI Conference on Artificial Intelligence},
  publisher    = {AAAI Press},
  address      = {Menlo Park, CA, USA},
  pages        = {1637--1641},
  organization = {Max-Planck-Gesellschaft},
  institution  = {Association for the Advancement of Artificial Intelligence},
  school       = {Biologische Kybernetik},
  month_numeric = 7
}
@article{ChoRot20,
  title        = {A snapshot of the frontiers of fairness in machine learning},
  author       = {Alexandra Chouldechova and Aaron Roth},
  year         = 2020,
  journal      = {Commun. {ACM}},
  volume       = 63,
  number       = 5,
  pages        = {82--89}
}
@article{BarSel16,
  title        = {Big data's disparate impact},
  author       = {Barocas, S. and Selbst, A.},
  year         = 2016,
  journal      = {California Law Review},
  volume       = 104,
  number       = 3,
  pages        = {671--732}
}
@InProceedings{BarCraShaWal17,
  title        = {The problem with bias: from allocative to representational harms in machine learning},
  author       = {Barocas, Solon and Crawford, Kate and Shapiro, Aaron and Wallach, Hanna},
  year         = 2017,
  booktitle    = {Special Interest Group for Computing, Information and Society (SIGCIS)}
}
@InProceedings{KamAkaAso2012,
  title        = {Fairness-aware classifier with prejudice remover regularizer},
  author       = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  year         = 2012,
  booktitle    = {European conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  pages        = {35--50}
}
@article{CalVer10,
  title        = {Three naive Bayes approaches for discrimination-free classification},
  author       = {Calders, Toon and Verwer, Sicco},
  year         = 2010,
  month        = sep,
  day          = {01},
  journal      = {Data Mining and Knowledge Discovery},
  volume       = 21,
  number       = 2,
  pages        = {277--292},
  doi          = {10.1007/s10618-010-0190-x},
  issn         = {1573-756X},
  url          = {https://doi.org/10.1007/s10618-010-0190-x},
  abstract     = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.}
}
@InProceedings{JiaNac20,
  title        = {Identifying and Correcting Label Bias in Machine Learning},
  author       = {Jiang, Heinrich and Nachum, Ofir},
  year         = 2020,
  booktitle    = {The 23rd International Conference on Artificial Intelligence and Statistics, {AISTATS} 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 108,
  pages        = {702--712},
  editor       = {Chiappa, Silvia and Calandra, Roberto}
}
@InProceedings{KalZho18,
  title        = {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
  author       = {Kallus, Nathan and Zhou, Angela},
  year         = 2018,
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {2444--2453},
  editor       = {Dy, Jennifer G. and Krause, Andreas}
}
@InProceedings{PedRugTur08,
  title        = {Discrimination-aware data mining},
  author       = {Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  year         = 2008,
  booktitle    = {Proceedings of the 14th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008},
  publisher    = {ACM},
  pages        = {560--568},
  editor       = {Li, Ying and Liu, Bing and Sarawagi, Sunita},
  doi          = {10.1145/1401890.1401959},
  isbn         = 9781605581934
}
@InProceedings{AgaBeyDudLanetal18,
  title        = {A Reductions Approach to Fair Classification},
  author       = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  year         = 2018,
  month        = {10--15 Jul},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Stockholmsmässan, Stockholm Sweden},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {60--69},
  url          = {http://proceedings.mlr.press/v80/agarwal18a.html},
  editor       = {Jennifer Dy and Andreas Krause},
  pdf          = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  abstract     = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.}
}
@article{Rubin90,
  title        = {Formal mode of statistical inference for causal effects},
  author       = {Rubin, Donald B},
  year         = 1990,
  journal      = {Journal of statistical planning and inference},
  publisher    = {Elsevier},
  volume       = 25,
  number       = 3,
  pages        = {279--292}
}
@article{Wachter20,
  title        = {Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI},
  author       = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year         = 2020,
  journal      = {SSRN Electronic Journal},
  publisher    = {Elsevier BV},
  doi          = {10.2139/ssrn.3547922},
  issn         = {1556-5068},
  url          = {http://dx.doi.org/10.2139/ssrn.3547922}
}
@InProceedings{MadPitZem18,
  title        = {Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
  author       = {Madras, David and Pitassi, Toni and Zemel, Richard},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 31,
  pages        = {6147--6157},
  url          = {https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf},
  editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@article{Miller19,
  title        = {Explanation in artificial intelligence: Insights from the social sciences},
  author       = {Tim Miller},
  year         = 2019,
  journal      = {Artificial Intelligence},
  volume       = 267,
  pages        = {1--38},
  doi          = {https://doi.org/10.1016/j.artint.2018.07.007},
  issn         = {0004-3702},
  url          = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
  keywords     = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
  abstract     = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}
@book{Hume00,
  title        = {An enquiry concerning human understanding: A critical edition},
  author       = {Hume, David},
  year         = 2000,
  publisher    = {Oxford University Press},
  volume       = 3
}
@InProceedings{BluSta20,
  title        = {Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?},
  author       = {Avrim Blum and Kevin Stangl},
  year         = 2020,
  booktitle    = {1st Symposium on Foundations of Responsible Computing, {FORC} 2020, June 1-3, 2020, Harvard University, Cambridge, MA, {USA} (virtual conference)},
  publisher    = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  series       = {LIPIcs},
  volume       = 156,
  pages        = {3:1--3:20},
  editor       = {Aaron Roth}
}
@InProceedings{YeoTsc21,
  title        = {Avoiding Disparity Amplification under Different Worldviews},
  author       = {Samuel Yeom and Michael Carl Tschantz},
  year         = 2021,
  booktitle    = {ACM Conference on Fairness, Accountability, and Transparency}
}
@article{KamZliCal13,
  title        = {Quantifying explainable discrimination and removing illegal discrimination in automated decision making},
  author       = {Faisal Kamiran and Indrė Žliobaitė and Toon Calders},
  year         = 2013,
  journal      = {Knowl Inf Syst},
  volume       = 35,
  url          = {https://doi.org/10.1007/s10115-012-0584-8}
}
@article{ShaHenDarQua20,
  title        = {Contrastive Examples for Addressing the Tyranny of the Majority},
  author       = {Viktoriia Sharmanska and Lisa Anne Hendricks and Trevor Darrell and Novi Quadrianto},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2004.06524}
}
@article{GoeGuLiRe20,
  title        = {Model Patching: Closing the Subgroup Performance Gap with Data Augmentation},
  author       = {Karan Goel and Albert Gu and Yixuan Li and Christopher R{\'{e}}},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2008.06775}
}
@article{RosRub83,
  title        = {The central role of the propensity score in observational studies for causal effects},
  author       = {Paul R Rosenbaum and Donald B Rubin},
  year         = 1983,
  journal      = {Biometrika},
  volume       = 70,
  number       = 1,
  pages        = {41--55}
}
@misc{FeiGogUhl21,
  title        = {Hiding Behind Machines: When Blame Is Shifted to Artificial Agents},
  author       = {Till Feier and Jan Gogoll and Matthias Uhl},
  year         = 2021,
  eprint       = {2101.11465},
  archiveprefix = {arXiv},
  primaryclass = {cs.CY}
}
@article{Tol19,
  title        = {Fair and Unbiased Algorithmic Decision Making: Current State and Future Challenges},
  author       = {Song{\"{u}}l Tolan},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1901.04730},
  url          = {http://arxiv.org/abs/1901.04730},
  archiveprefix = {arXiv},
  eprint       = {1901.04730},
  timestamp    = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-04730.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{FuHeHou2014,
  author       = {Fu, S. and
                  He, H. and
                  Hou, Z.},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {Learning Race from Face: A Survey},
  year         = {2014},
  volume       = {36},
  number       = {12},
  pages        = {2483-2509},
  doi          = {10.1109/TPAMI.2014.2321570}
}
@article{BruBurHan93,
  author       = {Bruce, Vicki and
                  Burton, A~Mike and
                  Hanna, Elias and
                  Healey, Pat and
                  Mason, Oli and
                  Coombes, Anne and
                  Fright, Rick and
                  Linney, Alf},
  title        = {Sex Discrimination: How Do We Tell the Difference between Male and Female Faces?},
  journal      = {Perception},
  volume       = {22},
  number       = {2},
  pages        = {131-152},
  year         = {1993},
  doi          = {10.1068/p220131},
  note         = {PMID: 8474840},
  URL          = {https://doi.org/10.1068/p220131},
  eprint       = {https://doi.org/10.1068/p220131},
  abstract     = {People are remarkably accurate (approaching ceiling) at deciding whether faces are male or female, even when cues from hairstyle, makeup, and facial hair are minimised. Experiments designed to explore the perceptual basis of our ability to categorise the sex of faces are reported. Subjects were considerably less accurate when asked to judge the sex of three-dimensional (3-D) representations of faces obtained by laser-scanning, compared with a condition where photographs were taken with hair concealed and eyes closed. This suggests that cues from features such as eyebrows, and skin texture, play an important role in decisionmaking. Performance with the laser-scanned heads remained quite high with 3/4-view faces, where the 3-D shape of the face should be easiest to see, suggesting that the 3-D structure of the face is a further source of information contributing to the classification of its sex. Performance at judging the sex from photographs (with hair concealed) was disrupted if the photographs were inverted, which implies that the superficial cues contributing to the decision are not processed in a purely ‘local’ way. Performance was also disrupted if the faces were shown in photographic negatives, which is consistent with the use of 3-D information, since negation probably operates by disrupting the computation of shape from shading. In 3-D, the ‘average’ male face differs from the ‘average’ female face by having a more protuberant nose/brow and more prominent chin/jaw. The effects of manipulating the shapes of the noses and chins of the laser-scanned heads were assessed and significant effects of such manipulations on the apparent masculinity or femininity of the heads were revealed. It appears that our ability to make this most basic of facial categorisations may be multiply determined by a combination of 2-D, 3-D, and textural cues and their interrelationships.}
}
@article{BroPer93,
  author       = {Brown, Elizabeth and
                  Perrett, David~I},
  title        = {What Gives a Face its Gender?},
  journal      = {Perception},
  volume       = {22},
  number       = {7},
  pages        = {829-840},
  year         = {1993},
  doi          = {10.1068/p220829},
  note         = {PMID: 8115240},
  URL          = {https://doi.org/10.1068/p220829},
  eprint       = {https://doi.org/10.1068/p220829},
  abstract     = { An experiment is reported in which the attribution of gender to isolated facial features and to faces whose features have been interchanged with those of a face of the opposite gender has been examined. Sixteen male faces were averaged to create a prototype male face and sixteen female faces averaged to create a prototype female face. The prototypes were then masked to exclude ears, neck, hair, and hairline. Individual features (brows, eyes, nose, mouth, and chin) and pairs of features (brows \& eyes, eyes \& nose, nose \& mouth, mouth \& chin) from the prototypes were then presented in isolation for classification according to their perceived gender. The results showed that, for the faces used, the brows \& eyes, brows alone, eyes alone, the whole jaw, the chin, the nose \& mouth, the mouth alone (in descending order), ie all the features except the nose, carried some information about gender when they were seen in isolation. In the second part of the experiment different features from one prototype face were grafted into the prototype of the opposite gender and the resulting composite faces classified by their perceived gender. The results of this feature substitution showed the jaw, brows \& eyes, chin, and brows (in descending order) effecting significant change in perceived gender. The difference in gender information carried by feature(s) when they were viewed in isolation from that when they were substituted for each other is attributed to the role of configuration in the perception of the gender of a face. }
}
@InProceedings{LevHas15,
  author       = {Levi, G. and
                  Hassncer, T.},
  booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title        = {Age and gender classification using convolutional neural networks},
  year         = {2015},
  pages        = {34-42},
  doi          = {10.1109/CVPRW.2015.7301352}
}
@InProceedings{IsoZhuZhoEfr17,
  author       = {Isola, Phillip and
                  Zhu, Jun-Yan and
                  Zhou, Tinghui and
                  Efros, Alexei~A.},
  title        = {Image-To-Image Translation With Conditional Adversarial Networks},
  booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month        = {July},
  year         = {2017}
}
@techreport{WEF18,
  title        = {How to prevent discriminatory outcomes in machine learning},
  author       = {{Global Future Council on Human Rights {2016-18}}},
  institution  = {World Economic Forum},
  series       = {White Paper},
  year         = {2018}
}
@article{MirOsi14,
  author       = {Mirza, Mehdi and
                  Osindero, Simon},
  title        = {Conditional Generative Adversarial Nets},
  journal      = {CoRR},
  volume       = {abs/1411.1784},
  year         = {2014},
  url          = {http://arxiv.org/abs/1411.1784},
  archivePrefix= {arXiv},
  eprint       = {1411.1784},
  timestamp    = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{ZhuParIsoEfr17,
  author       = {Zhu, J. and
                  Park, T. and
                  Isola, P. and
                  Efros, A.~A.},
  booktitle    = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title        = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
  year         = {2017},
  pages        = {2242-2251},
  doi          = {10.1109/ICCV.2017.244}
}
@InProceedings{ChoChoKimHa18,
  author       = {Choi, Yunjey and
                  Choi, Minje and
                  Kim, Munyoung and
                  Ha, Jung-Woo and
                  Kim, Sunghun and
                  Choo, Jaegul},
  title        = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
  booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month        = {June},
  year         = {2018}
}
@InProceedings{GreBouSmoSch05,
  author       = {Gretton, Arthur and
                  Bousquet, Olivier and
                  Smola, Alex and
                  Sch{\"o}lkopf, Bernhard},
  editor       = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji},
  title        = {Measuring Statistical Dependence with Hilbert-Schmidt Norms},
  booktitle    = {Algorithmic Learning Theory},
  year         = {2005},
  publisher    = {Springer Berlin Heidelberg},
  address      = {Berlin, Heidelberg},
  pages        = {63--77},
  abstract     = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.},
  isbn         = {978-3-540-31696-1}
}
@InProceedings{GrgRedGumWel18,
  author       = {Grgic-Hlaca, Nina and
                  Redmiles, Elissa~M. and
                  Gummadi, Krishna~P. and
                  Weller, Adrian},
  title        = {Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction},
  year         = {2018},
  isbn         = {9781450356398},
  publisher    = {International World Wide Web Conferences Steering Committee},
  address      = {Republic and Canton of Geneva, CHE},
  url          = {https://doi.org/10.1145/3178876.3186138},
  doi          = {10.1145/3178876.3186138},
  abstract     = {As algorithms are increasingly used to make important decisions that affect human lives, ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have been raised about the fairness of algorithmic decision making. Most prior works on algorithmic fairness normatively prescribe how fair decisions ought to be made. In contrast, here, we descriptively survey users for how they perceive and reason about fairness in algorithmic decision making. A key contribution of this work is the framework we propose to understand why people perceive certain features as fair or unfair to be used in algorithms. Our framework identifies eight properties of features, such as relevance, volitionality and reliability, as latent considerations that inform people»s moral judgments about the fairness of feature use in decision-making algorithms. We validate our framework through a series of scenario-based surveys with 576 people. We find that, based on a person»s assessment of the eight latent properties of a feature in our exemplar scenario, we can accurately (> 85%) predict if the person will judge the use of the feature as fair. Our findings have important implications. At a high-level, we show that people»s unfairness concerns are multi-dimensional and argue that future studies need to address unfairness concerns beyond discrimination. At a low-level, we find considerable disagreements in people»s fairness judgments. We identify root causes of the disagreements, and note possible pathways to resolve them.},
  booktitle    = {Proceedings of the 2018 World Wide Web Conference},
  pages        = {903–912},
  numpages     = {10},
  location     = {Lyon, France},
  series       = {WWW '18}
}
@InProceedings{LiuLuoWanTan15,
  author       = {Liu, Z. and
                  Luo, P. and
                  Wang, X. and
                  Tang, X.},
  booktitle    = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title        = {Deep Learning Face Attributes in the Wild},
  year         = {2015},
  pages        = {3730-3738},
  doi          = {10.1109/ICCV.2015.425}
}
@InProceedings{ZafValRodGum17b,
  author       = {Zafar, Muhammad~Bilal and
                  Valera, Isabel and
                  Gomez~Rodriguez, Manuel and
                  Gummadi, Krishna~P.},
  title        = {Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment},
  year         = {2017},
  isbn         = {9781450349130},
  publisher    = {International World Wide Web Conferences Steering Committee},
  address      = {Republic and Canton of Geneva, CHE},
  url          = {https://doi.org/10.1145/3038912.3052660},
  doi          = {10.1145/3038912.3052660},
  abstract     = {Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.},
  booktitle    = {Proceedings of the 26th International Conference on World Wide Web},
  pages        = {1171–1180},
  numpages     = {10},
  keywords     = {fair decision making, fair classification, machine learning and law, discrimination in decision making, algorithmic decision making},
  location     = {Perth, Australia},
  series       = {WWW '17}
}
@article{WacMitRus18,
  author       = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
  journal      = {Harvard Journal of Law \& Technology},
  title        = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  year         = {2018},
  volume       = {31},
  number       = {2}
}
@InProceedings{AgaBeyDudLanWal18,
  title        = {A Reductions Approach to Fair Classification},
  author       = {Agarwal, Alekh and
                  Beygelzimer, Alina and
                  Dudik, Miroslav and
                  Langford, John and
                  Wallach, Hanna},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  pages        = {60--69},
  year         = {2018},
  editor       = {Dy, Jennifer and Krause, Andreas},
  volume       = {80},
  series       = {Proceedings of Machine Learning Research},
  month        = {10--15 Jul},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  url          = {http://proceedings.mlr.press/v80/agarwal18a.html},
  abstract     = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.}
}
@InProceedings{HeZhaRenSun16,
  author       = {He, K. and
                  Zhang, X. and
                  Ren, S. and
                  Sun, J.},
  booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title        = {Deep Residual Learning for Image Recognition},
  year         = {2016},
  pages        = {770-778},
  doi          = {10.1109/CVPR.2016.90}
}
@InProceedings{MooJanPetSch09,
  author       = {Mooij, Joris and
                  Janzing, Dominik and
                  Peters, Jonas and
                  Sch\"{o}lkopf, Bernhard},
  title        = {Regression by Dependence Minimization and Its Application to Causal Inference in Additive Noise Models},
  year         = {2009},
  isbn         = {9781605585161},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/1553374.1553470},
  doi          = {10.1145/1553374.1553470},
  abstract     = {Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution of the noise, i.e., it is non-parametric with respect to the noise distribution. We argue that the proposed regression method is well suited to the task of causal inference in additive noise models. A practical disadvantage is that the resulting optimization problem is generally non-convex and can be difficult to solve. Nevertheless, we report good results on one of the tasks of the NIPS 2008 Causality Challenge, where the goal is to distinguish causes from effects in pairs of statistically dependent variables. In addition, we propose an algorithm for efficiently inferring causal models from observational data for more than two variables. The required number of regressions and independence tests is quadratic in the number of variables, which is a significant improvement over the simple method that tests all possible DAGs.},
  booktitle    = {Proceedings of the 26th Annual International Conference on Machine Learning},
  pages        = {745–752},
  numpages     = {8},
  location     = {Montreal, Quebec, Canada},
  series       = {ICML '09}
}
@article{GreBorRasSchSmo12,
  author       = {Gretton, Arthur and
                  Borgwardt, Karsten~M. and
                  Rasch, Malte~J. and
                  Sch{{\"o}}lkopf, Bernhard and
                  Smola, Alexander},
  title        = {A Kernel Two-Sample Test},
  journal      = {Journal of Machine Learning Research},
  year         = {2012},
  volume       = {13},
  number       = {25},
  pages        = {723-773},
  url          = {http://jmlr.org/papers/v13/gretton12a.html}
}
@article{SonSmoGreBedBor12,
  author       = {Song, Le and
                  Smola, Alex and
                  Gretton, Arthur and
                  Bedo, Justin and
                  Borgwardt, Karsten},
  title        = {Feature Selection via Dependence Maximization},
  journal      = {Journal of Machine Learning Research},
  year         = {2012},
  volume       = {13},
  number       = {47},
  pages        = {1393-1434},
  url          = {http://jmlr.org/papers/v13/song12a.html}
}
@inproceedings{NemShaBia02,
  author       = {Nemenman, Ilya and
                  Shafee, F. and
                  Bialek, William},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {T. Dietterich and S. Becker and Z. Ghahramani},
  publisher    = {MIT Press},
  title        = {Entropy and Inference, Revisited},
  url          = {https://proceedings.neurips.cc/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf},
  volume       = {14},
  year         = {2002}
}
@article{GatEckBat16,
  title        = {A Neural Algorithm of Artistic Style},
  volume       = {16},
  ISSN         = {1534-7362},
  url          = {http://dx.doi.org/10.1167/16.12.326},
  DOI          = {10.1167/16.12.326},
  number       = {12},
  journal      = {Journal of Vision},
  publisher    = {Association for Research in Vision and Ophthalmology (ARVO)},
  author       = {Gatys, Leon and
                  Ecker, Alexander and
                  Bethge, Matthias},
  year         = {2016},
  month        = {Sep},
  pages        = {326}
}



@article{Xiang21,
  author       = {Alice Xiang},
  title        = {Reconciling Legal and Technical Approaches to Algorithmic Bias},
  journal      = {Tennessee Law Review},
  volume       = {88},
  year         = {2021},
  url          = {https://ssrn.com/abstract=3650635},
}

@article{KehCheQua20,
  author       = {Kehrenberg, Thomas and
                  Chen, Zexun and
                  Quadrianto, Novi},
  title        = {Tuning Fairness by Balancing Target Labels},
  journal      = {Frontiers in Artificial Intelligence},
  volume       = {3},
  pages        = {33},
  year         = {2020},
  doi          = {10.3389/frai.2020.00033},
  issn         = {2624-8212},
  url          = { https://www.frontiersin.org/article/10.3389/frai.2020.00033 },
  abstract     = {The issue of fairness in machine learning models has recently attracted a lot of attention as ensuring it will ensure continued confidence of the general public in the deployment of machine learning systems. We focus on mitigating the harm incurred by a biased machine learning system that offers better outputs (e.g., loans, job interviews) for certain groups than for others. We show that bias in the output can naturally be controlled in probabilistic models by introducing a latent target output. This formulation has several advantages: first, it is a unified framework for several notions of group fairness such as Demographic Parity and Equality of Opportunity; second, it is expressed as a marginalization instead of a constrained problem; and third, it allows the encoding of our knowledge of what unbiased outputs should be. Practically, the second allows us to avoid unstable constrained optimization procedures and to reuse off-the-shelf toolboxes. The latter translates to the ability to control the level of fairness by directly varying fairness target rates. In contrast, existing approaches rely on intermediate, arguably unintuitive, control parameters such as covariance thresholds.}
}

@article{SimBhaWel21,
  author       = {Simons, Joshua and
                  Bhatti, Sophia~Adams and
                  Weller, Adrian},
  title        = {Machine Learning and the Meaning of Equal Treatment},
  journal      = {AIES},
  year         = {2021},
}

@InProceedings{ShaJohSon17,
  title        = {Estimating individual treatment effect: generalization bounds and algorithms},
  author       = {Shalit, Uri and
                  Johansson, Fredrik~D. and
                  Sontag, David},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
  pages        = {3076--3085},
  year         = {2017},
  editor       = {Precup, Doina and
                  Teh, Yee Whye},
  volume       = {70},
  series       = {Proceedings of Machine Learning Research},
  month        = {06--11 Aug},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf},
  url          = { http://proceedings.mlr.press/v70/shalit17a.html },
  abstract     = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.}
}

@misc{ParChuChaLeeShi21,
  title        = {Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts},
  author       = {Park, Song and
                  Chun, Sanghyuk and
                  Cha, Junbum and
                  Lee, Bado and
                  Shim, Hyunjung},
  year         = {2021},
  eprint       = {2104.00887},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@article{Ben19,
  title        = {Is algorithmic affirmative action legal},
  author       = {Bent, Jason R},
  journal      = {THE GEORGETOWN LAW JOURNAL},
  volume       = {108},
  pages        = {803},
  year         = {2019},
  publisher    = {HeinOnline}
}


@article{HarWal19,
  title        = {Stretching Human Laws to Apply to Machines: The Dangers of a ``Colorblind'' Computer},
  author       = {Harned, Zach and
                  Wallach, Hanna},
  journal      = {Florida State Univeristy Law Review},
  volume       = {47},
  pages        = {617},
  year         = {2019},
  publisher    = {HeinOnline}
}


@InProceedings{MozSon20,
  title        = {Consistent Estimators for Learning to Defer to an Expert},
  author       = {Mozannar, Hussein and
                  Sontag, David},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
  pages        = {7076--7087},
  year         = {2020},
  editor       = {III, Hal~Daumé and
                  Singh, Aarti},
  volume       = {119},
  series       = {Proceedings of Machine Learning Research},
  month        = {13--18 Jul},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v119/mozannar20b/mozannar20b.pdf},
  url          = {http://proceedings.mlr.press/v119/mozannar20b.html},
  abstract     = {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert’s decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.}
}

@article{JosKoyVijKimGho19,
  author       = {Joshi, Shalmali and
                  Koyejo, Oluwasanmi and
                  Vijitbenjaronk, Warut and
                  Kim, Been and
                  Ghosh, Joydeep},
  title        = {Towards Realistic Individual Recourse and Actionable Explanations
                  in Black-Box Decision Making Systems},
  journal      = {CoRR},
  volume       = {abs/1907.09615},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.09615},
  archivePrefix = {arXiv},
  eprint       = {1907.09615},
  timestamp    = {Tue, 30 Jul 2019 12:52:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-09615.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{UstSpaLiu19,
  author       = {Ustun, Berk and
                  Spangher, Alexander and
                  Liu, Yang},
  title        = {Actionable Recourse in Linear Classification},
  year         = {2019},
  isbn         = {9781450361255},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3287560.3287566},
  doi          = {10.1145/3287560.3287566},
  abstract     = {Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages        = {10–19},
  numpages     = {10},
  keywords     = {integer programming, audit, credit scoring, accountability, classification, recourse},
  location     = {Atlanta, GA, USA},
  series       = {FAT* '19}
}

@inproceedings{KarSchVal21,
  author       = {Karimi, Amir-Hossein and
                  Sch\"{o}lkopf, Bernhard and
                  Valera, Isabel},
  title        = {Algorithmic Recourse: From Counterfactual Explanations to Interventions},
  year         = {2021},
  isbn         = {9781450383097},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3442188.3445899},
  doi          = {10.1145/3442188.3445899},
  abstract     = {As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.},
  booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages        = {353–362},
  numpages     = {10},
  keywords     = {minimal interventions, algorithmic recourse, counterfactual explanations, causal inference, consequential recommendations, explainable artificial intelligence, contrastive explanations},
  location     = {Virtual Event, Canada},
  series       = {FAccT '21}
}

@inproceedings{BlaYeoFre20,
  author       = {Black, Emily and
                  Yeom, Samuel and
                  Fredrikson, Matt},
  title        = {FlipTest: Fairness Testing via Optimal Transport},
  year         = {2020},
  isbn         = {9781450369367},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  url          = {https://doi.org/10.1145/3351095.3372845},
  doi          = {10.1145/3351095.3372845},
  abstract     = {We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.},
  booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages        = {111–121},
  numpages     = {11},
  keywords     = {fairness, disparate impact, optimal transport, machine learning},
  location     = {Barcelona, Spain},
  series       = {FAT* '20}
}

@article{GebMorVecWorWalDauCra18,
  author       = {Gebru, Timnit and
                  Morgenstern, Jamie and
                  Vecchione, Briana and
                  Wortman~Vaughan, Jennifer and
                  Wallach, Hanna~M. and
                  Daum{\'{e}}~III, Hal and
                  Crawford, Kate},
  title        = {Datasheets for Datasets},
  journal      = {CoRR},
  volume       = {abs/1803.09010},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.09010},
  archivePrefix = {arXiv},
  eprint       = {1803.09010},
  timestamp    = {Mon, 20 Aug 2018 15:16:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-09010.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{KeaNeeRotWe18,
  title        = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author       = {Kearns, Michael and
                  Neel, Seth and
                  Roth, Aaron and
                  Wu, Zhiwei~Steven},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
  pages        = {2564--2572},
  year         = {2018},
  editor       = {Dy, Jennifer and Krause, Andreas},
  volume       = {80},
  series       = {Proceedings of Machine Learning Research},
  month        = {10--15 Jul},
  publisher    = {PMLR},
  pdf          = {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
  url          = {http://proceedings.mlr.press/v80/kearns18a.html},
  abstract     = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}

@article{SalKueSteAniHinLonGha18,
  title        = {Aequitas: A Bias and Fairness Audit Toolkit},
  author       = {Saleiro, Pedro and
                  Kuester, Benedict and
                  Stevens, Abby and
                  Anisfeld, Ari and
                  Hinkson, Loren and
                  London, Jesse and
                  Ghani, Rayid},
  journal      = {arXiv preprint arXiv:1811.05577},
  year         = {2018}
}

@inproceedings{BuoGeb18,
  title        = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author       = {Buolamwini, Joy and
                  Gebru, Timnit},
  booktitle    = {Conference on fairness, accountability and transparency},
  pages        = {77--91},
  year         = {2018},
  organization = {PMLR}
}

@article{BerMul04,
  title        = {Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination},
  author       = {Bertrand, Marianne and
                  Mullainathan, Sendhil},
  journal      = {American economic review},
  volume       = {94},
  number       = {4},
  pages        = {991--1013},
  year         = {2004}
}
